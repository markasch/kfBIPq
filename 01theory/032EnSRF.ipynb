{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "227cec44-fa87-4bb6-b0b3-4175c57c9087",
   "metadata": {},
   "source": [
    "# Deterministic Ensemble Kalman Filters\n",
    "\n",
    "\n",
    "We have seen the stochastic ensemble Kalman filter in the previous section. It has the advantage of great simplicity, but is known to have unreliable statistical behaviour and can suffer from sampling errors. In particular, it does not converge in the ensemble Kalman inversion (EKI) context - see next section. Filters that do not use (stochastically) perturbed observations are called _deterministic_ filters. The family of _ensemble square root filters_ are deterministic, in this sense. These filters exhibit exponential conversion in the EKI context.\n",
    "\n",
    "## The filterng problem.\n",
    "\n",
    "Nonlinear state equation and observations are given by,\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    " x_{k+1}^{n} &=  \\mathcal{M}  ( {x}_{k}^{n} ) +  w_{k}^{n} , \\quad n=1, \\ldots, N_e, \\\\\n",
    "{y}_{k+1}    &=  \\mathcal{H} (x_{k+1}) +  v_{k+1}, \\\\\n",
    "\n",
    "\\end{align}\n",
    "\n",
    "where the noise/error terms\n",
    "\n",
    "$$\n",
    " w_k \\sim  \\mathcal{N}(0, Q), \\quad  v_k \\sim  \\mathcal{N}(0, R).\n",
    "$$\n",
    "\n",
    "::: {.callout-note title=\"Filtering problem\"}\n",
    "Predict the optimal state from the noisy measurements.\n",
    ":::\n",
    "\n",
    "\n",
    "## Recall: Ensemble Kalman Filter (EnKF)\n",
    "\n",
    "Just to recall the notation, here are the two steps (forecast/analysis, or predict/correct) of the EnKF.\n",
    "\n",
    "*Prediction/Forecast Step*\n",
    "\n",
    "- evolve each ensemble member forward\n",
    "   $$ x_{k+1}^{n} =  \\mathcal{M}  ( {x}_{k}^{n} ) +  w_{k}^{n}, \\quad n=1,\n",
    "   \\ldots, N_e$$\n",
    "- compute ensemble mean\n",
    "  $$ \\bar{x} = \\frac{1}{N_e} \\sum_{n=1}^{N_e}  x_{n}^{\\mathrm{f}} $$\n",
    "- compute covariance\n",
    "  $$ P^{\\mathrm{f}} = \\frac{1}{N_e - 1} X'^{\\mathrm{f}} ( X'^{\\mathrm{f}} )^{\\mathrm{T}} , $$\n",
    "  \n",
    "  where $X'^{\\mathrm{f}} = x^{\\mathrm{f}} - \\bar{x}$ is the ensemble state perturbation matrix.\n",
    "\n",
    "*Correction/Analysis Step*\n",
    "\n",
    "- compute the optimal Kalman gain\n",
    "  $$ K = P^{\\mathrm{f}} H^{\\mathrm{T}} (H P^{\\mathrm{f}} H^{\\mathrm{T}} + R)^{-1}  $$\n",
    "- update the ensemble using perturbed observations\n",
    "  $$  x_n^{\\mathrm{a}} =  x_n^{\\mathrm{f}}  + K( y_n + \\epsilon_y - H  x_n^{\\mathrm{f}}), \\quad n=1,\n",
    "   \\ldots, N_e,\n",
    "  $$\n",
    "  where $\\epsilon_n$ is the stochastic perturbation of the observations $y.$\n",
    "\n",
    "## Ensemble Square Root Filters (EnSRF)\n",
    "\n",
    "::: {.callout-tip title=\"Idea\"}\n",
    "Update the ensemble to preserve the covariance that is consistent with the KF _theoretical_ covariance\n",
    "$$ P_n^{\\mathrm{a}} = (I - K_n H) P_n^{\\mathrm{f}}. \n",
    "$$\n",
    ":::\n",
    "\n",
    "Recall the fully nonlinear formulation of the Kalman gain in terms of the state and observation anomailies,\n",
    "\n",
    "$$ K_n =  \\frac{1}{N_e - 1} \\mathbf{X}'^{\\mathrm{f}}(\\mathbf{Y}'^{\\mathrm{f}})^\\mathrm{T} S^{-1}, \n",
    "$$\n",
    "where\n",
    "$$ S = \\frac{1}{N_e - 1} Y'^{\\mathrm{f}} ( Y'^{\\mathrm{f}} )^{\\mathrm{T}} +  R.\n",
    "$$\n",
    "\n",
    "Then, to compute the posterior variance, we need to evaluate\n",
    "$$ P_n^{\\mathrm{a}} =  \\frac{1}{N_e - 1} X'^{\\mathrm{a}} ( X'^{\\mathrm{a}} )^{\\mathrm{T}}.\n",
    "$$\n",
    "\n",
    "So supposing there is a transform matrix, $T,$ such that\n",
    "$$  X'^{\\mathrm{a}} =  X'^{\\mathrm{f}} T,\n",
    "$$\n",
    "we can substitute in the definition of  $P_n^{\\mathrm{a}}$ to obtain\n",
    "\\begin{align}\n",
    "P_n^{\\mathrm{a}} &=  \\frac{1}{N_e - 1} X'^{\\mathrm{f}} T ( X'^{\\mathrm{f}} T )^{\\mathrm{T}} \\\\\n",
    "                 &=  \\frac{1}{N_e - 1} X'^{\\mathrm{f}} (T T^{\\mathrm{T}} ) ( X'^{\\mathrm{f}}  )^{\\mathrm{T}}.\n",
    "\\end{align}\n",
    "And, on the other hand, for the consistency, we want to obtain\n",
    "$$ (I - K_n H) P_n^{\\mathrm{f}} = \\frac{1}{N_e - 1} X'^{\\mathrm{f}} \n",
    "     \\left[ I - ( Y'^{\\mathrm{f}})^{\\mathrm{T}}  S^{-1}  Y'^{\\mathrm{f}}   \\right] ( X'^{\\mathrm{f}}  )^{\\mathrm{T}}, \n",
    "$$\n",
    "where we have used the relations\n",
    "\\begin{align}\n",
    "  K &=   P^{\\mathrm{f}} H^{\\mathrm{T}} (H P^{\\mathrm{f}} H^{\\mathrm{T}} + R)^{-1}, \\\\\n",
    "  P_n^{\\mathrm{f}} &=  \\frac{1}{N_e - 1} X'^{\\mathrm{f}} ( X'^{\\mathrm{f}} )^{\\mathrm{T}} \\\\\n",
    "  Y'^{\\mathrm{f}}  &= H X'^{\\mathrm{f}}. \n",
    "\\end{align}\n",
    "\n",
    "Hence,  $T$ must satisfy the so-called square root condition,\n",
    "\\begin{align}\n",
    "T T^{\\mathrm{T}}  &= I - ( Y'^{\\mathrm{f}})^{\\mathrm{T}}  S^{-1}  Y'^{\\mathrm{f}} \\\\\n",
    "                  &=  I - ( Y'^{\\mathrm{f}})^{\\mathrm{T}}  \n",
    "                      \\left( \\frac{1}{N_e - 1} Y'^{\\mathrm{f}} ( Y'^{\\mathrm{f}} )^{\\mathrm{T}} +  R  \\right)^{-1}  Y'^{\\mathrm{f}} \n",
    "\\end{align}\n",
    "\n",
    "We can replace the inversion of $S$ by the much simpler and more stable inversion of the diagonal measurement error covariance $R$ using the Sherman-Woodbury-Morrison formula\n",
    "$$ (A + UCV^{\\mathrm{T}})^{-1} = A^{-1} - A^{-1} V (C^{-1} + V A^{-1} U)^{-1} V A^{-1}.\n",
    "$$\n",
    "\n",
    "Identifying $A=I,$ $C = R^{-1},$ $U = Y^{\\mathrm{T}},$ $V=Y,$ we obtain the simpler form of the square root condition\n",
    "$$ T T^{\\mathrm{T}}  = \\left[I +  \\frac{1}{N_e - 1}( Y'^{\\mathrm{f}})^{\\mathrm{T}}  R^{-1}  Y'^{\\mathrm{f}} \\right]^{-1}. \n",
    "$$\n",
    "This form is the basis of the ETKF, or *transform filter.*\n",
    "\n",
    "Finally, the square root $T$ can be obtained from the eigenvalue factorization,\n",
    "$$  T T^{\\mathrm{T}}  = (U \\Sigma U^{\\mathrm{T}})^{-1}\n",
    "$$\n",
    "and thus\n",
    "$$ T = U \\Sigma^{-1/2} U^{\\mathrm{T}}.\n",
    "$$\n",
    "\n",
    "Note that $T$ is not unique, since for any orthogonal matrix $\\tilde{U},$ the product  $T \\tilde{U}$ will also satisfy the square root condition. This leads, in principle, to a large number of alternative forms for $T$ and the resulting square root filters.\n",
    "\n",
    "It can be shown that in general, this process can yield a biased and overconfident estimator of the posterior covariance. In order to remedy this, it suffices to ensure that $T$ is symmetric, which is the case in the above derivation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199cc198-09a9-4308-bceb-52f2fcfb304f",
   "metadata": {},
   "source": [
    "## ETKF in practice\n",
    "\n",
    "There are a number of possible simplifications that render the filter\n",
    "\n",
    "- unbiased\n",
    "- non-collapsing\n",
    "- computationally more stable\n",
    "- computationally cheaper\n",
    "\n",
    "One pathway is to scale the forecast observation ensemble perturbation matrix $Y^{\\mathrm{f}}$ to normalize the standard deviation, which then equals one, and thus reduce loss of accuracy due to roundoff errors.\n",
    "\n",
    "A second path is to avoid the eigenvalue decompostion of $T T^{\\mathrm{T}}$ and replace it by an SVD of $T$ alone. This is particularly advantageous in high dimensions and in the presence of bad conditioning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2997df4-89f6-4370-988e-b1550440ae74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
