<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.551">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Kalman Filters: from Bayes to Inverse Problems - 1&nbsp; Bayes’ Theorem</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../01theory/012BayesFilters.html" rel="next">
<link href="../index.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../01theory/011BayesTheory.html">Bayes’ Theory</a></li><li class="breadcrumb-item"><a href="../01theory/011BayesTheory.html"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Bayes’ Theorem</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="../index.html" class="sidebar-logo-link">
      <img src="../kf.png" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Kalman Filters: from Bayes to Inverse Problems</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome to <em>“Kalman Filters: from Bayes to Inverse Problems”</em></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">Bayes’ Theory</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../01theory/011BayesTheory.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Bayes’ Theorem</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../01theory/012BayesFilters.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Bayesian Filters</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
 <span class="menu-text">Kalman Filters</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../01theory/021BasicKF.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Kalman Filters</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../02examples/KF/021KFExample1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Example 1 - estimating a constant</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../02examples/KF/021KFExample2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Example 2 - scalar, Gaussian random walk</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../02examples/KF/021KFExample3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Example 3 - constant-velocity 1D position tracking</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../02examples/KF/021KFExample4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Example 4 - constant-velocity 2D motion tracking</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
 <span class="menu-text">Nonlinear Kalman Filters</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../01theory/022NlinKF.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Nonlinear Kalman Filters</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../02examples/EKF/022EKFExample1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Example 1 - tracking a random sinusoidal signal</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../02examples/EKF/022EKFExample2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Example 2 - tracking a noisy pendulum</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
 <span class="menu-text">Ensemble Filters</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../01theory/031EnKF.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Ensemble Kalman Filter</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../02examples/EnKF/032EnKFExample1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Example 1: noisy pendulum</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../02examples/EnKF/032EnKFExample2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Example 2: Lorenz63 system</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../02examples/EnKF/032EnKFExample3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Example 3: SIR Model</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../01theory/032EnSRF.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Deterministic Ensemble Kalman Filters</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../02examples/EnKF/032EnKFExample4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Example 4: ETKF for Lorenz63 system</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">
 <span class="menu-text">Inverse Problems</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../01theory/041BIP.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Bayesian Inversion</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../01theory/042EKI.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Ensemble Kalman Inversion (EKI)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../02examples/EKI/one_dim_EKI.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Example 1: One-dimensional EKI</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">1.1</span> Introduction</a></li>
  <li><a href="#bayesian-regression" id="toc-bayesian-regression" class="nav-link" data-scroll-target="#bayesian-regression"><span class="header-section-number">1.2</span> Bayesian Regression</a>
  <ul class="collapse">
  <li><a href="#classical-linear-regression" id="toc-classical-linear-regression" class="nav-link" data-scroll-target="#classical-linear-regression"><span class="header-section-number">1.2.1</span> Classical Linear Regression</a></li>
  <li><a href="#bayesian-linear-regression" id="toc-bayesian-linear-regression" class="nav-link" data-scroll-target="#bayesian-linear-regression"><span class="header-section-number">1.2.2</span> Bayesian Linear Regression</a></li>
  </ul></li>
  <li><a href="#some-examples" id="toc-some-examples" class="nav-link" data-scroll-target="#some-examples"><span class="header-section-number">1.3</span> Some Examples</a>
  <ul class="collapse">
  <li><a href="#bayesian-linear-regressionunivariate-scalar-parameter-case" id="toc-bayesian-linear-regressionunivariate-scalar-parameter-case" class="nav-link" data-scroll-target="#bayesian-linear-regressionunivariate-scalar-parameter-case"><span class="header-section-number">1.3.1</span> Bayesian Linear Regression—Univariate, Scalar Parameter Case</a></li>
  <li><a href="#other-examples" id="toc-other-examples" class="nav-link" data-scroll-target="#other-examples"><span class="header-section-number">1.3.2</span> Other Examples</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../01theory/011BayesTheory.html">Bayes’ Theory</a></li><li class="breadcrumb-item"><a href="../01theory/011BayesTheory.html"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Bayes’ Theorem</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Bayes’ Theorem</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="introduction" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">1.1</span> Introduction</h2>
<p>Bayes’ theorem is at the core of modern uncertainty quantification.</p>
<div id="thm-bayes" class="theorem">
<p><span class="theorem-title"><strong>Theorem 1.1 (Bayes Theorem)</strong></span> If we have two events, <span class="math inline">\(a\)</span> and <span class="math inline">\(b,\)</span> then <span class="math display">\[
P(b \vert a) = \frac{P(a\vert b) P(b)}{P(a)},
\]</span> where <span class="math inline">\(P(a \vert b)\)</span> is the conditional probability of <span class="math inline">\(a\)</span> given <span class="math inline">\(b.\)</span></p>
<p>In a more general setting, suppose that we have observations, <span class="math inline">\(y_{\mathrm{obs}} \in \mathbb{R}^{N_y},\)</span> of a state variable, <span class="math inline">\(x,\)</span> then the conditional PDF, <span class="math inline">\(\pi_X(x \vert y_{\mathrm{obs}}),\)</span> is given by Bayes’ formula <span class="math display">\[
\pi_X(x \vert y_{\mathrm{obs}}) = \frac{ \pi_Y( y_{\mathrm{obs}} \vert x) \pi_X(x)} {\pi_Y(y_{\mathrm{obs}}) }.
\]</span></p>
</div>
<p>Here,</p>
<ul>
<li><span class="math inline">\(\pi_X,\)</span> the <em>prior</em> PDF, quantifies our uncertainty about the state/parameters <span class="math inline">\(X\)</span> <strong>before</strong> observing <span class="math inline">\(y_{\mathrm{obs}},\)</span> while</li>
<li><span class="math inline">\(\pi_X(x \vert y_{\mathrm{obs}}),\)</span> the <em>posterior</em> PDF, quantifies our uncertainty <strong>after</strong> observing <span class="math inline">\(y_{\mathrm{obs}}.\)</span></li>
<li>The conditional PDF <span class="math inline">\(\pi_Y( y_{\mathrm{obs}} \vert x)\)</span> quantifies the likelihood of observing <span class="math inline">\(y\)</span> given a particular value of <span class="math inline">\(x.\)</span></li>
<li>Finally, the denominator, <span class="math inline">\(\pi_Y(y_{\mathrm{obs}}),\)</span> is simply a normalizing factor, and can be computed in a post-processing step.</li>
</ul>
<p>We thus rewrite the formula in <a href="#thm-bayes" class="quarto-xref">Theorem&nbsp;<span>1.1</span></a> as</p>
<p><span class="math display">\[
  \pi_X(x \vert y_{\mathrm{obs}})  \propto \pi_Y( y_{\mathrm{obs}} \vert x) \pi_X(x),
\]</span></p>
<p>or,</p>
<p><span class="math display">\[
    p(\mathrm{parameter}\mid\mathrm{data})\propto p(\mathrm{data}\mid\mathrm{parameter})\, p(\mathrm{parameter}),
\]</span></p>
<p>if we are dealing with a parameter-identification inverse problem.</p>
</section>
<section id="bayesian-regression" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="bayesian-regression"><span class="header-section-number">1.2</span> Bayesian Regression</h2>
<section id="classical-linear-regression" class="level3" data-number="1.2.1">
<h3 data-number="1.2.1" class="anchored" data-anchor-id="classical-linear-regression"><span class="header-section-number">1.2.1</span> Classical Linear Regression</h3>
<p>We recall the classical linear regression models. We present a general formulation that will prepare the terrain for the Bayesian approach.</p>
<p>Recall that in a regression problem we want to model the relationship between a dependent variable, <span class="math inline">\(y,\)</span> that is observed and independent variables, <span class="math inline">\({x_1,x_2, \ldots, x_p},\)</span> that represent the properties of a process. We have at our disposal <span class="math inline">\(n\)</span> data samples <span class="math display">\[\begin{equation}\label{eq:linreg_data}
{\cal D}=\{ (\mathbf {x}_i,\,y_i),\; i=1,\dots, n \} ,
\end{equation}\]</span> from which we can estimate the relationship between <span class="math inline">\(y\)</span> and <span class="math inline">\(x,\)</span> where <span class="math inline">\(\mathbf {x}_i = [x_{i1}, x_{i2}, \ldots, x_{ip}]\)</span>.</p>
<p>The data pairs come from observations, and we postulate a linear relationship between them of the form <span class="math display">\[\begin{align*}
y_{i} &amp; =\mathbf{x}_{i} \boldsymbol{\beta}+\epsilon_{i}, \quad i=1,\ldots,n,\\
&amp; =\beta_{0}+\beta_{1}x_{i1}+\beta_{2}x_{i2}+\cdots+\beta_{p}x_{ip}+\epsilon_{i},
\end{align*}\]</span> where</p>
<ul>
<li><span class="math inline">\(x_{ij}\)</span> is the <span class="math inline">\(i\)</span>-th value of the <span class="math inline">\(j\)</span>-th covariate, <span class="math inline">\(j=1,\ldots,p,\)</span></li>
<li><span class="math inline">\(\boldsymbol{\beta}=[\beta_0,\beta_1,\ldots,\beta_p]^\mathrm{T},\)</span></li>
<li>and <span class="math inline">\(\epsilon_i\)</span> are independent, identically distributed normal random variables, with mean zero and variance <span class="math inline">\(\sigma^2,\)</span> <span class="math display">\[
  \epsilon_{i}\sim\mathcal{N}(0,\sigma^{2}).
  \]</span></li>
</ul>
<p>We rewrite this in matrix-vector notation as <span class="math display">\[\begin{equation}
\mathbf{y} = X \boldsymbol{\beta}+\boldsymbol{\epsilon},
\end{equation}\]</span> where the augmented matrix <span id="eq-LSR"><span class="math display">\[
X=\left[\begin{array}{ccccc}
1      &amp; x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1p}\\
1      &amp; x_{21} &amp; x_{22} &amp; \cdots &amp; x_{2p}\\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots\\
1      &amp; x_{1p} &amp; x_{2p} &amp;  \cdots &amp; x_{np}
\end{array}\right]
\tag{1.1}\]</span></span> and <span class="math display">\[
\mathbf{y} = \left[\begin{array}{c}
      y_1 \\
      \vdots \\
      y_n
      \end{array}\right].
\]</span> An alternative way of writing this system, that will be needed below, is obtained by noticing that <span class="math display">\[
y_{i}  - \mathbf{x}_{i} \boldsymbol{\beta} = \epsilon_{i},
\]</span> which implies that the probability distribution of <span class="math display">\[
y_{i} - \mathbf{x}_{i} \boldsymbol{\beta} \sim \mathcal{N}(0,\sigma^{2})
\]</span> and hence that the probability distribution of each observation is Gaussian, <span class="math display">\[
y_{i} \sim \mathcal{N}(\mathbf{x}_{i} \boldsymbol{\beta},\sigma^{2}),
\]</span> or, in vector form, <span class="math display">\[
\mathbf{y} \sim \mathcal{N}(X \boldsymbol{\beta},\sigma^{2}I),
\]</span> where <span class="math inline">\(I\)</span> is the identity matrix.</p>
<p>For simplicity, let us consider the special case of a straight line regression for a single covariate, <span class="math inline">\(x,\)</span> <span class="math display">\[
  y_i = \beta_{0}+\beta_{1}x_{i},\quad i=1,\ldots,n.
\]</span> Then classical least squares regression consists of finding <span class="math inline">\((\beta_{0},\beta_{1})\)</span> that minimizes the sum of the squared errors <span class="math display">\[
E=\sum_{i=1}^{m}\epsilon_{i}^{2},
\]</span> where <span class="math display">\[
\epsilon_{i} = \left|\beta_0+ \beta_1 x_{i}-y_{i}\right|.
\]</span> The minimum was found by solving the equations for the optimum, <span class="math display">\[
\frac{\partial E}{\partial\beta_0}=0\,,\quad\frac{\partial E}{\partial\beta_1}=0.
\]</span> We find the two equations <span class="math display">\[\begin{eqnarray*}
    \left(\sum_{i=1}^{m}1\right)\beta_0+\left(\sum_{i=1}^{m}x_{i}\right)\beta_1 &amp; = &amp; \sum_{i=1}^{m}y_{i},\\
    \left(\sum_{i=1}^{m}x_{i}\right)\beta_0+\left(\sum_{i=1}^{m}x_{i}^{2}\right)\beta_1 &amp; = &amp; \sum_{i=1}^{m}x_{i}y_{i}.
\end{eqnarray*}\]</span> This can be rewritten as the following matrix system, <span id="eq-normeq"><span class="math display">\[
X^{\mathrm{T}} X \boldsymbol{\beta} = X^{\mathrm{T}}  \mathbf{y} ,
\tag{1.2}\]</span></span> with <span class="math display">\[
X=\left[\begin{array}{cc}
1      &amp; x_{1} \\
1      &amp; x_{2} \\
\vdots &amp; \vdots \\
1      &amp; x_{n}
\end{array}\right],
\quad  \mathbf{y} =\left[\begin{array}{c}
y_{1}\\
\vdots\\
y_{n}
\end{array}\right]
\quad\mathrm{and}\quad
\boldsymbol{\beta} =\left[\begin{array}{c}
\beta_0\\
\beta_1
\end{array}\right].
\]</span></p>
<p>In this univariate case, the system (<a href="#eq-normeq" class="quarto-xref">Equation&nbsp;<span>1.2</span></a>) can be solved explicitly. We obtain the optimal, least-squares parameter estimations, <span class="math display">\[
\beta_0 =  \bar{y}-\bar{x} \beta_1 , \quad \beta_1 = \sigma_{xy}/\sigma_{x}^{2},
\]</span> where the empirical means and variances are given by <span class="math display">\[
\bar{y}=\frac{1}{n}\sum_{i=1}^{n}y_{i},\quad\bar{x}=\frac{1}{n}\sum_{i=1}^{n}x_{i}
\]</span> and <span class="math display">\[
\sigma^2_{xy}=\frac{1}{n}\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)\left(y_{i}-\bar{y}\right),\quad\sigma_{x}^{2}=\frac{1}{n}\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}.
\]</span></p>
<p>Recall the general result expressed now in terms of linear regression.</p>
<div id="thm-linreg" class="theorem">
<p><span class="theorem-title"><strong>Theorem 1.2 (Linear Regression)</strong></span> For <span class="math inline">\(X \in \mathbb{R}^{n \times p},\)</span> <span class="math inline">\(\boldsymbol{\beta} \in \mathbb{R}^{p},\)</span> and <span class="math inline">\(\mathbf{y} \in\mathbb{R}^{n},\)</span> let <span class="math inline">\(\epsilon=\epsilon(x)= X \boldsymbol{\beta} - \mathbf{y}.\)</span> The general least-squares problem for linear regression is to find the vector <span class="math inline">\(\boldsymbol{\beta}\)</span> that minimizes the residual sum of squares, <span class="math display">\[
    \sum_{i=1}^{n}\epsilon_{i}^{2}=\epsilon^{\mathrm{T}}\epsilon=( X \boldsymbol{\beta} - \mathbf{y})^{\mathrm{T}}( X \boldsymbol{\beta} - \mathbf{y}).
    \]</span> Any vector that provides a minimal value is called a least-squares solution. The set of all least-squares solutions is precisely the set of solutions of the normal equations, <span class="math display">\[
       X^{\mathrm{T}}X \boldsymbol{\beta} = X^{\mathrm{T}}\mathbf{y}.
    \]</span><br>
There exists a unique least-squares solution, given by <span class="math inline">\(\boldsymbol{\beta}  =\left(X^{\mathrm{T}}X\right)^{-1}X^{\mathrm{T}}\mathbf{y},\)</span> if and only if <span class="math inline">\(\mathrm{rank}(X)=p.\)</span></p>
</div>
</section>
<section id="bayesian-linear-regression" class="level3" data-number="1.2.2">
<h3 data-number="1.2.2" class="anchored" data-anchor-id="bayesian-linear-regression"><span class="header-section-number">1.2.2</span> Bayesian Linear Regression</h3>
<p>We can now generalize to the case of Bayesian regression. In the Bayesian approach, not only the error term is random, but the elements of the model parameter vector <span class="math inline">\(\boldsymbol{\beta}\)</span> are also considered to be random variables. We also suppose that we have a prior distribution <span class="math inline">\(p(\boldsymbol{\beta})\)</span> available before any data are observed. Then, as observation data <span class="math inline">\(X\)</span> become available, the initial probabilities of the parameters can be updated to a posterior probability distribution, according to Bayes’ Law. This posterior is a refined, narrower distribution and provides not only a better estimate of the parameters, but also a complete uncertainty quantification.</p>
<p>Recall our linear model (<a href="#eq-LSR" class="quarto-xref">Equation&nbsp;<span>1.1</span></a>), <span class="math display">\[
\mathbf{y} = X \boldsymbol{\beta} + \boldsymbol{\epsilon},
\]</span> where the noise term is assumed to be Gaussian. The probabilistic model for regression is then a conditional probability, <span class="math display">\[
p(\mathbf{y} \mid X, \boldsymbol{\beta} ) = \mathcal{N} \left(\mathbf{y} \mid \mu_X , \Sigma_X  \right)
\]</span> and the Bayesian regression problem is to estimate the posterior probability distribution of the parameters, <span class="math inline">\(p(\boldsymbol{\theta} \mid \mathbf{y}),\)</span> where <span class="math inline">\(\boldsymbol{\theta}\)</span> includes <span class="math inline">\(\boldsymbol{\beta}\)</span> and can also include the constant noise variance <span class="math inline">\(\sigma^2,\)</span> where <span class="math inline">\(\Sigma_X = \sigma^2 I\)</span> and <span class="math inline">\(I\)</span> is the identity matrix. In the above case, <span class="math inline">\(\mu_X = X \boldsymbol{\beta}\)</span> is a linear function of <span class="math inline">\(X,\)</span> but, in general, linear regression can be extended to non-linear cases by simply replacing <span class="math inline">\(X\)</span> by a non-linear function of the inputs, <span class="math inline">\(\phi(X).\)</span> This is called basis function expansion, and in particular, when <span class="math inline">\(\phi(X)=[1,x,x^2,\ldots,x^d],\)</span> we obtain polynomial regression.</p>
<p>In general, we need to perform the following steps:</p>
<ol type="1">
<li>Specify the prior distribution of the parameters, <span class="math inline">\(\boldsymbol{\beta}.\)</span> Note that the prior law can either be known (from historical data or models, for example), or its parameters can be included in the estimation process.</li>
<li>Specify the distribution of the noise term. This is always taken as Gaussian in the case of regression analysis. As before, its parameter (variance) is either known, or can be inferred.</li>
<li>Compute the likelihood function of the parameters, using the noise distribution and the independence of each observation.</li>
<li>Calculate the posterior distribution of the parameters from the prior and the likelihood.</li>
</ol>
</section>
</section>
<section id="some-examples" class="level2" data-number="1.3">
<h2 data-number="1.3" class="anchored" data-anchor-id="some-examples"><span class="header-section-number">1.3</span> Some Examples</h2>
<section id="bayesian-linear-regressionunivariate-scalar-parameter-case" class="level3" data-number="1.3.1">
<h3 data-number="1.3.1" class="anchored" data-anchor-id="bayesian-linear-regressionunivariate-scalar-parameter-case"><span class="header-section-number">1.3.1</span> Bayesian Linear Regression—Univariate, Scalar Parameter Case</h3>
<p>As seen above, the simplest linear regression problem for a univariate independent variable, <span class="math inline">\(x,\)</span> is defined as <span class="math display">\[
y_i = \beta_{0}+\beta_{1}x_{i} + \epsilon_{i},\quad i=1,\ldots,n,
\]</span> where <span class="math display">\[
\epsilon_{i}\sim\mathcal{N}(0,\sigma^{2})
\]</span> and we assume that the individual observations are independent and identically distributed (i.i.d.). As a first step, we consider the case of estimating a noisy scalar, with <span class="math inline">\(\beta_0 = \beta,\)</span> <span class="math inline">\(\beta_1 =0\)</span> and <span class="math display">\[
y_i = \beta + \epsilon_{i},\quad i=1,\ldots,n.
\]</span> We are in possession of a Gaussian prior distribution for <span class="math inline">\(\beta,\)</span> <span class="math display">\[
\beta \sim\mathcal{N}(\mu_{\beta },\sigma_{\beta }^{2})
\]</span> with expectation <span class="math inline">\(\mu_{\beta }\)</span> and variance <span class="math inline">\(\sigma^2_{\beta },\)</span> which could come from historical data or some other model, for example. We also have <span class="math inline">\(n\)</span> independent, noisy observations, <span class="math display">\[
\mathbf{y}=\left(y_{1},y_{2},\ldots,y_{n}\right).
\]</span></p>
<p>The conditional distribution of the observations, given the data, is thus the distribution of the noise term, shifted by the data value (since the expectation of <span class="math inline">\(\beta\)</span> is equal to <span class="math inline">\(\beta\)</span> and the expectation of the noise is zero), <span class="math display">\[
y_{i}\mid \beta \sim\mathcal{N}(\beta,\sigma^{2})
\]</span> where we have conditioned on the true value of the parameter <span class="math inline">\(\beta.\)</span> Then, thanks to the i.i.d. property, the conditional distribution of the observations, or likelihood, is a product of Gaussians, <span class="math display">\[\begin{eqnarray} \label{eq:Gauss_likeli_scalar}
    p(\mathbf{y}\mid \beta) &amp; = &amp; \prod_{i=1}^{n}\frac{1}{\sqrt{2\pi\sigma^{2}}}\exp\left\{ -\frac{1}{2\sigma^{2}}\left(y_{i}-\beta\right)^{2}\right\} \\
    &amp; \propto &amp; \exp\left\{ -\frac{1}{2\sigma^{2}}\sum_{i=1}^{n}\left(y_{i}-\beta\right)^{2}\right\}
     \nonumber.
\end{eqnarray}\]</span> Now, according to Bayes’ Law (<span class="math inline">\(\ref{eq:Bayes-2}\)</span>), the posterior (inverse) distribution of the data given the measurements is <span class="math display">\[
p(\beta \mid\mathbf{y})\propto p(\mathbf{y}\mid \beta)p(\beta),
\]</span> so, using the data likelihood and the prior distributions, we have <span class="math display">\[\begin{eqnarray*}
    p(\beta \mid \mathbf{y}) &amp; \propto &amp; \exp\left\{ -\frac{1}{2}\sum_{i=1}^{n} \frac{ \left(y_{i}-\beta\right)^{2}}{\sigma^{2}} + \frac{\left(\beta-\mu_{\beta}\right)^{2}}{\sigma_{\beta}^{2}}\right\} \\
    &amp; \propto &amp; \exp\left\{ -\frac{1}{2}\left[\beta^{2}\left(\frac{n}{\sigma^{2}}+\frac{1}{\sigma_{\beta}^{2}}\right)-2\left(\sum_{i=1}^{n}\frac{y_{i}}{\sigma^{2}}+\frac{\mu_{\beta}}{\sigma_{\beta}^{2}}\right)\beta\right]\right\} .
\end{eqnarray*}\]</span> Notice that this is the product of two Gaussians which, by completing the square, can be show to be Gaussian itself. We obtain the posterior Gaussian distribution, <span class="math display">\[\begin{equation}
\label{eq:post_gaussian_scalar1}
\beta \mid \mathbf{y}\sim\mathcal{N}\left(\mu_{\beta\mid\mathbf{y}},\sigma_{\beta\mid\mathbf{y}}^{2}\right),
\end{equation}\]</span> where <span class="math display">\[\begin{equation}\label{eq:post_gaussian_scalar1_mu}
\mu_{\beta\mid\mathbf{y}}=\left(\frac{n}{\sigma^{2}}+\frac{1}{\sigma_{\beta}^{2}}\right)^{-1}\left(\sum_{i=1}^{n}\frac{y_{i}}{\sigma^{2}}+\frac{\mu_{\beta}}{\sigma_{\beta}^{2}}\right)
\end{equation}\]</span> and <span class="math display">\[\begin{equation}\label{eq:post_gaussian_scalar1_sig}
\sigma_{\beta\mid\mathbf{y}}^{2}=\left(\frac{n}{\sigma^{2}}+\frac{1}{\sigma_{\beta}^{2}}\right)^{-1}.
\end{equation}\]</span> Let us now examine more closely these two parameters of the posterior law. There are two important observations here:</p>
<ol type="1">
<li>The inverse of the posterior variance, called the posterior , is equal to the sum of the prior precision, <span class="math inline">\(1/\sigma_{\beta}^{2},\)</span> and the data precision, <span class="math inline">\(n/\sigma^{2}.\)</span></li>
<li>The posterior mean, or conditional expectation, can also be written as a sum of two terms, <span class="math display">\[\begin{eqnarray*}
     \mathrm{E}(\beta\mid\mathbf{y}) &amp; = &amp;     
     \frac{\sigma^{2}\sigma_{\beta}^{2}}{\sigma^{2}+n\sigma_{\beta}^{2}}\left(\frac{n}{\sigma^{2}}\bar
     {y}+\frac{\mu_{\beta}}{\sigma_{\beta}^{2}}\right)\\
     &amp; = &amp; w_{y}\bar{y}+w_{\mu_{\beta}}\mu_{\beta},
\end{eqnarray*}\]</span> where the sample mean, <span class="math display">\[
\bar{y}=\frac{1}{n}\sum_{i=1}^{n}y_{i}
\]</span> and the two weights, <span class="math display">\[
w_{y}=\frac{n\sigma_{\beta}^{2}}{\sigma^{2}+n\sigma_{\beta}^{2}}\,,\quad w_{\mu_{\beta}}=\frac{\sigma^{2}}{\sigma^{2}+n\sigma_{\beta}^{2}},
\]</span> add up to <span class="math display">\[
w_{y}+w_{\mu_{\beta}}=1.
\]</span></li>
</ol>
<p>We observe immediately that the posterior mean is the weighted sum, or weighted average, of the data mean, <span class="math inline">\(\bar{y},\)</span> and the prior mean, <span class="math inline">\(\mu_{\beta}.\)</span> Now let us examine the weights themselves.</p>
<ul>
<li>If there is a large uncertaintyin the prior, then <span class="math inline">\(\sigma_{\beta}^{2}\rightarrow\infty\)</span> and hence <span class="math inline">\(w_{y}\rightarrow1,\)</span> <span class="math inline">\(w_{\mu_{\beta}}\rightarrow 0\)</span> and the likelihood dominates the prior,leading to what is known as the sampling distribution for the posterior, <span class="math display">\[
p(\beta\mid\mathbf{y})\rightarrow\mathcal{N}(\bar{y},\sigma^{2}/n).
\]</span></li>
<li>If we have a large number of observations, then <span class="math inline">\(n\rightarrow\infty,\)</span> implying that <span class="math inline">\(w_{\mu_{\beta}}\rightarrow 0,\)</span> and the posterior now tends to the sample mean, whereas if we have few observations, then <span class="math inline">\(n \rightarrow 0\)</span> and the posterior <span class="math display">\[
p(\beta\mid\mathbf{y})\rightarrow\mathcal{N}(\mu_{\beta},\sigma_{\beta}^{2})
\]</span> tends to the prior.</li>
<li>In the case of equal uncertainties between data and prior, <span class="math inline">\(\sigma^{2}=\sigma_{\beta}^{2}\)</span> and the prior mean has the weight of a single additional observation.</li>
<li>Finally, if the uncertainties are small, either the prior is infinitely more precise than the data (<span class="math inline">\(\sigma_{\beta}^{2}\rightarrow0\)</span>) or the data are perfectly precise (<span class="math inline">\(\sigma^{2}\rightarrow0\)</span>).</li>
</ul>
<p>We end this example by rewriting the posterior mean and variance in a special form. Let us start with the mean <span id="eq-post_mean"><span class="math display">\[ \begin{eqnarray}
\mathrm{E}(\beta\mid\mathbf{y}) &amp; = &amp; \mu_{\beta}+\frac{n\sigma_{\beta}^{2}}{\sigma^{2}+n\sigma_{\beta}^{2}}\left(\bar{y}-\mu_{\beta}\right),\nonumber \\
&amp; = &amp; \mu_{\beta}+G\left(\bar{y}-\mu_{\beta}\right).  \end{eqnarray}
\tag{1.3}\]</span></span> We conclude that the prior mean <span class="math inline">\(\mu_{\beta}\)</span> is adjusted towards the sample mean <span class="math inline">\(\bar{y}\)</span> by a gain (or amplification factor) of <span class="math inline">\(G=1/(1+\sigma^{2}/n\sigma_{\beta}^{2}),\)</span> multiplied by the innovation <span class="math inline">\(\bar{y}-\mu_{\beta},\)</span> and we observe that the variance ratio, between data and prior, plays the essential role. In the same way, the posterior variance can be reformulated as <span id="eq-post_var"><span class="math display">\[
\sigma_{\beta\mid\mathbf{y}}^{2}=(1-G)\sigma_{\beta}^{2}
\tag{1.4}\]</span></span> and the posterior variance is thus updated from the prior variance according to the same gain <span class="math inline">\(G.\)</span></p>
<p>These last two equations, (<a href="#eq-post_mean" class="quarto-xref">Equation&nbsp;<span>1.3</span></a>) and (<a href="#eq-post_var" class="quarto-xref">Equation&nbsp;<span>1.4</span></a>), are fundamental for a good understanding of Bayesian parameter estimation (BPE) and of Data Assimilation, since they clearly express the interplay between prior and data, and the effect that each has on the posterior.</p>
</section>
<section id="other-examples" class="level3" data-number="1.3.2">
<h3 data-number="1.3.2" class="anchored" data-anchor-id="other-examples"><span class="header-section-number">1.3.2</span> Other Examples</h3>
<p>Many more examples can be found in <span class="citation" data-cites="Asch2022">(<a href="../index.html#ref-Asch2022" role="doc-biblioref">Asch 2022</a>)</span>.</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-Asch2022" class="csl-entry" role="listitem">
Asch, Mark. 2022. <em>A <span>Toolbox</span> for <span>Digital</span> <span>Twins</span>: <span>From</span> <span>Model</span>-<span>Based</span> to <span>Data</span>-<span>Driven</span></em>. Philadelphia, PA: Society for Industrial; Applied Mathematics. <a href="https://doi.org/10.1137/1.9781611976977">https://doi.org/10.1137/1.9781611976977</a>.
</div>
<div id="ref-Asch2016" class="csl-entry" role="listitem">
Asch, Mark, Marc Bocquet, and Maëlle Nodet. 2016. <em>Data <span>Assimilation</span>: <span>Methods</span>, <span>Algorithms</span>, and <span>Applications</span></em>. Philadelphia, PA: Society for Industrial; Applied Mathematics. <a href="https://doi.org/10.1137/1.9781611974546">https://doi.org/10.1137/1.9781611974546</a>.
</div>
<div id="ref-Stuart2022" class="csl-entry" role="listitem">
Calvello, Edoardo, Sebastian Reich, and Andrew M. Stuart. 2022. <span>“Ensemble <span>Kalman</span> <span>Methods</span>: <span>A</span> <span>Mean</span> <span>Field</span> <span>Perspective</span>.”</span> arXiv (to appear in Acta Numerica 2025). <a href="http://arxiv.org/abs/2209.11371">http://arxiv.org/abs/2209.11371</a>.
</div>
<div id="ref-carrillo2024statistical" class="csl-entry" role="listitem">
Carrillo, J. A., F. Hoffmann, A. M. Stuart, and U. Vaes. 2024a. <span>“Statistical Accuracy of Approximate Filtering Methods.”</span> <a href="https://arxiv.org/abs/2402.01593">https://arxiv.org/abs/2402.01593</a>.
</div>
<div id="ref-carrillo2024mean" class="csl-entry" role="listitem">
———. 2024b. <span>“The Mean Field Ensemble Kalman Filter: Near-Gaussian Setting.”</span> <a href="https://arxiv.org/abs/2212.13239">https://arxiv.org/abs/2212.13239</a>.
</div>
<div id="ref-Stuart2015" class="csl-entry" role="listitem">
Dashti, Masoumeh, and Andrew M. Stuart. 2015. <span>“The <span>Bayesian</span> <span>Approach</span> to <span>Inverse</span> <span>Problems</span>.”</span> In <em>Handbook of <span>Uncertainty</span> <span>Quantification</span></em>, edited by Roger Ghanem, David Higdon, and Houman Owhadi, 1–118. Cham: Springer International Publishing. <a href="https://doi.org/10.1007/978-3-319-11259-6_7-1">https://doi.org/10.1007/978-3-319-11259-6_7-1</a>.
</div>
<div id="ref-Huang2022" class="csl-entry" role="listitem">
Huang, Daniel Zhengyu, Jiaoyang Huang, Sebastian Reich, and Andrew M Stuart. 2022. <span>“Efficient Derivative-Free Bayesian Inference for Large-Scale Inverse Problems.”</span> <em>Inverse Problems</em> 38 (12): 125006. <a href="https://doi.org/10.1088/1361-6420/ac99fa">https://doi.org/10.1088/1361-6420/ac99fa</a>.
</div>
<div id="ref-Stuart2013" class="csl-entry" role="listitem">
Iglesias, Marco A, Kody J H Law, and Andrew M Stuart. 2013. <span>“Ensemble <span>Kalman</span> Methods for Inverse Problems.”</span> <em>Inverse Problems</em> 29 (4): 045001. <a href="https://doi.org/10.1088/0266-5611/29/4/045001">https://doi.org/10.1088/0266-5611/29/4/045001</a>.
</div>
<div id="ref-LSZ2015" class="csl-entry" role="listitem">
Law, Kody, Andrew Stuart, and Konstantinos Zygalakis. 2015. <em>Data <span>Assimilation</span>: <span>A</span> <span>Mathematical</span> <span>Introduction</span></em>. Vol. 62. Texts in <span>Applied</span> <span>Mathematics</span>. Cham: Springer International Publishing. <a href="https://doi.org/10.1007/978-3-319-20325-6">https://doi.org/10.1007/978-3-319-20325-6</a>.
</div>
<div id="ref-vetra2018" class="csl-entry" role="listitem">
Sanita Vetra-Carvalho, Lars Nerger, Peter Jan van Leeuwen, and Jean-Marie Beckers. 2018. <span>“State-of-the-Art Stochastic Data Assimilation Methods for High-Dimensional Non-Gaussian Problems.”</span> <em>Tellus A: Dynamic Meteorology and Oceanography</em> 70 (1): 1–43. <a href="https://doi.org/10.1080/16000870.2018.1445364">https://doi.org/10.1080/16000870.2018.1445364</a>.
</div>
<div id="ref-Sarkka2023" class="csl-entry" role="listitem">
Särkkä, S., and L. Svensson. 2023. <em>Bayesian Filtering and Smoothing</em>. 2nd ed. Institute of Mathematical Statistics Textbooks. Cambridge University Press. <a href="https://doi.org/10.1017/9781108917407">https://doi.org/10.1017/9781108917407</a>.
</div>
<div id="ref-wu2023learning" class="csl-entry" role="listitem">
Wu, Jin-Long, Matthew E. Levine, Tapio Schneider, and Andrew Stuart. 2023. <span>“Learning about Structural Errors in Models of Complex Dynamical Systems.”</span> <a href="https://arxiv.org/abs/2401.00035">https://arxiv.org/abs/2401.00035</a>.
</div>
</div>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../index.html" class="pagination-link" aria-label="Welcome to _&quot;Kalman Filters: from Bayes to Inverse Problems&quot;_">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Welcome to <em>“Kalman Filters: from Bayes to Inverse Problems”</em></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../01theory/012BayesFilters.html" class="pagination-link" aria-label="Bayesian Filters">
        <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Bayesian Filters</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>