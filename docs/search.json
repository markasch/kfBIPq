[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Kalman Filters: from Bayes to Inverse Problems",
    "section": "",
    "text": "Welcome to “Kalman Filters: from Bayes to Inverse Problems”\nThis book contains a presentation of Kalman filters, from basics to nonlinear and ensemble filters. To understand these well, examples are provided in the form of jupyter notebooks. Then the notion of Bayesian inverse problems (BIP) is introduced. Finally, there is a detailed presentation of the use of the ensemble Kalman filter as a basis for the solution of inverse problems. This is denoted EKI, or ensemble Kalman inversion, following the magnificent work of Andrew Stuart and his collaborators.\nThis book is based upon a number of sources. The original Bayesian formulation for inverse problems (Dashti and Stuart 2015) was the basis for the later ensemble Kalman inversion, presented in a series of papers (Iglesias, Law, and Stuart 2013; Calvello, Reich, and Stuart 2022; Huang et al. 2022).\nIn (Asch, Bocquet, and Nodet 2016) and (Law, Stuart, and Zygalakis 2015) the reader can find detailed presentations of Kalman filter approaches for data assimilation. In (Asch 2022) there are basic explanations of uncertainty quantification, inverse problems and their use for digital twins.",
    "crumbs": [
      "Welcome to _\"Kalman Filters: from Bayes to Inverse Problems\"_"
    ]
  },
  {
    "objectID": "index.html#author",
    "href": "index.html#author",
    "title": "Kalman Filters: from Bayes to Inverse Problems",
    "section": "Author",
    "text": "Author\nMark Asch is Emeritus Professor at the Université de Picardie Jules Verne in the mathematics department.\nhttps://markasch.github.io/DT-tbx-v1/, http://masch.perso.math.cnrs.fr/",
    "crumbs": [
      "Welcome to _\"Kalman Filters: from Bayes to Inverse Problems\"_"
    ]
  },
  {
    "objectID": "index.html#citation",
    "href": "index.html#citation",
    "title": "Kalman Filters: from Bayes to Inverse Problems",
    "section": "Citation",
    "text": "Citation\nAsch, Mark. Kalman Filters: from Bayes to Inverse Problems. Online (2024) https://markasch.github.io/kfBIPq/\n@book{Asch2024\n    title = {Kalman {F}ilters: from {B}ayes to {I}nverse {P}roblems},\n    author = {Asch, Mark},\n    url = {https://markasch.github.io/kfBIPq/},\n    year = {2024},\n    publisher = {Online}\n}",
    "crumbs": [
      "Welcome to _\"Kalman Filters: from Bayes to Inverse Problems\"_"
    ]
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Kalman Filters: from Bayes to Inverse Problems",
    "section": "License",
    "text": "License\nThis online book is frequently updated and edited. It’s content is free to use, licensed under a Creative Commons licence, and the code can be found on GitHub. A physical copy of the book will be available at a later date.\nLicense: Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.",
    "crumbs": [
      "Welcome to _\"Kalman Filters: from Bayes to Inverse Problems\"_"
    ]
  },
  {
    "objectID": "index.html#references",
    "href": "index.html#references",
    "title": "Kalman Filters: from Bayes to Inverse Problems",
    "section": "References",
    "text": "References\n\n\nAsch, Mark. 2022. A Toolbox for Digital\nTwins: From\nModel-Based to\nData-Driven. Philadelphia, PA: Society\nfor Industrial; Applied Mathematics. https://doi.org/10.1137/1.9781611976977.\n\n\nAsch, Mark, Marc Bocquet, and Maëlle Nodet. 2016. Data\nAssimilation: Methods,\nAlgorithms, and Applications.\nPhiladelphia, PA: Society for Industrial; Applied Mathematics. https://doi.org/10.1137/1.9781611974546.\n\n\nCalvello, Edoardo, Sebastian Reich, and Andrew M. Stuart. 2022.\n“Ensemble Kalman Methods: A\nMean Field Perspective.”\narXiv (to appear in Acta Numerica 2025). http://arxiv.org/abs/2209.11371.\n\n\nCarrillo, J. A., F. Hoffmann, A. M. Stuart, and U. Vaes. 2024a.\n“Statistical Accuracy of Approximate Filtering Methods.” https://arxiv.org/abs/2402.01593.\n\n\n———. 2024b. “The Mean Field Ensemble Kalman Filter: Near-Gaussian\nSetting.” https://arxiv.org/abs/2212.13239.\n\n\nDashti, Masoumeh, and Andrew M. Stuart. 2015. “The\nBayesian Approach to Inverse\nProblems.” In Handbook of\nUncertainty Quantification, edited by\nRoger Ghanem, David Higdon, and Houman Owhadi, 1–118. Cham: Springer\nInternational Publishing. https://doi.org/10.1007/978-3-319-11259-6_7-1.\n\n\nHuang, Daniel Zhengyu, Jiaoyang Huang, Sebastian Reich, and Andrew M\nStuart. 2022. “Efficient Derivative-Free Bayesian Inference for\nLarge-Scale Inverse Problems.” Inverse Problems 38 (12):\n125006. https://doi.org/10.1088/1361-6420/ac99fa.\n\n\nIglesias, Marco A, Kody J H Law, and Andrew M Stuart. 2013.\n“Ensemble Kalman Methods for Inverse\nProblems.” Inverse Problems 29 (4): 045001. https://doi.org/10.1088/0266-5611/29/4/045001.\n\n\nLaw, Kody, Andrew Stuart, and Konstantinos Zygalakis. 2015. Data\nAssimilation: A Mathematical\nIntroduction. Vol. 62. Texts in Applied\nMathematics. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-319-20325-6.\n\n\nSanita Vetra-Carvalho, Lars Nerger, Peter Jan van Leeuwen, and\nJean-Marie Beckers. 2018. “State-of-the-Art Stochastic Data\nAssimilation Methods for High-Dimensional Non-Gaussian Problems.”\nTellus A: Dynamic Meteorology and Oceanography 70 (1): 1–43. https://doi.org/10.1080/16000870.2018.1445364.\n\n\nSärkkä, S., and L. Svensson. 2023. Bayesian Filtering and\nSmoothing. 2nd ed. Institute of Mathematical Statistics Textbooks.\nCambridge University Press. https://doi.org/10.1017/9781108917407.\n\n\nWu, Jin-Long, Matthew E. Levine, Tapio Schneider, and Andrew Stuart.\n2023. “Learning about Structural Errors in Models of Complex\nDynamical Systems.” https://arxiv.org/abs/2401.00035.",
    "crumbs": [
      "Welcome to _\"Kalman Filters: from Bayes to Inverse Problems\"_"
    ]
  },
  {
    "objectID": "01theory/011BayesTheory.html",
    "href": "01theory/011BayesTheory.html",
    "title": "1  Bayes’ Theorem",
    "section": "",
    "text": "1.1 Introduction\nBayes’ theorem is at the core of modern uncertainty quantification.\nHere,\nWe thus rewrite the formula in Theorem 1.1 as\n\\[\n  \\pi_X(x \\vert y_{\\mathrm{obs}})  \\propto \\pi_Y( y_{\\mathrm{obs}} \\vert x) \\pi_X(x),\n\\]\nor,\n\\[\n    p(\\mathrm{parameter}\\mid\\mathrm{data})\\propto p(\\mathrm{data}\\mid\\mathrm{parameter})\\, p(\\mathrm{parameter}),\n\\]\nif we are dealing with a parameter-identification inverse problem.",
    "crumbs": [
      "Bayes' Theory",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Bayes' Theorem</span>"
    ]
  },
  {
    "objectID": "01theory/011BayesTheory.html#introduction",
    "href": "01theory/011BayesTheory.html#introduction",
    "title": "1  Bayes’ Theorem",
    "section": "",
    "text": "Theorem 1.1 (Bayes Theorem) If we have two events, \\(a\\) and \\(b,\\) then \\[\nP(b \\vert a) = \\frac{P(a\\vert b) P(b)}{P(a)},\n\\] where \\(P(a \\vert b)\\) is the conditional probability of \\(a\\) given \\(b.\\)\nIn a more general setting, suppose that we have observations, \\(y_{\\mathrm{obs}} \\in \\mathbb{R}^{N_y},\\) of a state variable, \\(x,\\) then the conditional PDF, \\(\\pi_X(x \\vert y_{\\mathrm{obs}}),\\) is given by Bayes’ formula \\[\n\\pi_X(x \\vert y_{\\mathrm{obs}}) = \\frac{ \\pi_Y( y_{\\mathrm{obs}} \\vert x) \\pi_X(x)} {\\pi_Y(y_{\\mathrm{obs}}) }.\n\\]\n\n\n\n\\(\\pi_X,\\) the prior PDF, quantifies our uncertainty about the state/parameters \\(X\\) before observing \\(y_{\\mathrm{obs}},\\) while\n\\(\\pi_X(x \\vert y_{\\mathrm{obs}}),\\) the posterior PDF, quantifies our uncertainty after observing \\(y_{\\mathrm{obs}}.\\)\nThe conditional PDF \\(\\pi_Y( y_{\\mathrm{obs}} \\vert x)\\) quantifies the likelihood of observing \\(y\\) given a particular value of \\(x.\\)\nFinally, the denominator, \\(\\pi_Y(y_{\\mathrm{obs}}),\\) is simply a normalizing factor, and can be computed in a post-processing step.",
    "crumbs": [
      "Bayes' Theory",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Bayes' Theorem</span>"
    ]
  },
  {
    "objectID": "01theory/011BayesTheory.html#theory",
    "href": "01theory/011BayesTheory.html#theory",
    "title": "1  Bayes’ Theorem",
    "section": "1.2 Theory",
    "text": "1.2 Theory\nStill to come…",
    "crumbs": [
      "Bayes' Theory",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Bayes' Theorem</span>"
    ]
  },
  {
    "objectID": "01theory/011BayesTheory.html#some-examples",
    "href": "01theory/011BayesTheory.html#some-examples",
    "title": "1  Bayes’ Theorem",
    "section": "1.3 Some Examples",
    "text": "1.3 Some Examples\n\n1.3.1 Bayesian Linear Regression—Univariate, Scalar Parameter Case\nAs seen above, the simplest linear regression problem for a univariate independent variable, \\(x,\\) is defined as \\[\ny_i = \\beta_{0}+\\beta_{1}x_{i} + \\epsilon_{i},\\quad i=1,\\ldots,n,\n\\] where \\[\n\\epsilon_{i}\\sim\\mathcal{N}(0,\\sigma^{2})\n\\] and we assume that the individual observations are independent and identically distributed (i.i.d.). As a first step, we consider the case of estimating a noisy scalar, with \\(\\beta_0 = \\beta,\\) \\(\\beta_1 =0\\) and \\[\ny_i = \\beta + \\epsilon_{i},\\quad i=1,\\ldots,n.\n\\] We are in possession of a Gaussian prior distribution for \\(\\beta,\\) \\[\n\\beta \\sim\\mathcal{N}(\\mu_{\\beta },\\sigma_{\\beta }^{2})\n\\] with expectation \\(\\mu_{\\beta }\\) and variance \\(\\sigma^2_{\\beta },\\) which could come from historical data or some other model, for example. We also have \\(n\\) independent, noisy observations, \\[\n\\mathbf{y}=\\left(y_{1},y_{2},\\ldots,y_{n}\\right).\n\\]\nThe conditional distribution of the observations, given the data, is thus the distribution of the noise term, shifted by the data value (since the expectation of \\(\\beta\\) is equal to \\(\\beta\\) and the expectation of the noise is zero), \\[\ny_{i}\\mid \\beta \\sim\\mathcal{N}(\\beta,\\sigma^{2})\n\\] where we have conditioned on the true value of the parameter \\(\\beta.\\) Then, thanks to the i.i.d. property, the conditional distribution of the observations, or likelihood, is a product of Gaussians, \\[\\begin{eqnarray} \\label{eq:Gauss_likeli_scalar}\n    p(\\mathbf{y}\\mid \\beta) & = & \\prod_{i=1}^{n}\\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}\\exp\\left\\{ -\\frac{1}{2\\sigma^{2}}\\left(y_{i}-\\beta\\right)^{2}\\right\\} \\\\\n    & \\propto & \\exp\\left\\{ -\\frac{1}{2\\sigma^{2}}\\sum_{i=1}^{n}\\left(y_{i}-\\beta\\right)^{2}\\right\\}\n     \\nonumber.\n\\end{eqnarray}\\] Now, according to Bayes’ Law (\\(\\ref{eq:Bayes-2}\\)), the posterior (inverse) distribution of the data given the measurements is \\[\np(\\beta \\mid\\mathbf{y})\\propto p(\\mathbf{y}\\mid \\beta)p(\\beta),\n\\] so, using the data likelihood and the prior distributions, we have \\[\\begin{eqnarray*}\n    p(\\beta \\mid \\mathbf{y}) & \\propto & \\exp\\left\\{ -\\frac{1}{2}\\sum_{i=1}^{n} \\frac{ \\left(y_{i}-\\beta\\right)^{2}}{\\sigma^{2}} + \\frac{\\left(\\beta-\\mu_{\\beta}\\right)^{2}}{\\sigma_{\\beta}^{2}}\\right\\} \\\\\n    & \\propto & \\exp\\left\\{ -\\frac{1}{2}\\left[\\beta^{2}\\left(\\frac{n}{\\sigma^{2}}+\\frac{1}{\\sigma_{\\beta}^{2}}\\right)-2\\left(\\sum_{i=1}^{n}\\frac{y_{i}}{\\sigma^{2}}+\\frac{\\mu_{\\beta}}{\\sigma_{\\beta}^{2}}\\right)\\beta\\right]\\right\\} .\n\\end{eqnarray*}\\] Notice that this is the product of two Gaussians which, by completing the square, can be show to be Gaussian itself. We obtain the posterior Gaussian distribution, \\[\\begin{equation}\n\\label{eq:post_gaussian_scalar1}\n\\beta \\mid \\mathbf{y}\\sim\\mathcal{N}\\left(\\mu_{\\beta\\mid\\mathbf{y}},\\sigma_{\\beta\\mid\\mathbf{y}}^{2}\\right),\n\\end{equation}\\] where \\[\\begin{equation}\\label{eq:post_gaussian_scalar1_mu}\n\\mu_{\\beta\\mid\\mathbf{y}}=\\left(\\frac{n}{\\sigma^{2}}+\\frac{1}{\\sigma_{\\beta}^{2}}\\right)^{-1}\\left(\\sum_{i=1}^{n}\\frac{y_{i}}{\\sigma^{2}}+\\frac{\\mu_{\\beta}}{\\sigma_{\\beta}^{2}}\\right)\n\\end{equation}\\] and \\[\\begin{equation}\\label{eq:post_gaussian_scalar1_sig}\n\\sigma_{\\beta\\mid\\mathbf{y}}^{2}=\\left(\\frac{n}{\\sigma^{2}}+\\frac{1}{\\sigma_{\\beta}^{2}}\\right)^{-1}.\n\\end{equation}\\] Let us now examine more closely these two parameters of the posterior law. There are two important observations here:\n\nThe inverse of the posterior variance, called the posterior , is equal to the sum of the prior precision, \\(1/\\sigma_{\\beta}^{2},\\) and the data precision, \\(n/\\sigma^{2}.\\)\nThe posterior mean, or conditional expectation, can also be written as a sum of two terms, \\[\\begin{eqnarray*}\n     \\mathrm{E}(\\beta\\mid\\mathbf{y}) & = &     \n     \\frac{\\sigma^{2}\\sigma_{\\beta}^{2}}{\\sigma^{2}+n\\sigma_{\\beta}^{2}}\\left(\\frac{n}{\\sigma^{2}}\\bar\n     {y}+\\frac{\\mu_{\\beta}}{\\sigma_{\\beta}^{2}}\\right)\\\\\n     & = & w_{y}\\bar{y}+w_{\\mu_{\\beta}}\\mu_{\\beta},\n\\end{eqnarray*}\\] where the sample mean, \\[\n\\bar{y}=\\frac{1}{n}\\sum_{i=1}^{n}y_{i}\n\\] and the two weights, \\[\nw_{y}=\\frac{n\\sigma_{\\beta}^{2}}{\\sigma^{2}+n\\sigma_{\\beta}^{2}}\\,,\\quad w_{\\mu_{\\beta}}=\\frac{\\sigma^{2}}{\\sigma^{2}+n\\sigma_{\\beta}^{2}},\n\\] add up to \\[\nw_{y}+w_{\\mu_{\\beta}}=1.\n\\]\n\nWe observe immediately that the posterior mean is the weighted sum, or weighted average, of the data mean, \\(\\bar{y},\\) and the prior mean, \\(\\mu_{\\beta}.\\) Now let us examine the weights themselves.\n\nIf there is a large uncertaintyin the prior, then \\(\\sigma_{\\beta}^{2}\\rightarrow\\infty\\) and hence \\(w_{y}\\rightarrow1,\\) \\(w_{\\mu_{\\beta}}\\rightarrow 0\\) and the likelihood dominates the prior,leading to what is known as the sampling distribution for the posterior, \\[\np(\\beta\\mid\\mathbf{y})\\rightarrow\\mathcal{N}(\\bar{y},\\sigma^{2}/n).\n\\]\nIf we have a large number of observations, then \\(n\\rightarrow\\infty,\\) implying that \\(w_{\\mu_{\\beta}}\\rightarrow 0,\\) and the posterior now tends to the sample mean, whereas if we have few observations, then \\(n \\rightarrow 0\\) and the posterior \\[\np(\\beta\\mid\\mathbf{y})\\rightarrow\\mathcal{N}(\\mu_{\\beta},\\sigma_{\\beta}^{2})\n\\] tends to the prior.\nIn the case of equal uncertainties between data and prior, \\(\\sigma^{2}=\\sigma_{\\beta}^{2}\\) and the prior mean has the weight of a single additional observation.\nFinally, if the uncertainties are small, either the prior is infinitely more precise than the data (\\(\\sigma_{\\beta}^{2}\\rightarrow0\\)) or the data are perfectly precise (\\(\\sigma^{2}\\rightarrow0\\)).\n\nWe end this example by rewriting the posterior mean and variance in a special form. Let us start with the mean \\[ \\begin{eqnarray}\n\\mathrm{E}(\\beta\\mid\\mathbf{y}) & = & \\mu_{\\beta}+\\frac{n\\sigma_{\\beta}^{2}}{\\sigma^{2}+n\\sigma_{\\beta}^{2}}\\left(\\bar{y}-\\mu_{\\beta}\\right),\\nonumber \\\\\n& = & \\mu_{\\beta}+G\\left(\\bar{y}-\\mu_{\\beta}\\right).  \\end{eqnarray}\n\\tag{1.3}\\] We conclude that the prior mean \\(\\mu_{\\beta}\\) is adjusted towards the sample mean \\(\\bar{y}\\) by a gain (or amplification factor) of \\(G=1/(1+\\sigma^{2}/n\\sigma_{\\beta}^{2}),\\) multiplied by the innovation \\(\\bar{y}-\\mu_{\\beta},\\) and we observe that the variance ratio, between data and prior, plays the essential role. In the same way, the posterior variance can be reformulated as \\[\n\\sigma_{\\beta\\mid\\mathbf{y}}^{2}=(1-G)\\sigma_{\\beta}^{2}\n\\tag{1.4}\\] and the posterior variance is thus updated from the prior variance according to the same gain \\(G.\\)\nThese last two equations, (Equation 1.3) and (Equation 1.4), are fundamental for a good understanding of Bayesian parameter estimation (BPE) and of Data Assimilation, since they clearly express the interplay between prior and data, and the effect that each has on the posterior.\n\n\n1.3.2 Other Examples\nMany more examples can be found in (Asch 2022).\n\n\n\n\nAsch, Mark. 2022. A Toolbox for Digital Twins: From Model-Based to Data-Driven. Philadelphia, PA: Society for Industrial; Applied Mathematics. https://doi.org/10.1137/1.9781611976977.\n\n\nAsch, Mark, Marc Bocquet, and Maëlle Nodet. 2016. Data Assimilation: Methods, Algorithms, and Applications. Philadelphia, PA: Society for Industrial; Applied Mathematics. https://doi.org/10.1137/1.9781611974546.\n\n\nCalvello, Edoardo, Sebastian Reich, and Andrew M. Stuart. 2022. “Ensemble Kalman Methods: A Mean Field Perspective.” arXiv (to appear in Acta Numerica 2025). http://arxiv.org/abs/2209.11371.\n\n\nCarrillo, J. A., F. Hoffmann, A. M. Stuart, and U. Vaes. 2024a. “Statistical Accuracy of Approximate Filtering Methods.” https://arxiv.org/abs/2402.01593.\n\n\n———. 2024b. “The Mean Field Ensemble Kalman Filter: Near-Gaussian Setting.” https://arxiv.org/abs/2212.13239.\n\n\nDashti, Masoumeh, and Andrew M. Stuart. 2015. “The Bayesian Approach to Inverse Problems.” In Handbook of Uncertainty Quantification, edited by Roger Ghanem, David Higdon, and Houman Owhadi, 1–118. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-319-11259-6_7-1.\n\n\nHuang, Daniel Zhengyu, Jiaoyang Huang, Sebastian Reich, and Andrew M Stuart. 2022. “Efficient Derivative-Free Bayesian Inference for Large-Scale Inverse Problems.” Inverse Problems 38 (12): 125006. https://doi.org/10.1088/1361-6420/ac99fa.\n\n\nIglesias, Marco A, Kody J H Law, and Andrew M Stuart. 2013. “Ensemble Kalman Methods for Inverse Problems.” Inverse Problems 29 (4): 045001. https://doi.org/10.1088/0266-5611/29/4/045001.\n\n\nLaw, Kody, Andrew Stuart, and Konstantinos Zygalakis. 2015. Data Assimilation: A Mathematical Introduction. Vol. 62. Texts in Applied Mathematics. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-319-20325-6.\n\n\nSanita Vetra-Carvalho, Lars Nerger, Peter Jan van Leeuwen, and Jean-Marie Beckers. 2018. “State-of-the-Art Stochastic Data Assimilation Methods for High-Dimensional Non-Gaussian Problems.” Tellus A: Dynamic Meteorology and Oceanography 70 (1): 1–43. https://doi.org/10.1080/16000870.2018.1445364.\n\n\nSärkkä, S., and L. Svensson. 2023. Bayesian Filtering and Smoothing. 2nd ed. Institute of Mathematical Statistics Textbooks. Cambridge University Press. https://doi.org/10.1017/9781108917407.\n\n\nWu, Jin-Long, Matthew E. Levine, Tapio Schneider, and Andrew Stuart. 2023. “Learning about Structural Errors in Models of Complex Dynamical Systems.” https://arxiv.org/abs/2401.00035.",
    "crumbs": [
      "Bayes' Theory",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Bayes' Theorem</span>"
    ]
  },
  {
    "objectID": "01theory/012BayesFilters.html",
    "href": "01theory/012BayesFilters.html",
    "title": "2  Bayesian Filters",
    "section": "",
    "text": "All filters can be expressed in a Bayesian form.\n\n\n\n\n\n\nImportant\n\n\n\nKalman filters are a special case of the more general Bayesian filter formulation\n\n\nComing soon…\n\n\n\n\nAsch, Mark. 2022. A Toolbox for Digital Twins: From Model-Based to Data-Driven. Philadelphia, PA: Society for Industrial; Applied Mathematics. https://doi.org/10.1137/1.9781611976977.\n\n\nAsch, Mark, Marc Bocquet, and Maëlle Nodet. 2016. Data Assimilation: Methods, Algorithms, and Applications. Philadelphia, PA: Society for Industrial; Applied Mathematics. https://doi.org/10.1137/1.9781611974546.\n\n\nCalvello, Edoardo, Sebastian Reich, and Andrew M. Stuart. 2022. “Ensemble Kalman Methods: A Mean Field Perspective.” arXiv (to appear in Acta Numerica 2025). http://arxiv.org/abs/2209.11371.\n\n\nCarrillo, J. A., F. Hoffmann, A. M. Stuart, and U. Vaes. 2024a. “Statistical Accuracy of Approximate Filtering Methods.” https://arxiv.org/abs/2402.01593.\n\n\n———. 2024b. “The Mean Field Ensemble Kalman Filter: Near-Gaussian Setting.” https://arxiv.org/abs/2212.13239.\n\n\nDashti, Masoumeh, and Andrew M. Stuart. 2015. “The Bayesian Approach to Inverse Problems.” In Handbook of Uncertainty Quantification, edited by Roger Ghanem, David Higdon, and Houman Owhadi, 1–118. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-319-11259-6_7-1.\n\n\nHuang, Daniel Zhengyu, Jiaoyang Huang, Sebastian Reich, and Andrew M Stuart. 2022. “Efficient Derivative-Free Bayesian Inference for Large-Scale Inverse Problems.” Inverse Problems 38 (12): 125006. https://doi.org/10.1088/1361-6420/ac99fa.\n\n\nIglesias, Marco A, Kody J H Law, and Andrew M Stuart. 2013. “Ensemble Kalman Methods for Inverse Problems.” Inverse Problems 29 (4): 045001. https://doi.org/10.1088/0266-5611/29/4/045001.\n\n\nLaw, Kody, Andrew Stuart, and Konstantinos Zygalakis. 2015. Data Assimilation: A Mathematical Introduction. Vol. 62. Texts in Applied Mathematics. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-319-20325-6.\n\n\nSanita Vetra-Carvalho, Lars Nerger, Peter Jan van Leeuwen, and Jean-Marie Beckers. 2018. “State-of-the-Art Stochastic Data Assimilation Methods for High-Dimensional Non-Gaussian Problems.” Tellus A: Dynamic Meteorology and Oceanography 70 (1): 1–43. https://doi.org/10.1080/16000870.2018.1445364.\n\n\nSärkkä, S., and L. Svensson. 2023. Bayesian Filtering and Smoothing. 2nd ed. Institute of Mathematical Statistics Textbooks. Cambridge University Press. https://doi.org/10.1017/9781108917407.\n\n\nWu, Jin-Long, Matthew E. Levine, Tapio Schneider, and Andrew Stuart. 2023. “Learning about Structural Errors in Models of Complex Dynamical Systems.” https://arxiv.org/abs/2401.00035.",
    "crumbs": [
      "Bayes' Theory",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Bayesian Filters</span>"
    ]
  },
  {
    "objectID": "01theory/021BasicKF.html",
    "href": "01theory/021BasicKF.html",
    "title": "3  Kalman Filters",
    "section": "",
    "text": "3.1 Introduction\nThe Kalman filter can be used for\nKalman filters are linear (and Gaussian) or nonlinear. Among the nonlinear approaches, the Ensemble Kalman filter (EnKF) provides a derivative-free approach to the solution of inverse problems.",
    "crumbs": [
      "Kalman Filters",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Kalman Filters</span>"
    ]
  },
  {
    "objectID": "01theory/021BasicKF.html#introduction",
    "href": "01theory/021BasicKF.html#introduction",
    "title": "3  Kalman Filters",
    "section": "",
    "text": "state estimation—this is the direct filtering (or smoothing) problem\nparameter estimation—this is the inverse problem based on filtering with a pseudo-time\n\n\n\n\n\n\n\n\nDefinition: filtering problem\n\n\n\nFiltering is the sequential updating of the probability distribution of the state of a (possibly stochastic) dynamical system, given partial (or sparse), noisy observations.\nThe KF provides an explicit, optimal solution to this problem in the setting of linear dynamical systems, linear observations, and additive Gaussian noise. In this case, we obtain explicit update formulas for the mean and covariance of the resulting posterior Gaussian probability distribution.",
    "crumbs": [
      "Kalman Filters",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Kalman Filters</span>"
    ]
  },
  {
    "objectID": "01theory/021BasicKF.html#kalman-filter-problem---general-formulation",
    "href": "01theory/021BasicKF.html#kalman-filter-problem---general-formulation",
    "title": "3  Kalman Filters",
    "section": "3.2 Kalman filter problem - General formulation",
    "text": "3.2 Kalman filter problem - General formulation\nWe present a very general formulation that will later be convenient for joint state and parameter estimation problems.\nWe consider a discrete-time dynamical system with noisy state transitions and noisy observations.\n\nDynamics:\n\\[v_{j+1} = \\Psi(v_j) + \\xi_j, \\quad j \\in \\mathbb{Z}^+\\]\nObservations:\n\\[y_{j+1} = h (v_{j+1}) + \\eta_{j+1}, \\quad j \\in \\mathbb{Z}^+\\]\nProbability (densities):\n\\[v_0 \\sim \\mathcal{N}(m_0,C_0), \\quad \\xi_j \\sim \\mathcal{N}(0,\\Sigma), \\quad \\eta_j \\sim \\mathcal{N}(0,\\Gamma)\\]\nProbability (independence):\n\\[v_0 \\perp {\\xi_j} \\perp {\\eta_j}\\]\nOperators:\n\\[\\begin{eqnarray}\n    \\Psi \\colon \\mathcal{H}_s &\\mapsto \\mathcal{H}_s, \\\\\n    h  \\colon \\mathcal{H}_s &\\mapsto \\mathcal{H}_o,\n   \\end{eqnarray}\\] where \\(v_j \\in \\mathcal{H}_s,\\) \\(y_j \\in \\mathcal{H}_o\\) and \\(\\mathcal{H}\\) is a finite-dimensional Hilbert space.\n\nFiltering problem: estimate (optimally) the state \\(v_j\\) of the dynamical system at time \\(j,\\) given the data \\(Y_j = \\{y_i\\}_{i=1}^{j}\\) up to time \\(j.\\) This is achieved by using a two-step predictor-corrector method. We will use the more general notation of (Law, Stuart, and Zygalakis 2015) instead of the usual, classical state-space formulation that is used in (Asch, Bocquet, and Nodet 2016) and (Asch 2022).\nThe objective is to update the filtering distribution \\(\\mathbb{P}(v_j \\vert Y_j),\\) from time \\(j\\) to time \\(j+1,\\) in the linear, Gaussian case, where - \\(\\Psi\\) and \\(h\\) are linear maps - all distributions are Gaussian.\nSuppose \\[\\begin{eqnarray}\n   \\Psi(v) &= Mv \\\\\n   h(v) &= Hv,\n\\end{eqnarray}\\] where the matrices \\(M \\in \\mathbb{R}^{n \\times n},\\) \\(H \\in \\mathbb{R}^{m \\times n},\\) with \\(m \\le n\\) and \\(\\mathrm{rank}(H)=m.\\)\n\nLet \\((m_j, C_j)\\) denote the mean and covariance of \\(v_j \\vert Y_j\\) and note that these entirely characterize the random variable since it is Gaussian.\nLet \\((\\hat{m}_{j+1}, \\hat{C}_{j+1})\\) denote the mean and covariance of \\(v_{j+1} \\vert Y_j\\) and note that these entirely characterize the random variable since it is Gaussian.\nDerive the map \\((m_j, C_j) \\mapsto (m_{j+1}, C_{j+1})\\) using the previous step.\n\n\n3.2.1 Prediction/Forecast\n\\[ \\mathbb{P}(v_n \\vert y_1, \\ldots, y_n) \\mapsto \\mathbb{P}(v_{n+1} \\vert y_1, \\ldots, y_n) \\]\n\nP0: initialize \\((m_0, C_0)\\) and compute \\(v_0\\)\nP1: predict the state, measurement\n\\[\\begin{align}\n   v_{j+1} &= M v_j + \\xi_j \\\\\n   y_{j+1} &= H v_{j+1} + \\eta_{j+1}\n   \\end{align}\\]\nP2: predict the mean and covariance\n\\[\\begin{align}\n   \\hat{m}_{j+1}  &= M m_j \\\\\n   \\hat{C}_{j+1}  &= M C_j M^{\\mathrm{T}} + \\Sigma\n   \\end{align}\\]\n\n\n\n3.2.2 Correction/Analysis\n\\[ \\mathbb{P}(v_{n+1} \\vert y_1, \\ldots, y_n) \\mapsto \\mathbb{P}(v_{n+1} \\vert y_1, \\ldots, y_{n+1}) \\]\n\nC1: compute the innovation\n\\[ d_{j+1} = y_{j+1} - H \\hat{m}_{j+1} \\]\nC2: compute the measurement covariance\n\\[ S_{j+1} = H \\hat{C}_{j+1}  H^{\\mathrm{T}} + \\Gamma \\]\nC3: compute the (optimal) Kalman gain\n\\[ K_{j+1} = \\hat{C}_{j+1} H^{\\mathrm{T}}  S_{j+1}^{-1} \\]\nC4: update/correct the mean and covariance\n\\[\\begin{align}\n   {m}_{j+1}  &= \\hat{m}_{j+1} + K_{j+1} d_{j+1}, \\\\\n   {C}_{j+1}  &= \\hat{C}_{j+1} - K_{j+1}  S_{j+1}  K_{j+1}^{\\mathrm{T}}.\n   \\end{align}\\]\n\n\n\n3.2.3 Loop over time\n\nset \\(j = j+1\\)\ngo to step P1",
    "crumbs": [
      "Kalman Filters",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Kalman Filters</span>"
    ]
  },
  {
    "objectID": "01theory/021BasicKF.html#state-space-formulation",
    "href": "01theory/021BasicKF.html#state-space-formulation",
    "title": "3  Kalman Filters",
    "section": "3.3 State-space formulation",
    "text": "3.3 State-space formulation\nIn classical (linear) filter theory, a state space formulation is usually used.\n\\[\\begin{eqnarray}\n&& x_{k+1}  = F x_k + B u_k + w_k \\\\\n&& y_{k+1}  = H x_k + v_k,\n\\end{eqnarray}\\] where \\(u\\) is a control input, and \\(w_k \\sim \\mathcal{N}(0,Q),\\) \\(v_k \\sim \\mathcal{N}(0,R).\\) Moreover, \\(A\\) is the dynamics and \\(H\\) the observation operator.\nThe 2-step filter:\nInitialization\n\\[ x_0, \\quad P_0 \\]\n1. Prediction\n\\[\\begin{eqnarray}\n&& x_{k+1}^- = F x_k \\\\\n&& P_{k+1}^- = F P_k F^{\\mathrm{T}} + Q\n\\end{eqnarray}\\]\n2. Correction\n\\[\\begin{eqnarray}\n  K_{k+1} && = P_{k+1}^{-} H^{\\mathrm{T}} ( H  P_{k+1}^{-} H^{\\mathrm{T}} + R )^{-1}\n               \\quad (= P_{k+1}^- H^{\\mathrm{T}} S^{-1})\\\\\n  x_{k+1} &&= x_{k+1}^{-} + K_{k+1} (y_{k+1} - H x_{k+1}^- ) \\\\\n  P_{k+1} &&= (I -  K_{k+1} H ) P_{k+1}^-\n              \\quad (= P_{k+1}^- - K_{k+1} S K^{\\mathrm{T}}_{k+1})\n\\end{eqnarray}\\]\nLoop\nSet \\(k = k+1\\) and go to step 1.\n\n\n\n\n\n\nTip\n\n\n\nIn some cases, the superscripts f and a are used to denote the forecast/prediction, and analysis/correction variables, respectively. We avoid this, for clarity of notation.",
    "crumbs": [
      "Kalman Filters",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Kalman Filters</span>"
    ]
  },
  {
    "objectID": "01theory/021BasicKF.html#passage-from-continuous-to-discrete-for-random-dynamic-systems",
    "href": "01theory/021BasicKF.html#passage-from-continuous-to-discrete-for-random-dynamic-systems",
    "title": "3  Kalman Filters",
    "section": "3.4 Passage from Continuous to Discrete for Random Dynamic Systems",
    "text": "3.4 Passage from Continuous to Discrete for Random Dynamic Systems\nMost often, in the use of Kalman filtering, we deal with a continuous dynamic system that is modelled by a system of ODEs, with some random process noise. The passage from the continuous to a discrete-time formulation, which is needed for the KF, requires some attention. Following (Särkkä and Svensson 2023), we will derive the associated discretization of the process matrix and the process noise covariance matrix.\nSuppose we have a linear, time invariant (LTI) system of ODEs with an additive random noise term (this is the engineering approach that avoids the use of SDEs and Itô calculus—see (Asch 2022)\n\\[\n  \\frac{\\mathrm{d}\\mathbf{x}}{\\mathrm{d}t} = F \\mathbf{x}(t) + L \\mathbf{w}(t) , \\quad \\mathbf{x}(0) = \\mathbf{x}_0,\n\\]\nwhere \\(\\mathbf{w}(t)\\) is a white, Gaussian noise process with expectation and covariance\n\\[\\begin{align}\n& \\mathrm{E}[\\mathbf{w}(t)] = 0, \\\\\n& \\mathrm{Cov}[\\mathbf{w}(\\tau_1)\\mathbf{w}(\\tau_2)] = \\delta(\\tau_1 - \\tau_2) Q^c\n\\end{align}\\]\nwith \\(Q^c\\) the spectral density matrix of the white noise process, which is the continuous-time analogue of a covariance matrix, and in the scalar case is just the noise variance. Usually\n\\[\n  Q^c = \\mathrm{diag}(q^c_1, \\ldots, q^c_n),\n\\]\nwhere \\(q^c_1, \\ldots, q^c_n\\) are the spectral densities of \\(w_1(t), \\ldots, w_n(t),\\) the components of \\(\\mathbf{w}(t).\\)\nWe now proceed to convert this continous LTI system into its discrete counterpart, needed for the KF algorithm,\n\\[\n\\mathbf{x}_{k+1 } = A_k  \\mathbf{x}_{k} + \\mathbf{q}_{k},\n\\]\nwhere \\(A_k\\) is the discretized process/dynamics transition matrix, and \\(\\mathbf{q}_{k} \\sim \\mathcal{N} (0, Q)\\) is the Gaussian process noise.\nTo obtain expressions for \\(A\\) and \\(Q,\\) we need to solve the random ODE system. Doing this, we can show that\n\\[\n  A_k = \\exp (F \\Delta t_{k}) = \\sum_{n=0}^{\\infty} \\frac{(F \\Delta t_{k})^n}{n !}\n\\]\nand\n\\[\n  Q_k = \\int_0^{\\Delta t_{k}}  \\exp (F s) L Q^c L^{\\mathrm{T}} \\exp (F s)^{\\mathrm{T}} \\mathrm{d}s.\n\\]\nUsually, \\(F\\) is nilpotent to some low order, so the computation of the matrix exponential is relatively easy to do. This will be illustrated in the examples.\n\n\n\n\nAsch, Mark. 2022. A Toolbox for Digital Twins: From Model-Based to Data-Driven. Philadelphia, PA: Society for Industrial; Applied Mathematics. https://doi.org/10.1137/1.9781611976977.\n\n\nAsch, Mark, Marc Bocquet, and Maëlle Nodet. 2016. Data Assimilation: Methods, Algorithms, and Applications. Philadelphia, PA: Society for Industrial; Applied Mathematics. https://doi.org/10.1137/1.9781611974546.\n\n\nCalvello, Edoardo, Sebastian Reich, and Andrew M. Stuart. 2022. “Ensemble Kalman Methods: A Mean Field Perspective.” arXiv (to appear in Acta Numerica 2025). http://arxiv.org/abs/2209.11371.\n\n\nCarrillo, J. A., F. Hoffmann, A. M. Stuart, and U. Vaes. 2024a. “Statistical Accuracy of Approximate Filtering Methods.” https://arxiv.org/abs/2402.01593.\n\n\n———. 2024b. “The Mean Field Ensemble Kalman Filter: Near-Gaussian Setting.” https://arxiv.org/abs/2212.13239.\n\n\nDashti, Masoumeh, and Andrew M. Stuart. 2015. “The Bayesian Approach to Inverse Problems.” In Handbook of Uncertainty Quantification, edited by Roger Ghanem, David Higdon, and Houman Owhadi, 1–118. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-319-11259-6_7-1.\n\n\nHuang, Daniel Zhengyu, Jiaoyang Huang, Sebastian Reich, and Andrew M Stuart. 2022. “Efficient Derivative-Free Bayesian Inference for Large-Scale Inverse Problems.” Inverse Problems 38 (12): 125006. https://doi.org/10.1088/1361-6420/ac99fa.\n\n\nIglesias, Marco A, Kody J H Law, and Andrew M Stuart. 2013. “Ensemble Kalman Methods for Inverse Problems.” Inverse Problems 29 (4): 045001. https://doi.org/10.1088/0266-5611/29/4/045001.\n\n\nLaw, Kody, Andrew Stuart, and Konstantinos Zygalakis. 2015. Data Assimilation: A Mathematical Introduction. Vol. 62. Texts in Applied Mathematics. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-319-20325-6.\n\n\nSanita Vetra-Carvalho, Lars Nerger, Peter Jan van Leeuwen, and Jean-Marie Beckers. 2018. “State-of-the-Art Stochastic Data Assimilation Methods for High-Dimensional Non-Gaussian Problems.” Tellus A: Dynamic Meteorology and Oceanography 70 (1): 1–43. https://doi.org/10.1080/16000870.2018.1445364.\n\n\nSärkkä, S., and L. Svensson. 2023. Bayesian Filtering and Smoothing. 2nd ed. Institute of Mathematical Statistics Textbooks. Cambridge University Press. https://doi.org/10.1017/9781108917407.\n\n\nWu, Jin-Long, Matthew E. Levine, Tapio Schneider, and Andrew Stuart. 2023. “Learning about Structural Errors in Models of Complex Dynamical Systems.” https://arxiv.org/abs/2401.00035.",
    "crumbs": [
      "Kalman Filters",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Kalman Filters</span>"
    ]
  },
  {
    "objectID": "02examples/KF/021KFExample1.html",
    "href": "02examples/KF/021KFExample1.html",
    "title": "4  Example 1 - estimating a constant",
    "section": "",
    "text": "4.1 Conclusion\nWhile the estimation of a constant is relatively straightforward, this example clearly demonstrates the workings of the Kalman filter. In the second Figure (R=1) in particular, the Kalman filtering is evident as the estimate appears considerably smoother than the noisy measurements. We observe the speed of convergence of the variance in the bottom subplot of the respective Figures.",
    "crumbs": [
      "Kalman Filters",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Example 1 - estimating a constant</span>"
    ]
  },
  {
    "objectID": "02examples/KF/021KFExample2.html",
    "href": "02examples/KF/021KFExample2.html",
    "title": "5  Example 2 - scalar, Gaussian random walk",
    "section": "",
    "text": "5.1 Implementation of the KF\nHere is a straigthforward, matrix-based implementation of the KF that follows exactly the thoeretical formulation above.\nFor more generality, below we will rewrite the Kalman filter as a class.\n# scalar KF\nx = x0\nP = P0\n# Allocate space for estimated position\nestimated_positions  = np.zeros(N)\nestimated_covariance = np.zeros(N)\n# Kalman Filter Loop\nfor k in range(N):\n    # Predict\n    x = F*x\n    P = F*P*F + Q    \n    # Correct\n    d = Y[k] - H*x\n    S = H*P*H + R\n    K = P*H / S\n    x = x + K*d\n    P = P - K*S*K\n    # Store the filtered position and covariance\n    estimated_positions[k]  = x\n    estimated_covariance[k] = P\n \n# Plot the true positions, noisy measurements, and the Kalman filter estimates\n# and the 2 sigma upper and lower analytic population bounds\nlower_bound = estimated_positions - 1.96*np.sqrt(estimated_covariance)\nupper_bound = estimated_positions + 1.96*np.sqrt(estimated_covariance)\n\nfig, ax = plt.subplots(1, figsize=(10,6))\nax.fill_between(t, lower_bound, upper_bound, facecolor='C0', alpha=0.2,\n                label='2 sigma range')\nax.legend(loc='upper left')\nax.plot(t, X, label='True Position', color='green')\nax.scatter(t, Y, label='Noisy Measurements', color='orange', marker='o')\nax.plot(t, estimated_positions, label='Kalman Filter Estimate', color='blue')\n\nplt.xlabel('Time Step')\nplt.ylabel('Position')\nplt.title('Kalman Filter Estimation Over Time')\nplt.legend()\nplt.show()",
    "crumbs": [
      "Kalman Filters",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Example 2 - scalar, Gaussian random walk</span>"
    ]
  },
  {
    "objectID": "02examples/KF/021KFExample2.html#implementation-of-kf-as-a-class",
    "href": "02examples/KF/021KFExample2.html#implementation-of-kf-as-a-class",
    "title": "5  Example 2 - scalar, Gaussian random walk",
    "section": "5.2 Implementation of KF as a class",
    "text": "5.2 Implementation of KF as a class\nFor more generality, we rewrite the Kalman filter as a class.\n\nclass KalmanFilter:\n    \"\"\"\n    An implementation of the classic Kalman Filter for a SCALAR linear dynamic systems.\n    The Kalman Filter is an optimal recursive data processing algorithm which\n    aims to estimate the state of a system from noisy observations.\n\n    Attributes:\n        F (np.ndarray): The state transition matrix.\n        B (np.ndarray): The control input marix.\n        H (np.ndarray): The observation matrix.\n        u (np.ndarray): the control input.\n        Q (np.ndarray): The process noise covariance matrix.\n        R (np.ndarray): The measurement noise covariance matrix.\n        x (np.ndarray): The mean state estimate of the previous step (k-1).\n        P (np.ndarray): The state covariance of previous step (k-1).\n    \"\"\"\n    def __init__(self, F=None, B=None, H=None, Q=None, R=None, x0=None, P0=None):\n        \"\"\"\n        Initializes the Kalman Filter with the necessary matrices and initial state.\n\n        Parameters:\n            F (np.ndarray): The state transition matrix.\n            B (np.ndarray): The control input marix.\n            H (np.ndarray): The observation matrix.\n            u (np.ndarray): the control input.\n            Q (np.ndarray): The process noise covariance matrix.\n            R (np.ndarray): The measurement noise covariance matrix.\n            x0 (np.ndarray): The initial state estimate.\n            P0 (np.ndarray): The initial state covariance matrix.\n        \"\"\"\n        self.F = F  # State transition matrix\n        self.B = B  # Control input matrix\n        self.u = u  # Control input\n        self.H = H  # Observation matrix\n        self.Q = Q  # Process noise covariance\n        self.R = R  # Measurement noise covariance\n        self.x = x0  # Initial state estimate\n        self.P = P0  # Initial estimate covariance\n\n    def predict(self):\n        \"\"\"\n        Predicts the state and the state covariance for the next time step.\n        \"\"\"\n        self.x = self.F @ self.x + self.B @ self.u\n        self.P = self.F @ self.P @ self.F.T + self.Q\n        #return self.x\n\n    def update(self, z):\n        \"\"\"\n        Updates the state estimate with the latest measurement.\n\n        Parameters:\n            z (np.ndarray): The measurement at the current step.\n        \"\"\"\n        y = z - self.H @ self.x\n        S = self.H @ self.P @ self.H.T + self.R\n        K = self.P @ self.H.T @ np.linalg.inv(S)\n        self.x = self.x + K @ y \n        I = np.eye(self.P.shape[0])\n        self.P = (I - K @ self.H) @ self.P\n\n\n# generate the GRW\nimport numpy as np\nimport matplotlib.pyplot as plt\nnp.random.seed(1955)\n# time interval and time-step\nT = 1\nN = 50\ndt = T/N\nt = np.linspace(0,1,N)\n# set parameters\nsig_w = 1\nsig_v = 0.5\nF = 1\nQ = sig_w**2\nH = 1\nR = sig_v**2\n# initialize\nx0 = 0\nP0 = 1\n# simulate data\nX = np.zeros(N)\nY = np.zeros(N)\nx = x0\n# loop over time\nfor j in range(N):\n    w = Q*np.random.randn()\n    x = F*x + w;\n    y = H*x + sig_v*np.random.randn()\n    X[j] = x\n    Y[j] = y# ready to execute the KF...\n\n# Kalman Filter Initialization\nF = np.array([[1]])        # State transition matrix\nB = np.array([[0]])        # No control input\nu = np.array([[0]])        # No control input\nH = np.array([[1]])        # Measurement function\nQ = np.array([[sig_w**2]]) # Process noise covariance\nR = np.array([[sig_v**2]]) # Measurement noise covariance\nx0 = np.array([[0]])       # Initial state estimate\nP0 = np.array([[1]])       # Initial estimate covariance\n\nkf = KalmanFilter(F, B, H, Q, R, x0, P0)\n\n# Allocate space for estimated position\nestimated_positions = np.zeros(N)\n\n# Kalman Filter Loop\nfor k in range(N):\n    # Predict\n    kf.predict()    \n    # Correct\n    measurement = np.array([[Y[k]]])\n    kf.update(measurement)    \n    # Store the filtered position \n    estimated_positions[k]  = np.ndarray.item(kf.x[0])\n\n# Plot the true positions, noisy measurements, and the Kalman filter estimates\nplt.figure(figsize=(10, 6))\nplt.plot(t, X, label='True Position', color='green')\nplt.scatter(t, Y, label='Noisy Measurements', color='red', marker='o')\nplt.plot(t, estimated_positions, label='Kalman Filter Estimate', color='blue')\n\nplt.xlabel('Time Step')\nplt.ylabel('Position')\nplt.title('Kalman Filter Estimation Over Time')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nAsch, Mark. 2022. A Toolbox for Digital Twins: From Model-Based to Data-Driven. Philadelphia, PA: Society for Industrial; Applied Mathematics. https://doi.org/10.1137/1.9781611976977.\n\n\nAsch, Mark, Marc Bocquet, and Maëlle Nodet. 2016. Data Assimilation: Methods, Algorithms, and Applications. Philadelphia, PA: Society for Industrial; Applied Mathematics. https://doi.org/10.1137/1.9781611974546.\n\n\nCalvello, Edoardo, Sebastian Reich, and Andrew M. Stuart. 2022. “Ensemble Kalman Methods: A Mean Field Perspective.” arXiv (to appear in Acta Numerica 2025). http://arxiv.org/abs/2209.11371.\n\n\nCarrillo, J. A., F. Hoffmann, A. M. Stuart, and U. Vaes. 2024a. “Statistical Accuracy of Approximate Filtering Methods.” https://arxiv.org/abs/2402.01593.\n\n\n———. 2024b. “The Mean Field Ensemble Kalman Filter: Near-Gaussian Setting.” https://arxiv.org/abs/2212.13239.\n\n\nDashti, Masoumeh, and Andrew M. Stuart. 2015. “The Bayesian Approach to Inverse Problems.” In Handbook of Uncertainty Quantification, edited by Roger Ghanem, David Higdon, and Houman Owhadi, 1–118. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-319-11259-6_7-1.\n\n\nHuang, Daniel Zhengyu, Jiaoyang Huang, Sebastian Reich, and Andrew M Stuart. 2022. “Efficient Derivative-Free Bayesian Inference for Large-Scale Inverse Problems.” Inverse Problems 38 (12): 125006. https://doi.org/10.1088/1361-6420/ac99fa.\n\n\nIglesias, Marco A, Kody J H Law, and Andrew M Stuart. 2013. “Ensemble Kalman Methods for Inverse Problems.” Inverse Problems 29 (4): 045001. https://doi.org/10.1088/0266-5611/29/4/045001.\n\n\nLaw, Kody, Andrew Stuart, and Konstantinos Zygalakis. 2015. Data Assimilation: A Mathematical Introduction. Vol. 62. Texts in Applied Mathematics. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-319-20325-6.\n\n\nSanita Vetra-Carvalho, Lars Nerger, Peter Jan van Leeuwen, and Jean-Marie Beckers. 2018. “State-of-the-Art Stochastic Data Assimilation Methods for High-Dimensional Non-Gaussian Problems.” Tellus A: Dynamic Meteorology and Oceanography 70 (1): 1–43. https://doi.org/10.1080/16000870.2018.1445364.\n\n\nSärkkä, S., and L. Svensson. 2023. Bayesian Filtering and Smoothing. 2nd ed. Institute of Mathematical Statistics Textbooks. Cambridge University Press. https://doi.org/10.1017/9781108917407.\n\n\nWu, Jin-Long, Matthew E. Levine, Tapio Schneider, and Andrew Stuart. 2023. “Learning about Structural Errors in Models of Complex Dynamical Systems.” https://arxiv.org/abs/2401.00035.",
    "crumbs": [
      "Kalman Filters",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Example 2 - scalar, Gaussian random walk</span>"
    ]
  },
  {
    "objectID": "02examples/KF/021KFExample3.html",
    "href": "02examples/KF/021KFExample3.html",
    "title": "6  Example 3 - constant-velocity 1D position tracking",
    "section": "",
    "text": "Kalman filters are extensively used in navigation and other GPS-based systems. These can be either linear or nonlinear. In the latter case, one usually resorts to the extended Kalman, filter (EKF) that is designed to deal better with nonlinear state equations—see below.\nProcess noise design for \\(Q\\)\n\ndiagonal\nconstant velocity model\nconstant acceleration model\n\nWe begin with a very simple case of tracking a 1D position due to constant velocity motion, described by\n\\[\\frac{\\mathrm{d}x}{\\mathrm{d}t} = v, \\quad x(0) = x_0.\\]\nThis equation can be written as a system of two equations\n\\[\\frac{\\mathrm{d}\\mathbf{x}}{\\mathrm{d}t} = F \\mathbf{x} + L \\mathbf{w} , \\quad \\mathbf{x}(0) = \\mathbf{x}_0,\\]\nwhere\n\\[\\mathbf{x}=\\begin{bmatrix}x\\\\  \\dot{x} \\end{bmatrix},\\quad\n   F=\\begin{bmatrix}0 & 1\\\\  0 & 0 \\end{bmatrix},\\quad\n   L=\\begin{bmatrix}0\\\\  1  \\end{bmatrix},\\quad\n   \\mathbf{w}=w.\\]\nIn discrete-time, we introduce a time-step, \\(\\Delta t,\\) that is sometimes just set equal to one, and obtain the discrete, state-space formulation\n\\[ \\mathbf{x}_{k+1 } = F  \\mathbf{x}_{k} + L w_k, \\]\nwhere\n\\[\n   F=\\begin{bmatrix}1 & \\Delta t\\\\  0 & 1 \\end{bmatrix},\\quad\n   L=\\begin{bmatrix} 1 \\\\  0  \\end{bmatrix}.\\]\nWe begin by simulating the motion and generating noisy measurements of the position.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n# Set the random seed for reproducibility\nnp.random.seed(42)\n\n# Simulate the ground truth position of the object\ntrue_velocity = 0.5  # units per time step\nnum_steps = 50\ntime_steps = np.linspace(0, num_steps, num_steps)\ntrue_positions = true_velocity * time_steps\n\n# Simulate the measurements with noise\nmeasurement_noise = 5 #1#0  # increase this value to make measurements noisier\nnoisy_measurements = true_positions + np.random.normal(0, measurement_noise, num_steps)\n\n# Plot the true positions and the noisy measurements\nplt.figure(figsize=(10, 6))\nplt.plot(time_steps, true_positions, label='True Position', color='green')\nplt.scatter(time_steps, noisy_measurements, label='Noisy Measurements', color='red', marker='o')\n\nplt.xlabel('Time Step')\nplt.ylabel('Position')\nplt.title('True Position and Noisy Measurements Over Time')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nNow, we set up the inputs for the Kalman filter, run it, and plot the results of the estimation.\nThe process noise covariance matrix, \\(Q,\\) is taken here as\n\\[\n  Q = \\begin{bmatrix}1 & 0\\\\  0 & \\sigma_w^2 \\end{bmatrix},\n\\]\nwhere we have supposed that\n\\[\n  w \\sim \\mathcal{N} (0, \\sigma_w^2) .\n\\]\nThis form for \\(Q\\) can be derived from the discretization of a Wiener process, as shown in {cite}Sarkka2023. We will consider a more general case in Example 4, below.\n\nsig_v = measurement_noise\nsig_w = 0.1 # np.sqrt(3)  # process noise\n# Kalman Filter Initialization\nF = np.array([[1, 1], [0, 1]])        # State transition matrix\nH = np.array([[1, 0]])                # Measurement function\nQ = np.array([[1, 0], [0, sig_w**2]]) # Process noise covariance\nR = np.array([[sig_v**2]])            # Measurement noise covariance\nx0 = np.array([[0], [0]])             # Initial state estimate\nP0 = np.array([[1, 0], [0, 1]])       # Initial estimate covariance\n\n# Initialize the state estimate and covariance\nnd = 2\nT = 50\n\nx_hat = np.zeros((T, nd, 1))\nP     = np.zeros((T, nd, nd))\n\nx_hat[0] = x0\nP[0] = P0 #Q\ny = noisy_measurements\n\n# Run the Kalman filter\nfor t in range(1, T):\n    # Prediction step\n    x_hat[t] = F @ x_hat[t-1]\n    P[t] = F @ P[t-1] @ F.T + Q\n\n    # Update step\n    K = P[t] @ H.T @ np.linalg.inv(H @ P[t] @ H.T + R)\n    x_hat[t] = x_hat[t] + K @ (y[t] - H @ x_hat[t])\n    P[t] = (np.eye(nd) - K @ H) @ P[t]\n\n\n# Plot the true positions, noisy measurements, and the Kalman filter estimates\ntime_steps = range(T)\nestimated_positions  = x_hat[:,0,:]\nestimated_velocities = x_hat[:,1,:]\nplt.figure(figsize=(10, 6))\nplt.plot(time_steps, true_positions, label='True Position $x$', color='green')\nplt.scatter(time_steps, noisy_measurements, label='Noisy Measurements $y$', color='red', marker='o')\nplt.plot(time_steps, estimated_positions, label='Kalman Filter Estimate of $x$', color='blue')\nplt.axhline(0.5,color='k',label='True velocity')\nplt.plot(time_steps, estimated_velocities, label='Kalman Filter Estimate of $\\dot{x}$', color='orange')\nplt.xlabel('Time Step')\nplt.ylabel('Position')\nplt.title('Kalman Filter Estimation Over Time')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nAsch, Mark. 2022. A Toolbox for Digital Twins: From Model-Based to Data-Driven. Philadelphia, PA: Society for Industrial; Applied Mathematics. https://doi.org/10.1137/1.9781611976977.\n\n\nAsch, Mark, Marc Bocquet, and Maëlle Nodet. 2016. Data Assimilation: Methods, Algorithms, and Applications. Philadelphia, PA: Society for Industrial; Applied Mathematics. https://doi.org/10.1137/1.9781611974546.\n\n\nCalvello, Edoardo, Sebastian Reich, and Andrew M. Stuart. 2022. “Ensemble Kalman Methods: A Mean Field Perspective.” arXiv (to appear in Acta Numerica 2025). http://arxiv.org/abs/2209.11371.\n\n\nCarrillo, J. A., F. Hoffmann, A. M. Stuart, and U. Vaes. 2024a. “Statistical Accuracy of Approximate Filtering Methods.” https://arxiv.org/abs/2402.01593.\n\n\n———. 2024b. “The Mean Field Ensemble Kalman Filter: Near-Gaussian Setting.” https://arxiv.org/abs/2212.13239.\n\n\nDashti, Masoumeh, and Andrew M. Stuart. 2015. “The Bayesian Approach to Inverse Problems.” In Handbook of Uncertainty Quantification, edited by Roger Ghanem, David Higdon, and Houman Owhadi, 1–118. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-319-11259-6_7-1.\n\n\nHuang, Daniel Zhengyu, Jiaoyang Huang, Sebastian Reich, and Andrew M Stuart. 2022. “Efficient Derivative-Free Bayesian Inference for Large-Scale Inverse Problems.” Inverse Problems 38 (12): 125006. https://doi.org/10.1088/1361-6420/ac99fa.\n\n\nIglesias, Marco A, Kody J H Law, and Andrew M Stuart. 2013. “Ensemble Kalman Methods for Inverse Problems.” Inverse Problems 29 (4): 045001. https://doi.org/10.1088/0266-5611/29/4/045001.\n\n\nLaw, Kody, Andrew Stuart, and Konstantinos Zygalakis. 2015. Data Assimilation: A Mathematical Introduction. Vol. 62. Texts in Applied Mathematics. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-319-20325-6.\n\n\nSanita Vetra-Carvalho, Lars Nerger, Peter Jan van Leeuwen, and Jean-Marie Beckers. 2018. “State-of-the-Art Stochastic Data Assimilation Methods for High-Dimensional Non-Gaussian Problems.” Tellus A: Dynamic Meteorology and Oceanography 70 (1): 1–43. https://doi.org/10.1080/16000870.2018.1445364.\n\n\nSärkkä, S., and L. Svensson. 2023. Bayesian Filtering and Smoothing. 2nd ed. Institute of Mathematical Statistics Textbooks. Cambridge University Press. https://doi.org/10.1017/9781108917407.\n\n\nWu, Jin-Long, Matthew E. Levine, Tapio Schneider, and Andrew Stuart. 2023. “Learning about Structural Errors in Models of Complex Dynamical Systems.” https://arxiv.org/abs/2401.00035.",
    "crumbs": [
      "Kalman Filters",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Example 3 - constant-velocity 1D position tracking</span>"
    ]
  },
  {
    "objectID": "02examples/KF/021KFExample4.html",
    "href": "02examples/KF/021KFExample4.html",
    "title": "7  Example 4 - constant-velocity 2D motion tracking",
    "section": "",
    "text": "7.1 State-space model\nAssuming the above dynamics, and adding a noisy position measurement model, we obtain the linear state-space model\n\\[\\begin{align}\n  \\mathbf{x}_{k+1 } &= F  \\mathbf{x}_{k} + \\mathbf{q}_{k}, \\quad \\mathbf{q}_{k} \\sim \\mathcal{N} (0, Q), \\\\\n   \\mathbf{y}_{k+1 } &= H \\mathbf{x}_{k} + \\mathbf{r}_{k}, \\quad \\mathbf{r}_{k} \\sim \\mathcal{N} (0, R),\n\\end{align}\\]\nwith\n\\[\n  H = \\begin{bmatrix} 1 & 0 & 0 & 0\\\\  0 & 1 & 0 & 0 \\end{bmatrix}, \\quad\n  R = \\begin{bmatrix} \\sigma_1^2 & 0 \\\\  0 & \\sigma_2^2  \\end{bmatrix}  .\n\\]\nWe are now ready to simulate the Kalman filter.\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy import linalg\n# we use Sarkka's utilities to streamline a bit...\nfrom common_utilities import generate_ssm, RandomState, rmse, plot_car_trajectory\n# initialize\nq = 1.    # process noise\ndt = 0.1  # time step\ns = 0.5   # measurement noise\n\nM = 4  # State dimension\nN = 2  # Observation dimension\n\nA = np.array([[1, 0, dt, 0],\n              [0, 1, 0, dt],\n              [0, 0, 1, 0],\n              [0, 0, 0, 1]])\n\nQ = q * np.array([[dt ** 3 / 3, 0, dt ** 2 / 2, 0],\n                  [0, dt **3 / 3, 0, dt ** 2 / 2],\n                  [dt ** 2 / 2, 0, dt, 0],\n                  [0, dt ** 2 / 2, 0, dt]])\n\nH = np.array([[1, 0, 0, 0],\n              [0, 1, 0, 0]])\n\nR = np.array([[s ** 2, 0],\n              [0, s ** 2]])\n\nx_0 = np.array([0., 0., 1., -1.])\n# Simulate trajectory and noisy measurements\nrandom_state = RandomState(6)\nsteps = 100\n\nstates, observations = generate_ssm(x_0, A, Q, H, R, steps, random_state)\n\nplot_car_trajectory(observations, states, \"Trajectory\")",
    "crumbs": [
      "Kalman Filters",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Example 4 - constant-velocity 2D motion tracking</span>"
    ]
  },
  {
    "objectID": "02examples/KF/021KFExample4.html#state-space-model",
    "href": "02examples/KF/021KFExample4.html#state-space-model",
    "title": "7  Example 4 - constant-velocity 2D motion tracking",
    "section": "",
    "text": "definition of all parameters\ninitilization of all matrices\nsimulation and generate noisy measurements - plot\nKalman filter\nKalman smoother",
    "crumbs": [
      "Kalman Filters",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Example 4 - constant-velocity 2D motion tracking</span>"
    ]
  },
  {
    "objectID": "02examples/KF/021KFExample4.html#kalman-filter",
    "href": "02examples/KF/021KFExample4.html#kalman-filter",
    "title": "7  Example 4 - constant-velocity 2D motion tracking",
    "section": "7.2 Kalman filter",
    "text": "7.2 Kalman filter\n\ndef kalman_filter(m_0, P_0, A, Q, H, R, observations):\n    M = m_0.shape[-1]\n    steps, N = observations.shape\n    \n    kf_m = np.empty((steps, M))\n    kf_P = np.empty((steps, M, M))\n    \n    m = m_0\n    P = P_0\n    \n    for i in range(steps):\n        y = observations[i]\n        m = A @ m\n        P = A @ P @ A.T + Q\n        \n        S = H @ P @ H.T + R\n        # More efficient and stable way of computing K = P @ H.T @ linalg.inv(S)\n        # This also leverages the fact that S is known to be a positive definite matrix (assume_a=\"pos\")\n        K = linalg.solve(S.T, H @ P, assume_a=\"pos\").T \n        \n        m = m + K @ (y - H @ m)\n        P = P - K @ S @ K.T\n        \n        kf_m[i] = m\n        kf_P[i] = P\n    return kf_m, kf_P\n\n\nm_0 = x_0\nP_0 = np.array([[1, 0, 0, 0],\n                [0, 1, 0, 0],\n                [0, 0, 1, 0],\n                [0, 0, 0, 1]])\n\nkf_m, kf_P = kalman_filter(m_0, P_0, A, Q, H, R, observations)\n\nplot_car_trajectory(observations, states, \"Trajectory\", kf_m, \"Filter Estimate\")\n\nrmse_raw = rmse(states[:, :2], observations)\nrmse_kf = rmse(kf_m[:, :2], states[:, :2])\nprint(f\"RAW RMSE: {rmse_raw}\")\nprint(f\"KF RMSE: {rmse_kf}\")\n\nRAW RMSE: 0.7131995943918173\nKF RMSE: 0.3746597043548562",
    "crumbs": [
      "Kalman Filters",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Example 4 - constant-velocity 2D motion tracking</span>"
    ]
  },
  {
    "objectID": "02examples/KF/021KFExample4.html#kalman-smoother",
    "href": "02examples/KF/021KFExample4.html#kalman-smoother",
    "title": "7  Example 4 - constant-velocity 2D motion tracking",
    "section": "7.3 Kalman smoother",
    "text": "7.3 Kalman smoother\nThe RTS smoother requires a forward run of the Kalman filter that provides the state and the covariance matrix, for all time steps.\n\ndef rts_smoother(kf_m, kf_P, A, Q):\n    steps, M = kf_m.shape\n    \n    rts_m = np.empty((steps, M))\n    rts_P = np.empty((steps, M, M))\n    \n    m = kf_m[-1]\n    P = kf_P[-1]\n    \n    rts_m[-1] = m\n    rts_P[-1] = P\n    \n    for i in range(steps-2, -1, -1):\n        filtered_m = kf_m[i]\n        filtered_P = kf_P[i]\n        \n        mp = A @ filtered_m\n        Pp = A @ filtered_P @ A.T + Q\n\n        # More efficient and stable way of computing Gk = filtered_P @ A.T @ linalg.inv(Pp)\n        # This also leverages the fact that Pp is known to be a positive definite matrix (assume_a=\"pos\")\n        Gk = linalg.solve(Pp, A @ filtered_P, assume_a=\"pos\").T \n\n        m = filtered_m + Gk @ (m - mp)\n        P = filtered_P + Gk @ (P - Pp) @ Gk.T\n        \n        rts_m[i] = m\n        rts_P[i] = P\n\n    return rts_m, rts_P\n\n\nrts_m, rts_P = rts_smoother(kf_m, kf_P, A, Q)\n\nplot_car_trajectory(observations, states, \"Trajectory\", rts_m, \"Smoother Estimate\")\n\nrmse_rts = rmse(states[:, :2], rts_m[:, :2])\n\nprint(f\"RAW RMSE: {rmse_raw}\")\nprint(f\"KF RMSE: {rmse_kf}\")\nprint(f\"RTS RMSE: {rmse_rts}\")\n\nRAW RMSE: 0.7131995943918173\nKF RMSE: 0.3746597043548562\nRTS RMSE: 0.1857332232186917",
    "crumbs": [
      "Kalman Filters",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Example 4 - constant-velocity 2D motion tracking</span>"
    ]
  },
  {
    "objectID": "02examples/KF/021KFExample4.html#conclusions-on-kalman-filters",
    "href": "02examples/KF/021KFExample4.html#conclusions-on-kalman-filters",
    "title": "7  Example 4 - constant-velocity 2D motion tracking",
    "section": "7.4 Conclusions on Kalman Filters",
    "text": "7.4 Conclusions on Kalman Filters\n\nworkhorse for all linear, Gaussian problems\ncases covered here:\n\ntrack a constant\ntrack a random walk\nmovement tracking: scalar consatnt velocity, 2D and 3D tracking\n\n2 basic philosophies:\n\nuse a KF classs\ninclude KF code each time\nuse KF module/function\n\nChoice: I prefer (2). Since the KF is coded in only 5 lines, there is no real need for a class and the resulting code remains very readable\nProcess noise modelling, to design the matrix \\(Q,\\) is a complex subject. See Saho and references therein.",
    "crumbs": [
      "Kalman Filters",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Example 4 - constant-velocity 2D motion tracking</span>"
    ]
  },
  {
    "objectID": "02examples/KF/021KFExample4.html#references",
    "href": "02examples/KF/021KFExample4.html#references",
    "title": "7  Example 4 - constant-velocity 2D motion tracking",
    "section": "7.5 References",
    "text": "7.5 References\n\n K. Law, A Stuart, K. Zygalakis. Data Assimilation. A Mathematical Introduction. Springer. 2015.\n M. Asch, M. Bocquet, M. Nodet. Data Assimilation: Methods, Algorithms and Applications. SIAM. 2016.\n M. Asch. A Toobox for Digital Twins. From Model-Based to Data-Driven. SIAM. 2022\n S. Sarkka, L. Svensson. Bayesian Filtering and Smoothing, 2nd ed., Cambridge University Press. 2023.\n\n\n\n\n\nAsch, Mark. 2022. A Toolbox for Digital Twins: From Model-Based to Data-Driven. Philadelphia, PA: Society for Industrial; Applied Mathematics. https://doi.org/10.1137/1.9781611976977.\n\n\nAsch, Mark, Marc Bocquet, and Maëlle Nodet. 2016. Data Assimilation: Methods, Algorithms, and Applications. Philadelphia, PA: Society for Industrial; Applied Mathematics. https://doi.org/10.1137/1.9781611974546.\n\n\nCalvello, Edoardo, Sebastian Reich, and Andrew M. Stuart. 2022. “Ensemble Kalman Methods: A Mean Field Perspective.” arXiv (to appear in Acta Numerica 2025). http://arxiv.org/abs/2209.11371.\n\n\nCarrillo, J. A., F. Hoffmann, A. M. Stuart, and U. Vaes. 2024a. “Statistical Accuracy of Approximate Filtering Methods.” https://arxiv.org/abs/2402.01593.\n\n\n———. 2024b. “The Mean Field Ensemble Kalman Filter: Near-Gaussian Setting.” https://arxiv.org/abs/2212.13239.\n\n\nDashti, Masoumeh, and Andrew M. Stuart. 2015. “The Bayesian Approach to Inverse Problems.” In Handbook of Uncertainty Quantification, edited by Roger Ghanem, David Higdon, and Houman Owhadi, 1–118. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-319-11259-6_7-1.\n\n\nHuang, Daniel Zhengyu, Jiaoyang Huang, Sebastian Reich, and Andrew M Stuart. 2022. “Efficient Derivative-Free Bayesian Inference for Large-Scale Inverse Problems.” Inverse Problems 38 (12): 125006. https://doi.org/10.1088/1361-6420/ac99fa.\n\n\nIglesias, Marco A, Kody J H Law, and Andrew M Stuart. 2013. “Ensemble Kalman Methods for Inverse Problems.” Inverse Problems 29 (4): 045001. https://doi.org/10.1088/0266-5611/29/4/045001.\n\n\nLaw, Kody, Andrew Stuart, and Konstantinos Zygalakis. 2015. Data Assimilation: A Mathematical Introduction. Vol. 62. Texts in Applied Mathematics. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-319-20325-6.\n\n\nSanita Vetra-Carvalho, Lars Nerger, Peter Jan van Leeuwen, and Jean-Marie Beckers. 2018. “State-of-the-Art Stochastic Data Assimilation Methods for High-Dimensional Non-Gaussian Problems.” Tellus A: Dynamic Meteorology and Oceanography 70 (1): 1–43. https://doi.org/10.1080/16000870.2018.1445364.\n\n\nSärkkä, S., and L. Svensson. 2023. Bayesian Filtering and Smoothing. 2nd ed. Institute of Mathematical Statistics Textbooks. Cambridge University Press. https://doi.org/10.1017/9781108917407.\n\n\nWu, Jin-Long, Matthew E. Levine, Tapio Schneider, and Andrew Stuart. 2023. “Learning about Structural Errors in Models of Complex Dynamical Systems.” https://arxiv.org/abs/2401.00035.",
    "crumbs": [
      "Kalman Filters",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Example 4 - constant-velocity 2D motion tracking</span>"
    ]
  },
  {
    "objectID": "01theory/022NlinKF.html",
    "href": "01theory/022NlinKF.html",
    "title": "8  Nonlinear Kalman Filters",
    "section": "",
    "text": "8.1 Introduction\nRecall that the Kalman filter can be used for\nKalman filters are linear (and Gaussian) or nonlinear. Here we will formulate the basic nonliner filter, known as the extended Kalman filter.",
    "crumbs": [
      "Nonlinear Kalman Filters",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Nonlinear Kalman Filters</span>"
    ]
  },
  {
    "objectID": "01theory/022NlinKF.html#introduction",
    "href": "01theory/022NlinKF.html#introduction",
    "title": "8  Nonlinear Kalman Filters",
    "section": "",
    "text": "state estimation—this is the direct filtering (or smoothing) problem\nparameter estimation—this is the inverse problem based on filtering with a pseudo-time",
    "crumbs": [
      "Nonlinear Kalman Filters",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Nonlinear Kalman Filters</span>"
    ]
  },
  {
    "objectID": "01theory/022NlinKF.html#recall-kalman-filter-problem---general-formulation",
    "href": "01theory/022NlinKF.html#recall-kalman-filter-problem---general-formulation",
    "title": "8  Nonlinear Kalman Filters",
    "section": "8.2 Recall: Kalman filter problem - general formulation",
    "text": "8.2 Recall: Kalman filter problem - general formulation\nWe present a very general formulation that will later be convenient for joint state and parameter estimation problems. Consider a discrete-time nonlinear dynamical system with noisy state transitions and noisy observations that are also noinlinear.\nWe consider a discrete-time dynamical system with noisy state transitions and noisy observations.\n\nDynamics:\n\\[v_{j+1} = \\Psi(v_j) + \\xi_j, \\quad j \\in \\mathbb{Z}^+\\]\nObservations:\n\\[y_{j+1} = h (v_{j+1}) + \\eta_{j+1}, \\quad j \\in \\mathbb{Z}^+\\]\nProbability (densities):\n\\[v_0 \\sim \\mathcal{N}(m_0,C_0), \\quad \\xi_j \\sim \\mathcal{N}(0,\\Sigma), \\quad \\eta_j \\sim \\mathcal{N}(0,\\Gamma)\\]\nProbability (independence):\n\\[v_0 \\perp {\\xi_j} \\perp {\\eta_j}\\]\nOperators:\n\\[\\begin{eqnarray}\n    \\Psi \\colon \\mathcal{H}_s &\\mapsto \\mathcal{H}_s, \\\\\n    h  \\colon \\mathcal{H}_s &\\mapsto \\mathcal{H}_o,\n   \\end{eqnarray}\\] where \\(v_j \\in \\mathcal{H}_s,\\) \\(y_j \\in \\mathcal{H}_o\\) and \\(\\mathcal{H}\\) is a finite-dimensional Hilbert space.\n\nFiltering problem:\nEstimate (optimally) the state \\(v_j\\) of the dynamical system at time \\(j,\\) given the data \\(Y_j = \\{y_i\\}_{i=1}^{j}\\) up to time \\(j.\\) This is achieved by using a two-step predictor-corrector method. We will use the more general notation of (Law, Stuart, and Zygalakis 2015) instead of the usual, classical state-space formulation that is used in (Asch, Bocquet, and Nodet 2016) and (Asch 2022).\nThe objective here is to update the filtering distribution \\(\\mathbb{P}(v_j \\vert Y_j),\\) from time \\(j\\) to time \\(j+1,\\) in the nonlinear, Gaussian case, where\n\n\\(\\Psi\\) and \\(h\\) are nonlinear functions,\nall distributions are Gaussian.\n\nSuppose the Jacobian matrices of \\(\\Psi\\) and \\(h\\) exist, and are denoted by\n\\[\\begin{eqnarray}\n   \\Psi_x(v) &= \\left[ \\frac{\\partial \\Psi}{\\partial x} \\right]_{x=m},\\\\\n   h_x(v)    &= \\left[ \\frac{\\partial h}{\\partial x} \\right]_{x=m}.\n\\end{eqnarray}\\]\n\nLet \\((m_j, C_j)\\) denote the mean and covariance of \\(v_j \\vert Y_j\\) and note that these entirely characterize the random variable since it is Gaussian.\nLet \\((\\hat{m}_{j+1}, \\hat{C}_{j+1})\\) denote the mean and covariance of \\(v_{j+1} \\vert Y_j\\) and note that these entirely characterize the random variable since it is Gaussian.\nDerive the map \\((m_j, C_j) \\mapsto (m_{j+1}, C_{j+1})\\) using the previous step.\n\n\n8.2.1 Prediction/Forecast\n\\[ \\mathbb{P}(v_n \\vert y_1, \\ldots, y_n) \\mapsto \\mathbb{P}(v_{n+1} \\vert y_1, \\ldots, y_n) \\]\n\nP0: initialize \\((m_0, C_0)\\) and compute \\(v_0\\)\nP1: predict the state, measurement\n\\[\\begin{align}\n   v_{j+1} &= \\Psi (v_j) + \\xi_j \\\\\n   y_{j+1} &= h (v_{j+1}) + \\eta_{j+1}\n   \\end{align}\\]\nP2: predict the mean and covariance\n\\[\\begin{align}\n   \\hat{m}_{j+1}  &= \\Psi (m_j) \\\\\n   \\hat{C}_{j+1}  &= \\Psi_x C_j \\Psi_x^{\\mathrm{T}} + \\Sigma\n   \\end{align}\\]\n\n\n\n8.2.2 Correction/Analysis\n\\[ \\mathbb{P}(v_{n+1} \\vert y_1, \\ldots, y_n) \\mapsto \\mathbb{P}(v_{n+1} \\vert y_1, \\ldots, y_{n+1}) \\]\n\nC1: compute the innovation\n\\[ d_{j+1} = y_{j+1} - h (\\hat{m}_{j+1}) \\]\nC2: compute the measurement covariance\n\\[ S_{j+1} = h_x \\hat{C}_{j+1}  h_x^{\\mathrm{T}} + \\Gamma \\]\nC3: compute the (optimal) Kalman gain\n\\[ K_{j+1} = \\hat{C}_{j+1} h_x^{\\mathrm{T}}  S_{j+1}^{-1} \\]\nC4: update/correct the mean and covariance\n\\[\\begin{align}\n   {m}_{j+1}  &= \\hat{m}_{j+1} + K_{j+1} d_{j+1}, \\\\\n   {C}_{j+1}  &= \\hat{C}_{j+1} - K_{j+1}  S_{j+1}  K_{j+1}^{\\mathrm{T}}.\n   \\end{align}\\]\n\n\n\n8.2.3 Loop over time\n\nset \\(j = j+1\\)\ngo to step P1",
    "crumbs": [
      "Nonlinear Kalman Filters",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Nonlinear Kalman Filters</span>"
    ]
  },
  {
    "objectID": "01theory/022NlinKF.html#state-space-formulation",
    "href": "01theory/022NlinKF.html#state-space-formulation",
    "title": "8  Nonlinear Kalman Filters",
    "section": "8.3 State-space formulation",
    "text": "8.3 State-space formulation\nIn classical filter theory, a state space formulation is usually used.\n\\[\\begin{eqnarray}\n&& x_{k+1}  = f (x_k)  + w_k \\\\\n&& y_{k+1}  = h (x_k) + v_k,\n\\end{eqnarray}\\]\nwhere \\(f\\) and \\(h\\) are nonlinear, differentiable functions with Jacobian matrices \\(D_f\\) and \\(D_h\\) respectively, \\(w_k \\sim \\mathcal{N}(0,Q),\\) \\(v_k \\sim \\mathcal{N}(0,R).\\)\nThe 2-step filter:\n\n8.3.1 Initialization\n\\[ x_0, \\quad P_0 \\]\n\n\n8.3.2 1. Prediction\n\\[\\begin{eqnarray}\n&& x_{k+1}^- = f (x_k) \\\\\n&& P_{k+1}^- = D_f P_k D_f^{\\mathrm{T}} + Q\n\\end{eqnarray}\\]\n\n\n8.3.3 2. Correction\n\\[\\begin{eqnarray}\n  K_{k+1} && = P_{k+1}^{-} D_h^{\\mathrm{T}} ( D_h  P_{k+1}^{-} D_h^{\\mathrm{T}} + R )^{-1}\n                 \\quad (= P_{k+1}^- D_h^{\\mathrm{T}} S^{-1}) \\\\\n  x_{k+1} &&= x_{k+1}^{-} + K_{k+1} (y_{k+1} - h (x_{k+1}^-) ) \\\\\n  P_{k+1} &&= (I -  K_{k+1} D_h ) P_{k+1}^-\n            \\quad (= P_{k+1}^- - K_{k+1} S K^{\\mathrm{T}}_{k+1})\n\\end{eqnarray}\\]\n\n\n8.3.4 Loop\nSet \\(k = k+1\\) and go to step 1.",
    "crumbs": [
      "Nonlinear Kalman Filters",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Nonlinear Kalman Filters</span>"
    ]
  },
  {
    "objectID": "01theory/022NlinKF.html#other-nonlinear-filters",
    "href": "01theory/022NlinKF.html#other-nonlinear-filters",
    "title": "8  Nonlinear Kalman Filters",
    "section": "8.4 Other nonlinear filters",
    "text": "8.4 Other nonlinear filters\n\nunscented Kalman filter\nparticle filter\n\nFor details, please consult the references.\n\n\n\n\nAsch, Mark. 2022. A Toolbox for Digital Twins: From Model-Based to Data-Driven. Philadelphia, PA: Society for Industrial; Applied Mathematics. https://doi.org/10.1137/1.9781611976977.\n\n\nAsch, Mark, Marc Bocquet, and Maëlle Nodet. 2016. Data Assimilation: Methods, Algorithms, and Applications. Philadelphia, PA: Society for Industrial; Applied Mathematics. https://doi.org/10.1137/1.9781611974546.\n\n\nCalvello, Edoardo, Sebastian Reich, and Andrew M. Stuart. 2022. “Ensemble Kalman Methods: A Mean Field Perspective.” arXiv (to appear in Acta Numerica 2025). http://arxiv.org/abs/2209.11371.\n\n\nCarrillo, J. A., F. Hoffmann, A. M. Stuart, and U. Vaes. 2024a. “Statistical Accuracy of Approximate Filtering Methods.” https://arxiv.org/abs/2402.01593.\n\n\n———. 2024b. “The Mean Field Ensemble Kalman Filter: Near-Gaussian Setting.” https://arxiv.org/abs/2212.13239.\n\n\nDashti, Masoumeh, and Andrew M. Stuart. 2015. “The Bayesian Approach to Inverse Problems.” In Handbook of Uncertainty Quantification, edited by Roger Ghanem, David Higdon, and Houman Owhadi, 1–118. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-319-11259-6_7-1.\n\n\nHuang, Daniel Zhengyu, Jiaoyang Huang, Sebastian Reich, and Andrew M Stuart. 2022. “Efficient Derivative-Free Bayesian Inference for Large-Scale Inverse Problems.” Inverse Problems 38 (12): 125006. https://doi.org/10.1088/1361-6420/ac99fa.\n\n\nIglesias, Marco A, Kody J H Law, and Andrew M Stuart. 2013. “Ensemble Kalman Methods for Inverse Problems.” Inverse Problems 29 (4): 045001. https://doi.org/10.1088/0266-5611/29/4/045001.\n\n\nLaw, Kody, Andrew Stuart, and Konstantinos Zygalakis. 2015. Data Assimilation: A Mathematical Introduction. Vol. 62. Texts in Applied Mathematics. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-319-20325-6.\n\n\nSanita Vetra-Carvalho, Lars Nerger, Peter Jan van Leeuwen, and Jean-Marie Beckers. 2018. “State-of-the-Art Stochastic Data Assimilation Methods for High-Dimensional Non-Gaussian Problems.” Tellus A: Dynamic Meteorology and Oceanography 70 (1): 1–43. https://doi.org/10.1080/16000870.2018.1445364.\n\n\nSärkkä, S., and L. Svensson. 2023. Bayesian Filtering and Smoothing. 2nd ed. Institute of Mathematical Statistics Textbooks. Cambridge University Press. https://doi.org/10.1017/9781108917407.\n\n\nWu, Jin-Long, Matthew E. Levine, Tapio Schneider, and Andrew Stuart. 2023. “Learning about Structural Errors in Models of Complex Dynamical Systems.” https://arxiv.org/abs/2401.00035.",
    "crumbs": [
      "Nonlinear Kalman Filters",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Nonlinear Kalman Filters</span>"
    ]
  },
  {
    "objectID": "02examples/EKF/022EKFExample1.html",
    "href": "02examples/EKF/022EKFExample1.html",
    "title": "9  Example 1 - tracking a random sinusoidal signal",
    "section": "",
    "text": "Before introducing more general nonlinear dyanmic systems (see the following examples), let us consider a simple case of tracking a random sine signal whose angular velocity and amplitude vary, randomly, over time. We express the nonlinearity through the measurement model, though this could be done with the dynamic model, as in the examples below.\nLet the state vector be\n\\[\n  \\mathbf{x}_k = \\left[\\begin{array}{c}\n        \\theta_k\\\\\n        \\omega_k \\\\\n        a_k\n    \\end{array}\\right],\n\\]\nwhere the \\(\\theta\\) is the angle, \\(\\omega\\) the angular velocity and \\(a\\) the amplitude of the sine function. The model is then\n\\[\\begin{align}\n\\frac{\\mathrm{d} \\theta}{\\mathrm{d}t} &=  \\omega \\\\\n\\frac{\\mathrm{d} \\omega}{\\mathrm{d}t} &=  w_{\\omega}(t) \\\\\n\\frac{\\mathrm{d} a}{\\mathrm{d}t} &=  w_a(t)\n\\end{align}\\]\nIn matrix form, we have\n\\[\n  \\frac{\\mathrm{d}\\mathbf{x}}{\\mathrm{d}t} = F \\mathbf{x}(t) + L \\mathbf{w}(t) , \\quad \\mathbf{x}(0) = \\mathbf{x}_0,\n\\]\nwhere\n\\[\nF = \\begin{bmatrix} 0 & 1 & 0\\\\ 0 & 0 & 0 \\\\  0 & 0 & 0 \\end{bmatrix}, \\quad\nL =  \\begin{bmatrix} 0 & 0\\\\ 1 & 0 \\\\ 0 & 1  \\end{bmatrix}, \\quad\n\\mathbf{w}(t) =  \\begin{bmatrix} w_\\omega (t)\\\\ w_a(t) \\end{bmatrix},\n\\]\nand the power spectral density of the white noise process \\(\\mathbf{w}(t)\\) is\n\\[\nQ^c = \\begin{bmatrix} q_1 & 1\\\\ 0 & q_2 \\end{bmatrix}.\n\\]\nUsing the matrix exponential formulas (see KF-tutorial), we get the discrete system\n\\[\\begin{align}\n\\mathbf{x}_{k+1 } &= A_k  \\mathbf{x}_{k} + \\mathbf{q}_{k}, \\\\\ny_k &= a_k \\sin \\theta_k + r_k\n\\end{align}\\]\nwhere \\(r_k \\sim \\mathcal{N}(0, \\sigma_r)\\) is univariate Gaussian noise,\n\\[\nA_k = \\begin{bmatrix} 0 & \\Delta t & 0\\\\ 0 & 1 & 0 \\\\  0 & 0 & 1 \\end{bmatrix},\n\\]\nand \\(\\mathbf{q}_{k} \\sim \\mathcal{N}(0, Q_k)\\) with\n\\[\nQ_k = \\begin{bmatrix} q_1 \\Delta t^3 / 3   & q_1 \\Delta t^2 / 2  & 0 \\\\\n        q_1 \\Delta t^2 / 2  &  q_1 \\Delta t  & 0 \\\\\n        0 &   0 &  q_2 \\Delta t \\end{bmatrix}.\n\\]\nFinally, the Jacobian of \\(h(\\mathbf{x}_{k}) = a_k \\sin \\theta_k\\) is easily seen to be\n\\[\n  Dh_k = [ a_k \\cos \\theta_k \\quad 0 \\quad \\sin \\theta_k ]\n\\]\n\n\n\n\nAsch, Mark. 2022. A Toolbox for Digital Twins: From Model-Based to Data-Driven. Philadelphia, PA: Society for Industrial; Applied Mathematics. https://doi.org/10.1137/1.9781611976977.\n\n\nAsch, Mark, Marc Bocquet, and Maëlle Nodet. 2016. Data Assimilation: Methods, Algorithms, and Applications. Philadelphia, PA: Society for Industrial; Applied Mathematics. https://doi.org/10.1137/1.9781611974546.\n\n\nCalvello, Edoardo, Sebastian Reich, and Andrew M. Stuart. 2022. “Ensemble Kalman Methods: A Mean Field Perspective.” arXiv (to appear in Acta Numerica 2025). http://arxiv.org/abs/2209.11371.\n\n\nCarrillo, J. A., F. Hoffmann, A. M. Stuart, and U. Vaes. 2024a. “Statistical Accuracy of Approximate Filtering Methods.” https://arxiv.org/abs/2402.01593.\n\n\n———. 2024b. “The Mean Field Ensemble Kalman Filter: Near-Gaussian Setting.” https://arxiv.org/abs/2212.13239.\n\n\nDashti, Masoumeh, and Andrew M. Stuart. 2015. “The Bayesian Approach to Inverse Problems.” In Handbook of Uncertainty Quantification, edited by Roger Ghanem, David Higdon, and Houman Owhadi, 1–118. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-319-11259-6_7-1.\n\n\nHuang, Daniel Zhengyu, Jiaoyang Huang, Sebastian Reich, and Andrew M Stuart. 2022. “Efficient Derivative-Free Bayesian Inference for Large-Scale Inverse Problems.” Inverse Problems 38 (12): 125006. https://doi.org/10.1088/1361-6420/ac99fa.\n\n\nIglesias, Marco A, Kody J H Law, and Andrew M Stuart. 2013. “Ensemble Kalman Methods for Inverse Problems.” Inverse Problems 29 (4): 045001. https://doi.org/10.1088/0266-5611/29/4/045001.\n\n\nLaw, Kody, Andrew Stuart, and Konstantinos Zygalakis. 2015. Data Assimilation: A Mathematical Introduction. Vol. 62. Texts in Applied Mathematics. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-319-20325-6.\n\n\nSanita Vetra-Carvalho, Lars Nerger, Peter Jan van Leeuwen, and Jean-Marie Beckers. 2018. “State-of-the-Art Stochastic Data Assimilation Methods for High-Dimensional Non-Gaussian Problems.” Tellus A: Dynamic Meteorology and Oceanography 70 (1): 1–43. https://doi.org/10.1080/16000870.2018.1445364.\n\n\nSärkkä, S., and L. Svensson. 2023. Bayesian Filtering and Smoothing. 2nd ed. Institute of Mathematical Statistics Textbooks. Cambridge University Press. https://doi.org/10.1017/9781108917407.\n\n\nWu, Jin-Long, Matthew E. Levine, Tapio Schneider, and Andrew Stuart. 2023. “Learning about Structural Errors in Models of Complex Dynamical Systems.” https://arxiv.org/abs/2401.00035.",
    "crumbs": [
      "Nonlinear Kalman Filters",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Example 1 - tracking a random sinusoidal signal</span>"
    ]
  },
  {
    "objectID": "02examples/EKF/022EKFExample2.html",
    "href": "02examples/EKF/022EKFExample2.html",
    "title": "10  Example 2 - tracking a noisy pendulum",
    "section": "",
    "text": "10.1 Extended Kalman Filter (EKF)\ndef extended_kalman_filter(m_0, P_0, g, Q, dt, R, observations):\n    n = m_0.shape[-1]\n    steps = observations.shape[0]\n    \n    ekf_m = np.empty((steps, n))\n    ekf_P = np.empty((steps, n, n))\n    \n    m = m_0[:]\n    P = P_0[:]\n    \n    for i in range(steps):\n        y = observations[i]\n        \n        # Jacobian of the dynamic model function\n        Df = np.array([[1.,                     dt], \n                      [-g * dt * np.cos(m[0]), 1.]])\n        \n        # Predicted state distribution\n        m = np.array([m[0] + dt * m[1],\n                      m[1] - g * dt * np.sin(m[0])])\n        P = Df @ P @ Df.T + Q\n        \n        # Predicted observation\n        h  = np.sin(m[0])\n        Dh = np.array([[np.cos(m[0]), 0.]])\n        S = Dh @ P @ Dh.T + R\n        \n        # Kalman Gain\n        K = linalg.solve(S, Dh @ P, assume_a=\"pos\").T \n        m = m + K @ np.atleast_1d(y - h)\n        P = P - K @ S @ K.T\n        \n        ekf_m[i] = m\n        ekf_P[i] = P\n    return ekf_m, ekf_P\n# initialize state and covariance\nm_0 = np.array([1.6, 0.])  # Slightly off\nP_0 = np.array([[0.1, 0.], \n                [0., 0.1]])\n\nekf_m, ekf_P = extended_kalman_filter(m_0, P_0, g, Q, dt, R, observations)\nplot_pendulum(timeline, observations, states, \"Trajectory\", ekf_m, \"EKF Estimate\")\n\nrmse_ekf = rmse(ekf_m[:, :1], states[:, :1])\nprint(f\"EKF RMSE: {rmse_ekf}\")\n\nEKF RMSE: 0.10306106181239276\n# plot covariances\nplt.plot(timeline, ekf_P[:,0,0], timeline,ekf_P[:,1,1] )\nplt.legend(['P(1,1)', 'P(2,2)'])\nplt.show()",
    "crumbs": [
      "Nonlinear Kalman Filters",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Example 2 - tracking a noisy pendulum</span>"
    ]
  },
  {
    "objectID": "02examples/EKF/022EKFExample2.html#extended-smoother",
    "href": "02examples/EKF/022EKFExample2.html#extended-smoother",
    "title": "10  Example 2 - tracking a noisy pendulum",
    "section": "10.2 Extended Smoother",
    "text": "10.2 Extended Smoother\n\ndef extended_smoother(ekf_m, ekf_P, g, Q, dt):    \n    steps, M = ekf_m.shape\n    \n    rts_m = np.empty((steps, M))\n    rts_P = np.empty((steps, M, M))\n    \n    m = ekf_m[-1]\n    P = ekf_P[-1]\n    \n    rts_m[-1] = m\n    rts_P[-1] = P\n    \n    for i in range(steps-2, -1, -1):\n        filtered_m = ekf_m[i]\n        filtered_P = ekf_P[i]\n        \n        Df = np.array([[1., dt], \n                      [-g * dt * np.cos(filtered_m[0]), 1.]])\n        \n        mp = np.array([filtered_m[0] + dt * filtered_m[1], \n                       filtered_m[1] - g * dt * np.sin(filtered_m[0])])\n        Pp = Df @ filtered_P @ Df.T + Q\n        \n        # More efficient and stable way of computing Gk = filtered_P @ A.T @ linalg.inv(Pp)\n        # This also leverages the fact that Pp is known to be a positive definite matrix (assume_a=\"pos\")\n        Gk = linalg.solve(Pp, Df @ filtered_P, assume_a=\"pos\").T \n\n        m = filtered_m + Gk @ (m - mp)\n        P = filtered_P + Gk @ (P - Pp) @ Gk.T\n        \n        rts_m[i] = m\n        rts_P[i] = P\n\n    return rts_m, rts_P\n\n\nrts_m, rts_P = extended_smoother(ekf_m, ekf_P, g, Q, dt)\nplot_pendulum(timeline, observations, states, \"Trajectory\", rts_m, \"Extended Smoother Estimate\")\nrmse_erts = rmse(rts_m[:, :1], states[:, :1])\nprint(f\"KF RMSE: {rmse_ekf}\")\nprint(f\"ERTS RMSE: {rmse_erts}\")\n\nKF RMSE: 0.10306106181239276\nERTS RMSE: 0.027612762479911554",
    "crumbs": [
      "Nonlinear Kalman Filters",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Example 2 - tracking a noisy pendulum</span>"
    ]
  },
  {
    "objectID": "02examples/EKF/022EKFExample2.html#conclusions-on-extended-kalman-filters",
    "href": "02examples/EKF/022EKFExample2.html#conclusions-on-extended-kalman-filters",
    "title": "10  Example 2 - tracking a noisy pendulum",
    "section": "10.3 Conclusions on Extended Kalman Filters",
    "text": "10.3 Conclusions on Extended Kalman Filters\nThe pros and cons of the EKF are:\n\nPros:\n\nRelative simplicity, based on well-known linearization methods.\nMaintains the simple, elegant, and computationally efficient KF update equations.\nGood performance for such a simple method.\nAbility to treat nonlinear process and observation models.\nAbility to treat both additive and more general nonlinear Gaussian noise.\n\nCons:\n\nPerformance can suffer in presence of strong nonlinearity because of the local validity of the linear approximation (valid for small perturbations around the linear term).\nCannot deal with non-Gaussian noise, such as discrete-valued random variables.\nRequires differentiable process and measurement operators and evaluation of Jacobian matrices, which might be problematic in very high dimensions.\n\n\nIn spite of this, the EKF remains a solid filter and remains the basis of most GPS and navigation systems.",
    "crumbs": [
      "Nonlinear Kalman Filters",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Example 2 - tracking a noisy pendulum</span>"
    ]
  },
  {
    "objectID": "02examples/EKF/022EKFExample2.html#references",
    "href": "02examples/EKF/022EKFExample2.html#references",
    "title": "10  Example 2 - tracking a noisy pendulum",
    "section": "10.4 References",
    "text": "10.4 References\n\n K. Law, A Stuart, K. Zygalakis. Data Assimilation. A Mathematical Introduction. Springer. 2015.\n M. Asch, M. Bocquet, M. Nodet. Data Assimilation: Methods, Algorithms and Applications. SIAM. 2016.\n M. Asch. A Toobox for Digital Twins. From Model-Based to Data-Driven. SIAM. 2022\n S. Sarkka, L. Svensson. Bayesian Filtering and Smoothing, 2nd ed., Cambridge University Press. 2023.\n\n\n\n\n\nAsch, Mark. 2022. A Toolbox for Digital Twins: From Model-Based to Data-Driven. Philadelphia, PA: Society for Industrial; Applied Mathematics. https://doi.org/10.1137/1.9781611976977.\n\n\nAsch, Mark, Marc Bocquet, and Maëlle Nodet. 2016. Data Assimilation: Methods, Algorithms, and Applications. Philadelphia, PA: Society for Industrial; Applied Mathematics. https://doi.org/10.1137/1.9781611974546.\n\n\nCalvello, Edoardo, Sebastian Reich, and Andrew M. Stuart. 2022. “Ensemble Kalman Methods: A Mean Field Perspective.” arXiv (to appear in Acta Numerica 2025). http://arxiv.org/abs/2209.11371.\n\n\nCarrillo, J. A., F. Hoffmann, A. M. Stuart, and U. Vaes. 2024a. “Statistical Accuracy of Approximate Filtering Methods.” https://arxiv.org/abs/2402.01593.\n\n\n———. 2024b. “The Mean Field Ensemble Kalman Filter: Near-Gaussian Setting.” https://arxiv.org/abs/2212.13239.\n\n\nDashti, Masoumeh, and Andrew M. Stuart. 2015. “The Bayesian Approach to Inverse Problems.” In Handbook of Uncertainty Quantification, edited by Roger Ghanem, David Higdon, and Houman Owhadi, 1–118. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-319-11259-6_7-1.\n\n\nHuang, Daniel Zhengyu, Jiaoyang Huang, Sebastian Reich, and Andrew M Stuart. 2022. “Efficient Derivative-Free Bayesian Inference for Large-Scale Inverse Problems.” Inverse Problems 38 (12): 125006. https://doi.org/10.1088/1361-6420/ac99fa.\n\n\nIglesias, Marco A, Kody J H Law, and Andrew M Stuart. 2013. “Ensemble Kalman Methods for Inverse Problems.” Inverse Problems 29 (4): 045001. https://doi.org/10.1088/0266-5611/29/4/045001.\n\n\nLaw, Kody, Andrew Stuart, and Konstantinos Zygalakis. 2015. Data Assimilation: A Mathematical Introduction. Vol. 62. Texts in Applied Mathematics. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-319-20325-6.\n\n\nSanita Vetra-Carvalho, Lars Nerger, Peter Jan van Leeuwen, and Jean-Marie Beckers. 2018. “State-of-the-Art Stochastic Data Assimilation Methods for High-Dimensional Non-Gaussian Problems.” Tellus A: Dynamic Meteorology and Oceanography 70 (1): 1–43. https://doi.org/10.1080/16000870.2018.1445364.\n\n\nSärkkä, S., and L. Svensson. 2023. Bayesian Filtering and Smoothing. 2nd ed. Institute of Mathematical Statistics Textbooks. Cambridge University Press. https://doi.org/10.1017/9781108917407.\n\n\nWu, Jin-Long, Matthew E. Levine, Tapio Schneider, and Andrew Stuart. 2023. “Learning about Structural Errors in Models of Complex Dynamical Systems.” https://arxiv.org/abs/2401.00035.",
    "crumbs": [
      "Nonlinear Kalman Filters",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Example 2 - tracking a noisy pendulum</span>"
    ]
  },
  {
    "objectID": "01theory/031EnKF.html",
    "href": "01theory/031EnKF.html",
    "title": "11  Ensemble Kalman Filter",
    "section": "",
    "text": "11.1 Stochastic EnKF - linear observation operator",
    "crumbs": [
      "Ensemble Filters",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Ensemble Kalman Filter</span>"
    ]
  },
  {
    "objectID": "01theory/031EnKF.html#stochastic-enkf---linear-observation-operator",
    "href": "01theory/031EnKF.html#stochastic-enkf---linear-observation-operator",
    "title": "11  Ensemble Kalman Filter",
    "section": "",
    "text": "11.1.1 Prediction/Forecast\n\\[\\begin{align}\n\\hat{v}_{k+1}^{n} &= \\Psi ( {v}_{k}^{n} ) +  \\xi_{k}^{n} , \\quad n=1, \\ldots, N, \\\\\n\\hat{m}_{k+1}     & =  \\frac{1}{N} \\sum_{i=1}^{N}   \\hat{v}_{k+1}^{n} , \\\\\n\\hat{C}_{k+1}     & = \\frac{1}{N-1} \\sum_{i=1}^{N} \\left( \\hat{v}_{k+1}^{n} - \\hat{m}_{k+1}  \\right)\n                     \\left( \\hat{v}_{k+1}^{n} - \\hat{m}_{k+1}  \\right)^{\\mathrm{T}}.\n\\end{align}\\]\n\n\n11.1.2 Correction/Analysis\n\\[\\begin{align}\n  {S}_{k+1} &= H \\hat{C}_{k+1} H^{\\mathrm{T}} + \\Gamma ,\\\\\n  {K}_{k+1} &= \\hat{C}_{k+1} H^{\\mathrm{T}} {S}_{k+1}^{-1} , \\\\\n  {y}_{k+1}^{n} &= {y}_{k+1} + \\eta^n_{k+1} , \\quad n=1, \\ldots, N, \\\\\n  {v}_{k+1}^{n} &= (I - K_{k+1}H )\\hat{v}_{k+1}^{n} + K_{k+1} y^n_{k+1} , \\quad n=1, \\ldots, N.\n\\end{align}\\]\nAlternatively, defining the innovation \\(d = {y}_{k+1}^{n} - H \\hat{v}_{k+1}^{n},\\) we can write the state update more simply as\n\\[\n  {v}_{k+1}^{n} = \\hat{v}_{k+1}^{n} + K_{k+1} d .\n\\]\nIn words:\n\nFor a given \\(N_e \\in  \\mathbb{N}\\) generate i.i.d. ensemble of states random variables from the distribution of \\(X(0).\\)\nFor \\(t \\in  \\mathbb{N}\\) recursively repeat the following steps:\n\n\nAdvance each ensemble member in time, using the nonlinear state equation with independently generated random state noise\nCompute the forecast sample mean and the forecast sample covariance\nCompute the sample Kalman gain\nAdd additional perturbation to the observation vector \\(Y\\) using independently generated random variables \\(\\eta(t)\\)\nUpdate each forecast ensemble member\n\nBurgers et al. [1998] shows that without the data perturbation, the covariance of the ensemble would go to the zero matrix as \\(t\\) goes to infinity. The data perturbation also guarantees that the relation between the forecast sample covariance and the analysis sample covariance is analogous to the relation between the forecast and analysis covariances in the standard KF.",
    "crumbs": [
      "Ensemble Filters",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Ensemble Kalman Filter</span>"
    ]
  },
  {
    "objectID": "01theory/031EnKF.html#sec-EnKFNonLin",
    "href": "01theory/031EnKF.html#sec-EnKFNonLin",
    "title": "11  Ensemble Kalman Filter",
    "section": "11.2 Full nonlinear formulation of the ensemble Kalman filter",
    "text": "11.2 Full nonlinear formulation of the ensemble Kalman filter\nThere are many ways to formulate the EnKF. Following Vetra-Carvalho, et al (Tellus A, 2018), we express the filter in terms of the anomalies of state and observations. This is indispensable for fully nonlinear state and measurement models,\n\\[\\begin{align}\nx_{k+1}^{n} &= \\Psi ( {x}_{k}^{n} ) +  w_{k}^{n} , \\quad n=1, \\ldots, N_e, \\\\\n{y}_{k+1}    &=  \\mathcal{H} (x_{k+1}) +  v_{k+1}.\n\\end{align}\\]\nTo fix notation:\n\nstate forecast \\(X^{\\mathrm{f}},\\) dimension \\((N_t \\times N_x)\\)\nensemble state forecast \\(\\mathbf{X}^{\\mathrm{f}},\\) dimension \\((N_t \\times N_x \\times N_e)\\)\nobservation, \\(Y,\\) dimension \\((N_t \\times N_y)\\)\nensemble state anomaly, \\[\\mathbf{X}' = \\frac{1}{\\sqrt{N_e - 1}} \\left(\\mathbf{X} - \\overline{{X}}\\right),\\] dimension \\((N_t \\times N_x \\times N_e)\\) with \\(\\overline{{X}} = (1/N_e) \\sum_{e=1}^{N_e} {X}_e\\)\nensemble observation anomaly,\n\\[\\mathbf{Y}' = \\frac{1}{\\sqrt{N_e - 1}} \\left( \\mathcal{H}(\\mathbf{X}) - \\overline{ \\mathcal{H}(\\mathbf{X} ) } \\right),\\] dimension \\((N_t \\times N_y \\times N_e)\\) with \\(\\overline{ \\mathcal{H}( \\mathbf{X} )} = (1/N_e)\\sum_{e=1}^{N_e} \\mathcal{H}({X}_e).\\)\n\nThen, the Kalman analysis update is\n\\[\n\\mathbf{X}^\\mathrm{a} = \\mathbf{X}^\\mathrm{f} + \\mathbf{X}'(\\mathbf{Y}')^\\mathrm{T} S^{-1} D,\n\\]\nwith\n\\[\\begin{align}\n   S &=  \\mathbf{Y}'(\\mathbf{Y}')^\\mathrm{T} + R & \\quad \\text{(observation covariance)}, \\\\\n   D &= ( \\mathbf{Y} +  \\mathbf{y}) - \\mathcal{H}(\\mathbf{X}) & \\quad \\text{(innovation)},\n\\end{align}\\]\nwhere \\(y \\sim \\mathcal{N}(0,R)\\) is the stochastic perturbation, and \\(R\\) is the measurement noise covariance matrix.\nOr, defining the Kalman gain matrix as\n\\[\n   K = \\mathbf{X}'(\\mathbf{Y}')^\\mathrm{T} S^{-1},\n\\]\nwe obtain the classical KF update,\n\\[\n   \\mathbf{X}^\\mathrm{a} = \\mathbf{X}^\\mathrm{f}  + K D.\n\\]",
    "crumbs": [
      "Ensemble Filters",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Ensemble Kalman Filter</span>"
    ]
  },
  {
    "objectID": "01theory/031EnKF.html#summary-of-enkf-properties",
    "href": "01theory/031EnKF.html#summary-of-enkf-properties",
    "title": "11  Ensemble Kalman Filter",
    "section": "11.3 Summary of EnKF properties",
    "text": "11.3 Summary of EnKF properties\n\nEnKF represents error statistics by ensembles of (nonlinear) model and (nonlinear) measurement realizations.\nEnKF performs sequential DA that processes measurements recursively in time.\nEnKF is suitable for weather-prediction and any other complex, chaotiic dynamic systems.\nError propagation is nonlinear (see point 1).\nFilter update is linear and computed in the low rank, ensemble subspace.\nEnKF does not require any gradients, adjoints, linearizations.\n\n\n\n\n\nAsch, Mark. 2022. A Toolbox for Digital Twins: From Model-Based to Data-Driven. Philadelphia, PA: Society for Industrial; Applied Mathematics. https://doi.org/10.1137/1.9781611976977.\n\n\nAsch, Mark, Marc Bocquet, and Maëlle Nodet. 2016. Data Assimilation: Methods, Algorithms, and Applications. Philadelphia, PA: Society for Industrial; Applied Mathematics. https://doi.org/10.1137/1.9781611974546.\n\n\nCalvello, Edoardo, Sebastian Reich, and Andrew M. Stuart. 2022. “Ensemble Kalman Methods: A Mean Field Perspective.” arXiv (to appear in Acta Numerica 2025). http://arxiv.org/abs/2209.11371.\n\n\nCarrillo, J. A., F. Hoffmann, A. M. Stuart, and U. Vaes. 2024a. “Statistical Accuracy of Approximate Filtering Methods.” https://arxiv.org/abs/2402.01593.\n\n\n———. 2024b. “The Mean Field Ensemble Kalman Filter: Near-Gaussian Setting.” https://arxiv.org/abs/2212.13239.\n\n\nDashti, Masoumeh, and Andrew M. Stuart. 2015. “The Bayesian Approach to Inverse Problems.” In Handbook of Uncertainty Quantification, edited by Roger Ghanem, David Higdon, and Houman Owhadi, 1–118. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-319-11259-6_7-1.\n\n\nHuang, Daniel Zhengyu, Jiaoyang Huang, Sebastian Reich, and Andrew M Stuart. 2022. “Efficient Derivative-Free Bayesian Inference for Large-Scale Inverse Problems.” Inverse Problems 38 (12): 125006. https://doi.org/10.1088/1361-6420/ac99fa.\n\n\nIglesias, Marco A, Kody J H Law, and Andrew M Stuart. 2013. “Ensemble Kalman Methods for Inverse Problems.” Inverse Problems 29 (4): 045001. https://doi.org/10.1088/0266-5611/29/4/045001.\n\n\nLaw, Kody, Andrew Stuart, and Konstantinos Zygalakis. 2015. Data Assimilation: A Mathematical Introduction. Vol. 62. Texts in Applied Mathematics. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-319-20325-6.\n\n\nSanita Vetra-Carvalho, Lars Nerger, Peter Jan van Leeuwen, and Jean-Marie Beckers. 2018. “State-of-the-Art Stochastic Data Assimilation Methods for High-Dimensional Non-Gaussian Problems.” Tellus A: Dynamic Meteorology and Oceanography 70 (1): 1–43. https://doi.org/10.1080/16000870.2018.1445364.\n\n\nSärkkä, S., and L. Svensson. 2023. Bayesian Filtering and Smoothing. 2nd ed. Institute of Mathematical Statistics Textbooks. Cambridge University Press. https://doi.org/10.1017/9781108917407.\n\n\nWu, Jin-Long, Matthew E. Levine, Tapio Schneider, and Andrew Stuart. 2023. “Learning about Structural Errors in Models of Complex Dynamical Systems.” https://arxiv.org/abs/2401.00035.",
    "crumbs": [
      "Ensemble Filters",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Ensemble Kalman Filter</span>"
    ]
  },
  {
    "objectID": "02examples/EnKF/032EnKFExample1.html",
    "href": "02examples/EnKF/032EnKFExample1.html",
    "title": "12  Example 1: noisy pendulum",
    "section": "",
    "text": "12.1 Generate noisy observations\nTo generate the noisy observations, we have 2 options\nWhatever the approach chosen, it must be used both for the observation generation and for the state evolution within the Kalman filter.\nWe will use the simpler Euler approximation. For reference, here is the RK4 method.\ndef Pendulum(state, *args): #nonlinear pendulum\n    g = args[0]\n    L = args[1]\n    x1, x2 = state #Unpack the state vector\n    f = np.zeros(2) #Derivatives\n    f[0] = x2\n    f[1] = -g/L * np.sin(x1)\n    return f \n\ndef RK4(rhs, state, dt, *args):\n    \n    k1 = rhs(state,         *args)\n    k2 = rhs(state+k1*dt/2, *args)\n    k3 = rhs(state+k2*dt/2, *args)\n    k4 = rhs(state+k3*dt,   *args)\n\n    new_state = state + (dt/6)*(k1 + 2*k2 + 2*k3 + k4)\n    return new_state\n\n#np.random.seed(55)\n# Solve system and generate noisy observations\nT = np.linspace(0, Nt, Nt)\n#x0True = np.array([1.8, 0.0]) # True initial conditions\nx0True = np.array([1.5, 0.0]) # True initial conditions\n#time integration\nchol_Q = np.linalg.cholesky(Q) # noise std dev.\nsqrt_R = np.sqrt(R)\nxxTrue = np.zeros([Nx, Nt])\nxxTrue[:,0] = x0True\nyy = np.zeros((Nt, Ny))\nyy[0] = np.sin(xxTrue[0, 0]) + sig_v * np.random.randn() # must initialize correctly!\nfor k in range(Nt-1):\n    w_k = chol_Q @ np.random.randn(2)\n    xxTrue[:,k+1] = RK4(Pendulum, xxTrue[:,k], dt, g, 1.0) + w_k\n    yy[k+1,0] = np.sin(xxTrue[0, k+1]) + sig_v * np.random.randn()\n\n# plot results\nfig, ax = plt.subplots(nrows=1,ncols=1, figsize=(10,4))\nax.plot(T*dt, np.sin(xxTrue[0, :]), color='b', linewidth = 3, label=\"Noisy state\")\nax.scatter(T*dt, yy[:,0], marker=\"o\", label=\"Measurements\", color=\"red\", alpha=0.66, s=10)\nax.set_xlabel('t')\nax.set_ylabel(\"$\\sin(x_1(t))$\", labelpad=5)\nax.legend()\nplt.show()\n# Use Euler method\nfrom common_utilities import generate_pendulum, RandomState, rmse, plot_pendulum\nrandom_state = RandomState(1)\nsteps = Nt\nx_0 = x0True\n#timeline, states, observations = generate_pendulum(x_0, g, Q, dt, R, steps, random_state)\n#plot_pendulum(timeline, observations, states, \"Trajectory\")\ny = np.zeros((Nt, Ny))\ntimeline, xTrue, y[:, 0] = generate_pendulum(x_0, g, Q, dt, R, steps, random_state)\nplot_pendulum(timeline, y, xTrue, \"Trajectory\")\nfig, ax = plt.subplots(nrows=1,ncols=1, figsize=(10,4))\nax.plot(T*dt, np.sin(xTrue[:, 0]), color='b', linewidth = 3, label=\"Noisy state\")\nax.scatter(T*dt, y[:,0], marker=\"o\", label=\"Measurements\", color=\"red\", alpha=0.66, s=10)\nax.legend()\nplt.ylabel('$\\sin x_1(t)$')\nplt.xlabel('$t$')\nplt.show()\nfig, ax = plt.subplots(nrows=1,ncols=1, figsize=(6,6))\n#plt.plot(xTrue[0, :], xTrue[1, :], 'b')\nplt.plot(xTrue[:, 0], xTrue[:, 1], 'b')\nplt.scatter(xTrue[0, 0], xTrue[0, 1], marker=\"o\", color=\"green\", s=500)\nplt.xlabel('$x_1(t)$')\nplt.ylabel('$x_2(t)$')\nplt.show()",
    "crumbs": [
      "Ensemble Filters",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Example 1: noisy pendulum</span>"
    ]
  },
  {
    "objectID": "02examples/EnKF/032EnKFExample1.html#generate-noisy-observations",
    "href": "02examples/EnKF/032EnKFExample1.html#generate-noisy-observations",
    "title": "12  Example 1: noisy pendulum",
    "section": "",
    "text": "Use an accurate RK4 method.\nUse a simpler, first-order Euler method.",
    "crumbs": [
      "Ensemble Filters",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Example 1: noisy pendulum</span>"
    ]
  },
  {
    "objectID": "02examples/EnKF/032EnKFExample1.html#ensemble-kalman-filter",
    "href": "02examples/EnKF/032EnKFExample1.html#ensemble-kalman-filter",
    "title": "12  Example 1: noisy pendulum",
    "section": "12.2 Ensemble Kalman Filter",
    "text": "12.2 Ensemble Kalman Filter\n\ndef ens_kalman_filter(m_0, P_0, Q, R, Ne, dt, Y):\n    Nx = m_0.shape[-1]\n    Nt, Ny = Y.shape\n    \n    enkf_m = np.empty((Nt, Nx))\n    enkf_P = np.empty((Nt, Nx, Nx))\n    X      = np.empty((Nx, Ne))\n    HXf    = np.empty((Ny, Nx))\n\n    X[:,:] = np.tile(m_0, (Ne,1)).T  + np.linalg.cholesky(P_0)@np.random.randn(Nx, Ne) # initial ensemble state\n    P        = P_0 # initial state covariance\n    enkf_m[0, :]    = m_0\n    enkf_P[0, :, :] = P_0\n    \n    for i in range(Nt-1):\n        # ==== predict/forecast ====\n        Xf = Ax(X[:,:],dt) + np.linalg.cholesky(Q)@np.random.randn(Nx, Ne) # predict state ensemble\n        mX = np.mean(Xf, axis=1)        # state ensemble mean\n        Xfp = Xf - mX[:, None]          # state forecast anomaly\n        #Phat = Xfp @ Xfp.T / (Ne - 1)  # predict covariance (not needed)\n        # ==== prepare =====\n        HXf = Hx(Xf)                    # nonlinear observation\n        mY = np.mean(HXf, axis=1)       # observation ensemble mean\n        HXp = HXf - mY[:, None]         # observation anomaly\n        S = (HXp @ HXp.T)/(Ne - 1) + R  # observation covariance        \n        K = linalg.solve(S, HXp @ Xfp.T, assume_a=\"pos\").T  / (Ne - 1)  # Kalman gain\n        # === perturb y and compute innovation ====\n        ypert = Y[i,:] +  np.linalg.cholesky(R)@np.random.randn(Ny, Ne)\n        d = ypert - HXf\n        # ==== correct/analyze ====\n        X[:,:] = Xf + K @ d         # update state ensemble\n        mX = np.mean(X[:,:], axis=1)# state analysis ensemble mean\n        Xap = X[:,:] - mX[:, None]  # state analysis anomaly\n        P = Xap @ Xap.T / ( Ne - 1)     # update covariance\n        # ==== save ====\n        enkf_m[i+1] = mX                # save KF state estimate (mean)\n        enkf_P[i+1] = P                 # save KF error estimate (covariance)\n    return enkf_m, enkf_P\n\n\n# initialize state and covariance\nx_0 = np.array([1.5, 0.])   # Initial state\nm_0 = np.array([1.6, 0.])   # Initial state estimate (slightly off)\nP_0 = np.array([[r, 0.], \n                [0., r]]) # Initial estimate covariance\n\nenkf_m, enkf_P = ens_kalman_filter(m_0, P_0, Q, R, Ne, dt, y)\n\n#rmse_ekf = rmse(ekf_m[:, :1], states[:, :1])\n#print(f\"EKF RMSE: {rmse_ekf}\")\n\n\nfig, axes = plt.subplots(ncols=2, figsize=(18, 8))\nx1 = xTrue  # states\nx2 = enkf_m # KF estimate\ny  = y      # Measurements\ntimeline = T*dt\naxes[1].scatter(timeline, y, marker=\".\", label=\"Measurements\", color=\"red\", alpha=0.66)\n#axes[1].plot(timeline, np.sin(x1[0, :]), linestyle=\"dashdot\", label=\"Trajectory\", color=\"blue\")\naxes[1].plot(timeline, np.sin(x1[:, 0]), linestyle=\"dashdot\", label=\"Trajectory\", color=\"blue\")\naxes[1].plot(timeline, np.sin(x2[:, 0]), linestyle=\"dashdot\", label=\"EnKF estimate\", color=\"green\")\n#axes[0].plot(x1[0, :], x1[1, :], label=\"Trajectory\", color=\"blue\")\naxes[0].plot(x1[:, 0], x1[:, 1], label=\"Trajectory\", color=\"blue\")\naxes[0].plot(x2[:, 0], x2[:, 1], label=\"EnKF estimate\", color=\"green\")\naxes[0].legend()\naxes[1].legend()\naxes[0].set_xlabel('$x_1(t)$')\naxes[1].set_xlabel('$t$')\naxes[0].set_ylabel('$ x_2(t)$')\naxes[1].set_ylabel('$\\sin x_1(t)$')\n\nplt.show()\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(nrows=1,ncols=1, figsize=(8,8))\nplt.scatter(x1[0, 0], x1[0, 1], marker=\"o\", color=\"green\", s=500)\nplt.plot(x1[:, 0], x1[:, 1], label=\"Trajectory\", color=\"blue\")\nplt.plot(x2[:, 0], x2[:, 1], label=\"EnKF estimate\", color=\"red\")\nplt.xlabel('$x_1(t)$')\nplt.ylabel('$x_2(t)$')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nThe ensemble Kalman filter tracks rapidly, smoothly and accurately the noisy trajectory of the pendulum. This can be compared to the performance of the EKF (extended Kalman filter) in Example 2, Section 4, that was less accurate (took longer to catch up) and was more noisy.\nNote that the error/anomaly covariance elements, \\(P_{ii},\\) are noisy, since they are empirically averaged quantities.\n\n\n\n\n\n\nAsch, Mark. 2022. A Toolbox for Digital Twins: From Model-Based to Data-Driven. Philadelphia, PA: Society for Industrial; Applied Mathematics. https://doi.org/10.1137/1.9781611976977.\n\n\nAsch, Mark, Marc Bocquet, and Maëlle Nodet. 2016. Data Assimilation: Methods, Algorithms, and Applications. Philadelphia, PA: Society for Industrial; Applied Mathematics. https://doi.org/10.1137/1.9781611974546.\n\n\nCalvello, Edoardo, Sebastian Reich, and Andrew M. Stuart. 2022. “Ensemble Kalman Methods: A Mean Field Perspective.” arXiv (to appear in Acta Numerica 2025). http://arxiv.org/abs/2209.11371.\n\n\nCarrillo, J. A., F. Hoffmann, A. M. Stuart, and U. Vaes. 2024a. “Statistical Accuracy of Approximate Filtering Methods.” https://arxiv.org/abs/2402.01593.\n\n\n———. 2024b. “The Mean Field Ensemble Kalman Filter: Near-Gaussian Setting.” https://arxiv.org/abs/2212.13239.\n\n\nDashti, Masoumeh, and Andrew M. Stuart. 2015. “The Bayesian Approach to Inverse Problems.” In Handbook of Uncertainty Quantification, edited by Roger Ghanem, David Higdon, and Houman Owhadi, 1–118. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-319-11259-6_7-1.\n\n\nHuang, Daniel Zhengyu, Jiaoyang Huang, Sebastian Reich, and Andrew M Stuart. 2022. “Efficient Derivative-Free Bayesian Inference for Large-Scale Inverse Problems.” Inverse Problems 38 (12): 125006. https://doi.org/10.1088/1361-6420/ac99fa.\n\n\nIglesias, Marco A, Kody J H Law, and Andrew M Stuart. 2013. “Ensemble Kalman Methods for Inverse Problems.” Inverse Problems 29 (4): 045001. https://doi.org/10.1088/0266-5611/29/4/045001.\n\n\nLaw, Kody, Andrew Stuart, and Konstantinos Zygalakis. 2015. Data Assimilation: A Mathematical Introduction. Vol. 62. Texts in Applied Mathematics. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-319-20325-6.\n\n\nSanita Vetra-Carvalho, Lars Nerger, Peter Jan van Leeuwen, and Jean-Marie Beckers. 2018. “State-of-the-Art Stochastic Data Assimilation Methods for High-Dimensional Non-Gaussian Problems.” Tellus A: Dynamic Meteorology and Oceanography 70 (1): 1–43. https://doi.org/10.1080/16000870.2018.1445364.\n\n\nSärkkä, S., and L. Svensson. 2023. Bayesian Filtering and Smoothing. 2nd ed. Institute of Mathematical Statistics Textbooks. Cambridge University Press. https://doi.org/10.1017/9781108917407.\n\n\nWu, Jin-Long, Matthew E. Levine, Tapio Schneider, and Andrew Stuart. 2023. “Learning about Structural Errors in Models of Complex Dynamical Systems.” https://arxiv.org/abs/2401.00035.",
    "crumbs": [
      "Ensemble Filters",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Example 1: noisy pendulum</span>"
    ]
  },
  {
    "objectID": "02examples/EnKF/032EnKFExample2.html",
    "href": "02examples/EnKF/032EnKFExample2.html",
    "title": "13  Example 2: Lorenz63 system",
    "section": "",
    "text": "13.1 Ensemble KF for Data Assimilation\nHere we will generalize the ensemble Kalman filter to take into account the possibility of sparse observations. This is usually the case in real-life systems, where observations are only available et fixed instants, and hence the filtering can only be applied at these times. Inbetween observations, the system evolves freely (without correction) according to its underlying state equation.\nSuppose we have \\(N_y\\) measurements/observations at an interval of \\(\\delta t_y.\\) This gives measurements for times \\(t_0 \\le t \\le t_m,\\) where \\(t_m = N_m \\delta t_m.\\) This can be considered as the assimilation window. The system then evolves freely for \\(t &gt; t_m\\) until some final forecast window time \\(t_f.\\) The state, or equation itself is simulated with a smaller \\(\\delta t\\) and for a large number \\(N_t\\) steps, giving \\(t_f = N_t \\delta t.\\) Usually, for real life systems, we will have\n\\[\n  \\delta t_m \\ge \\delta t,  \\quad  N_m \\le N_t , \\quad t_m \\le t_f.\n\\]\nFor code testing, we make the simplifying (unrealistic) academic assumption that\n\\[\n  \\delta t_m = \\delta t, \\quad N_m = N_t, \\quad t_f = t_m.\n\\]\nThis implies the availabilty of measurements at each (and every) time step. Note that in many of the previous examples, this was indeed the case.\ndef enKF_Lorenz63_setup(dt, T, dt_m, T_m, sig_w, sig_v): \n    \"\"\"\n    Prepare input (true state and observations) for the stochastic \n    ensemble filter of the Lorenz63 system.\n\n    Parameters:\n        dt: time step for state evolution\n        T:  time interval for state evolution\n        dt_m: time interval between 2 measurements (can equal dt for dense observations)\n        T_m: time interval for observations\n        sig_w: state noise sd., cov. Q = sig_w**2 x np.eye(3)\n        sig_v: measurement noise sd., cov. R = sig_v**2 x np.eye(3) \n    \"\"\"\n    # parameters Lorenz63\n    sigma = 10.0     \n    rho  = 28.0     \n    beta = 8.0/3.0\n    dim_x = 3\n    dim_y = 3\n    # noise covariances\n    Q  = sig_w**2 * np.eye(dim_x)\n    R  = sig_v**2 * np.eye(dim_y)\n    # measurement operator (identity here)\n    def H(u):\n        w = u\n        return w\n\n    # Solve system and generate noisy observations\n    Nt = int(T/dt)     # number of time steps\n    Nm = int(T_m/dt_m) # number of observations\n    t = np.linspace(0, Nt, Nt+1) * dt # time vector (including 0 and T)\n    ind_m = (np.linspace(int(dt_m/dt),int(T_m/dt),Nm)).astype(int) # obs. indices\n    t_m = t[ind_m] # measurement time vector\n    x0True = np.array([1.0, 1.0, 1.0]) # True initial conditions\n    sqrt_Q = np.linalg.cholesky(Q) # noise std dev.\n    sqrt_R = np.linalg.cholesky(R)\n    # initialize (correctly!)\n    xTrue = np.zeros([Nt+1, dim_x])\n    xTrue[0, :] = x0True\n    y = np.zeros((Nm, dim_y))\n    km = 0 # index for measurement times\n    y[0,:] = H(xTrue[0,:]) + sig_v * np.random.randn(dim_y) \n    for k in range(Nt):\n        w_k = sqrt_Q @ np.random.randn(dim_x)\n        xTrue[k+1,:] = RK4(Lorenz63,xTrue[k,:], dt,sigma, rho, beta) #+ w_k\n        if (km &lt; Nm) and (k+1 == ind_m[km]):\n            v_k = sqrt_R @ np.random.randn(dim_y)\n            y[km,:]     = H(xTrue[k+1,:]) + v_k\n            km = km + 1\n    # plot state and measurements\n    fig, ax = plt.subplots(nrows=3,ncols=1, figsize=(10,8))\n    ax = ax.flat\n    #t = T*dt\n    for k in range(3):\n        ax[k].plot(t,xTrue[:,k], label='True', linewidth = 3)\n        ax[k].plot(t[ind_m],y[:,k], 'o', fillstyle='none', \\\n                   label='Observation', markersize = 8, markeredgewidth = 2)\n        ax[k].set_xlabel('t')\n        ax[k].axvspan(0, T_m, color='lightgray', alpha=0.4, lw=0)    \n    ax[0].legend(loc=\"center\", bbox_to_anchor=(0.5,1.25),ncol =4,fontsize=15)   \n    ax[0].set_ylabel('x(t)')\n    ax[1].set_ylabel('y(t)')\n    ax[2].set_ylabel('z(t)')\n    fig.subplots_adjust(hspace=0.5)\n\n    return Q, R, xTrue, y, ind_m, Nt, Nm\nQ, R, xTrue, y, ind_m, Nt, Nm = enKF_Lorenz63_setup(dt=0.01, T=10, dt_m=0.2, T_m =2, sig_w=0.001, sig_v=0.15)\ndef enKF_Lorenz63_DA(x0, P0, Q, R, y, ind_m, Nt, Nm, Ne=10):\n    \"\"\"\n    Run DA of the Lorenz63 system using the stochastic \n    ensemble filter with sparse observations in the DA\n    window, defined by time index set `ind_m`.\n\n    Parameters:\n    \n    \"\"\"\n    # parameters Lorenz63\n    sigma = 10.0     \n    rho  = 28.0     \n    beta = 8.0/3.0\n    def Hx(u):\n        w = u\n        return w\n    Nx     = x0.shape[-1]\n    Ny     = y.shape[-1]\n    enkf_m = np.empty((Nt+1, Nx))\n    enkf_P = np.empty((Nt+1, Nx, Nx))\n    X      = np.empty((Nx, Ne))\n    Xf     = np.empty((Nx, Ne))\n    HXf    = np.empty((Ny, Nx))\n\n    X[:,:] = np.tile(x0, (Ne,1)).T  + np.linalg.cholesky(P0)@np.random.randn(Nx, Ne) # initial ensemble state\n    P       = P0 # initial state covariance\n    enkf_m[0, :]    = x0 \n    enkf_P[0, :, :] = P0\n\n    i_m = 0 # index for measurement times\n    \n    for i in range(Nt):\n        # ==== predict/forecast ====\n        for e in range(Ne):\n            w_i = np.linalg.cholesky(Q) @ np.random.randn(Nx)#, Ne)\n            Xf[:,e] = RK4(Lorenz63, X[:,e], dt, sigma, rho, beta) + w_i # predict state ensemble\n        mX = np.mean(Xf, axis=1)        # state ensemble mean\n        Xfp = Xf - mX[:, None]          # state forecast anomaly\n        P = Xfp @ Xfp.T / (Ne - 1)      # predict covariance \n        # ==== prepare analysis step =====\n        if (i_m &lt; Nm) and (i+1 == ind_m[i_m]):\n            HXf = Hx(Xf)                    # nonlinear observation\n            mY = np.mean(HXf, axis=1)       # observation ensemble mean\n            HXp = HXf - mY[:, None]         # observation anomaly\n            S = (HXp @ HXp.T)/(Ne - 1) + R  # observation covariance\n            K = linalg.solve(S, HXp @ Xfp.T, assume_a=\"pos\").T  / (Ne - 1)  # Kalman gain\n            # === perturb y and compute innovation ====\n            ypert = y[i_m, :] +  (np.linalg.cholesky(R)@np.random.randn(Ny, Ne)).T\n            d = ypert.T - HXf\n            # ==== correct/analyze ====\n            X[:,:] = Xf + K @ d          # update state ensemble\n            mX = np.mean(X[:,:], axis=1) # state analysis ensemble mean\n            Xap = X[:,:] - mX[:, None]   # state analysis anomaly\n            P = Xap @ Xap.T / ( Ne - 1)  # update covariance\n            i_m = i_m + 1\n        else:\n            X[:,:] = Xf  # when there is no obs, then state=forecast\n        # ==== save ====\n        enkf_m[i+1] = mX                # save KF state estimate (mean)\n        enkf_P[i+1] = P                 # save KF error estimate (covariance)\n    return enkf_m, enkf_P\n# Initialize and run the analysis\nsig_w = 0.0015\nsig_v = 0.15\nQ  = sig_w**2 * np.eye(3) #* 1.e-6 # for comparison with DT\nR  = sig_v**2 * np.eye(3)\n\nx0 = np.array([2., 3., 4.]) # a little off\nsig_vv = 0.1\nP0 = np.eye(3) * sig_vv**2 # Initial estimate covariance\nNe = 10\nXa, P = enKF_Lorenz63_DA(x0, P0, Q, R, y, ind_m, Nt, Nm, Ne=10)\n# Post-process and plot the results\n# generate unfiltered state\nXb = np.empty((Nt+1, 3))\nXb[0,:] = x0\nfor i in range(Nt):\n    Xb[i+1,:] =  RK4(Lorenz63, Xb[i,:], dt, sigma, rho, beta)    \n# plot state and measurements\nt = np.linspace(0, Nt, Nt+1) * dt # time vector\nT_m = 2.\nfig, ax = plt.subplots(nrows=3,ncols=1, figsize=(10,8))\nax = ax.flat\n\nfor k in range(3):\n    ax[k].plot(t,xTrue[:,k], label='True')#, linewidth = 3)\n    ax[k].plot(t,Xa[:,k], '--', label='EnKF analysis')#, linewidth = 3)\n    ax[k].plot(t[ind_m],y[:,k], 'o', fillstyle='none', \\\n               label='Observation')#, markersize = 8, markeredgewidth = 2)\n    ax[k].plot(t,Xb[:,k], ':', label='Unfiltered')#, linewidth = 3)\n    ax[k].set_xlabel('t')\n    ax[k].axvspan(0, T_m, color='lightgray', alpha=0.4, lw=0)    \nax[0].legend(loc=\"center\", bbox_to_anchor=(0.5,1.25),ncol =4,fontsize=15)   \nax[0].set_ylabel('x(t)')\nax[1].set_ylabel('y(t)')\nax[2].set_ylabel('z(t)')\nfig.subplots_adjust(hspace=0.5)",
    "crumbs": [
      "Ensemble Filters",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Example 2: Lorenz63 system</span>"
    ]
  },
  {
    "objectID": "02examples/EnKF/032EnKFExample2.html#conclusion",
    "href": "02examples/EnKF/032EnKFExample2.html#conclusion",
    "title": "13  Example 2: Lorenz63 system",
    "section": "13.2 Conclusion",
    "text": "13.2 Conclusion\nThe ensemble Kalman filter, even with very sparse observations and a chaotic system, does an excellent job of\n\ntracking within the DA window,\nforecasting way beyond the window, whereas the unfiltered/unassimilated, freely evolving system deviates considerably, as is to be expected from the chaotic Lorenz system.\n\nFrom \\(t \\approx 6,\\) the KF starts to deviate very slightly, mostly a phase difference. However, in real situations, there will already be new measurements available by this time. Then the KF analysis will kick in again, and rectify the trajectory.\n\n\n\n\nAsch, Mark. 2022. A Toolbox for Digital Twins: From Model-Based to Data-Driven. Philadelphia, PA: Society for Industrial; Applied Mathematics. https://doi.org/10.1137/1.9781611976977.\n\n\nAsch, Mark, Marc Bocquet, and Maëlle Nodet. 2016. Data Assimilation: Methods, Algorithms, and Applications. Philadelphia, PA: Society for Industrial; Applied Mathematics. https://doi.org/10.1137/1.9781611974546.\n\n\nCalvello, Edoardo, Sebastian Reich, and Andrew M. Stuart. 2022. “Ensemble Kalman Methods: A Mean Field Perspective.” arXiv (to appear in Acta Numerica 2025). http://arxiv.org/abs/2209.11371.\n\n\nCarrillo, J. A., F. Hoffmann, A. M. Stuart, and U. Vaes. 2024a. “Statistical Accuracy of Approximate Filtering Methods.” https://arxiv.org/abs/2402.01593.\n\n\n———. 2024b. “The Mean Field Ensemble Kalman Filter: Near-Gaussian Setting.” https://arxiv.org/abs/2212.13239.\n\n\nDashti, Masoumeh, and Andrew M. Stuart. 2015. “The Bayesian Approach to Inverse Problems.” In Handbook of Uncertainty Quantification, edited by Roger Ghanem, David Higdon, and Houman Owhadi, 1–118. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-319-11259-6_7-1.\n\n\nHuang, Daniel Zhengyu, Jiaoyang Huang, Sebastian Reich, and Andrew M Stuart. 2022. “Efficient Derivative-Free Bayesian Inference for Large-Scale Inverse Problems.” Inverse Problems 38 (12): 125006. https://doi.org/10.1088/1361-6420/ac99fa.\n\n\nIglesias, Marco A, Kody J H Law, and Andrew M Stuart. 2013. “Ensemble Kalman Methods for Inverse Problems.” Inverse Problems 29 (4): 045001. https://doi.org/10.1088/0266-5611/29/4/045001.\n\n\nLaw, Kody, Andrew Stuart, and Konstantinos Zygalakis. 2015. Data Assimilation: A Mathematical Introduction. Vol. 62. Texts in Applied Mathematics. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-319-20325-6.\n\n\nSanita Vetra-Carvalho, Lars Nerger, Peter Jan van Leeuwen, and Jean-Marie Beckers. 2018. “State-of-the-Art Stochastic Data Assimilation Methods for High-Dimensional Non-Gaussian Problems.” Tellus A: Dynamic Meteorology and Oceanography 70 (1): 1–43. https://doi.org/10.1080/16000870.2018.1445364.\n\n\nSärkkä, S., and L. Svensson. 2023. Bayesian Filtering and Smoothing. 2nd ed. Institute of Mathematical Statistics Textbooks. Cambridge University Press. https://doi.org/10.1017/9781108917407.\n\n\nWu, Jin-Long, Matthew E. Levine, Tapio Schneider, and Andrew Stuart. 2023. “Learning about Structural Errors in Models of Complex Dynamical Systems.” https://arxiv.org/abs/2401.00035.",
    "crumbs": [
      "Ensemble Filters",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Example 2: Lorenz63 system</span>"
    ]
  },
  {
    "objectID": "02examples/EnKF/032EnKFExample3.html",
    "href": "02examples/EnKF/032EnKFExample3.html",
    "title": "14  Example 3: SIR Model",
    "section": "",
    "text": "14.1 Ensemble KF for Data Assimilation\nHere we will generalize the ensemble Kalman filter to take into account the possibility of sparse observations. This is usually the case in real-life systems, where observations are only available et fixed instants, and hence the filtering can only be applied at these times. Inbetween observations, the system evolves freely (without correction) according to its underlying state equation.\nSuppose we have \\(N_y\\) measurements/observations at an interval of \\(\\delta t_y.\\) This gives measurements for times \\(t_0 \\le t \\le t_m,\\) where \\(t_m = N_m \\delta t_m.\\) This can be considered as the assimilation window. The system then evolves freely for \\(t &gt; t_m\\) until some final forecast window time \\(t_f.\\) The state, or equation itself is simulated with a smaller \\(\\delta t\\) and for a large number \\(N_t\\) steps, giving \\(t_f = N_t \\delta t.\\) Usually, for real life systems, we will have\n\\[\n  \\delta t_m \\ge \\delta t,  \\quad  N_m \\le N_t , \\quad t_m \\le t_f.\n\\]\nFor code testing, we make the simplifying (unrealistic) academic assumption that\n\\[\n  \\delta t_m = \\delta t, \\quad N_m = N_t, \\quad t_f = t_m.\n\\]\nThis implies the availabilty of measurements at each (and every) time step. Note that in many of the previous examples, this was indeed the case.\ndef enKF_SIR_setup(dt, T, dt_m, T_m, sig_w, sig_v): \n    \"\"\"\n    Prepare input (true state and observations) for the stochastic \n    ensemble filter of the Lorenz63 system.\n\n    Parameters:\n        dt: time step for state evolution\n        T:  time interval for state evolution\n        dt_m: time interval between 2 measurements (can equal dt for dense observations)\n        T_m: time interval for observations\n        sig_w: state noise sd., cov. Q = sig_w**2 x np.eye(3)\n        sig_v: measurement noise sd., cov. R = sig_v**2 x np.eye(3) \n    \"\"\"\n    # parameters SIR\n    beta  = 4.0\n    lambd = 1.0\n    dim_x = 3\n    dim_y = 3\n    # noise covariances\n    Q  = sig_w**2 * np.eye(dim_x)\n    R  = sig_v**2 * np.eye(dim_y)\n    # measurement operator (identity here)\n    def H(u):\n        w = u\n        return w\n\n    # Solve system and generate noisy observations\n    Nt = int(T/dt)     # number of time steps\n    Nm = int(T_m/dt_m) # number of observations\n    t = np.linspace(0, Nt, Nt+1) * dt # time vector\n    ind_m = (np.linspace(int(dt_m/dt),int(T_m/dt),Nm)).astype(int) # obs. indices\n    t_m = t[ind_m] # measurement time vector\n    x0True = np.array([0.99, 0.01, 0]) # True initial conditions\n    sqrt_Q = np.linalg.cholesky(Q) # noise std dev.\n    sqrt_R = np.linalg.cholesky(R)\n    # initialize (correctly!)\n    xTrue = np.zeros([Nt+1, dim_x])\n    xTrue[0, :] = x0True\n    y = np.zeros((Nm, dim_y))\n    km = 0 # index for measurement times\n    y[0,:] = H(xTrue[0,:]) + sig_v * np.random.randn(dim_y) \n    for k in range(Nt):\n        w_k = sqrt_Q @ np.random.randn(dim_x)\n        xTrue[k+1,:] = RK4(SIR, xTrue[k,:], dt, beta, lambd) #+ w_k\n        if (km &lt; Nm) and (k+1 == ind_m[km]):\n            v_k = sqrt_R @ np.random.randn(dim_y)\n            y[km,:]     = H(xTrue[k+1,:]) + v_k\n            km = km + 1\n    # plot state and measurements\n    fig, ax = plt.subplots(nrows=3,ncols=1, figsize=(10,8))\n    ax = ax.flat\n    #t = T*dt\n    for k in range(3):\n        ax[k].plot(t,xTrue[:,k], label='True', linewidth = 3)\n        ax[k].plot(t[ind_m],y[:,k], 'o', fillstyle='none', \\\n                   label='Observation', markersize = 8, markeredgewidth = 2)\n        ax[k].set_xlabel('t')\n        ax[k].axvspan(0, T_m, color='lightgray', alpha=0.4, lw=0)    \n    ax[0].legend(loc=\"center\", bbox_to_anchor=(0.5,1.25),ncol =4,fontsize=15)   \n    ax[0].set_ylabel('S(t)')\n    ax[1].set_ylabel('I(t)')\n    ax[2].set_ylabel('R(t)')\n    fig.subplots_adjust(hspace=0.5)\n\n    return Q, R, xTrue, y, ind_m, Nt, Nm\nQ, R, xTrue, y, ind_m, Nt, Nm = enKF_SIR_setup(dt=0.1, T=5, dt_m=0.2, T_m =2, sig_w=0.001, sig_v=0.02)\ndef enKF_SIR_DA(x0, P0, Q, R, y, ind_m, Nt, Nm, Ne=10):\n    \"\"\"\n    Run DA of the SIR system using the stochastic \n    ensemble filter with sparse observations in the DA\n    window, defined by time index set `ind_m`.\n\n    Parameters:\n    \n    \"\"\"\n    # parameters SIR\n    beta  = 4.0\n    lambd = 1.0\n    def Hx(u):\n        w = u\n        return w\n    Nx     = x0.shape[-1]\n    Ny     = y.shape[-1]\n    enkf_m = np.empty((Nt+1, Nx))\n    enkf_P = np.empty((Nt+1, Nx, Nx))\n    X      = np.empty((Nx, Ne))\n    Xf     = np.empty((Nx, Ne))\n    HXf    = np.empty((Ny, Nx))\n\n    X[:,:] = np.tile(x0, (Ne,1)).T  + np.linalg.cholesky(P0)@np.random.randn(Nx, Ne) # initial ensemble state\n    P       = P0 # initial state covariance\n    enkf_m[0, :]    = x0 \n    enkf_P[0, :, :] = P0\n\n    i_m = 0 # index for measurement times\n    \n    for i in range(Nt):\n        # ==== predict/forecast ====\n        for e in range(Ne):\n            w_i = np.linalg.cholesky(Q) @ np.random.randn(Nx)#, Ne)\n            Xf[:,e] = RK4(SIR, X[:,e], dt,  beta, lambd) + w_i # predict state ensemble\n        mX = np.mean(Xf, axis=1)        # state ensemble mean\n        Xfp = Xf - mX[:, None]          # state forecast anomaly\n        P = Xfp @ Xfp.T / (Ne - 1)      # predict covariance \n        # ==== prepare analysis step =====\n        if (i_m &lt; Nm) and (i+1 == ind_m[i_m]):\n            HXf = Hx(Xf)              # nonlinear observation\n            mY = np.mean(HXf, axis=1)       # observation ensemble mean\n            HXp = HXf - mY[:, None]         # observation anomaly\n            S = (HXp @ HXp.T)/(Ne - 1) + R  # observation covariance\n            K = linalg.solve(S, HXp @ Xfp.T, assume_a=\"pos\").T  / (Ne - 1)  # Kalman gain\n            # === perturb y and compute innovation ====\n            ypert = y[i_m, :] +  (np.linalg.cholesky(R)@np.random.randn(Ny, Ne)).T\n            d = ypert.T - HXf\n            # ==== correct/analyze ====\n            X[:,:] = Xf + K @ d         # update state ensemble\n            mX = np.mean(X[:,:], axis=1)# state analysis ensemble mean\n            Xap = X[:,:] - mX[:, None]  # state analysis anomaly\n            P = Xap @ Xap.T / ( Ne - 1)     # update covariance\n            i_m = i_m + 1\n        else:\n            X[:,:] = Xf  # when there is no obs, then state=forecast\n        # ==== save ====\n        enkf_m[i+1] = mX                # save KF state estimate (mean)\n        enkf_P[i+1] = P                 # save KF error estimate (covariance)\n    return enkf_m, enkf_P\n# Initialize and run the analysis\nsig_w = 0.0015\nsig_v = 0.02\nQ  = sig_w**2 * np.eye(3) #* 1.e-6 # for comparison with DT\nR  = sig_v**2 * np.eye(3)\n\nx0 = np.array([0.95, 0.05, 0]) # a little off [0.99, 0.01, 0]\nsig_vv = 0.1\nP0 = np.eye(3) * sig_vv**2 # Initial estimate covariance\nNe = 10\nXa, P = enKF_SIR_DA(x0, P0, Q, R, y, ind_m, Nt, Nm, Ne=10)\n# Post-process and plot the results\n# generate unfiltered state\nXb = np.empty((Nt+1, 3))\nXb[0,:] = x0\nfor i in range(Nt):\n    Xb[i+1,:] =  RK4(SIR, Xb[i,:], dt, beta, lambd)    \n# plot state and measurements\nt = np.linspace(0, Nt, Nt+1) * dt # time vector\nT_m = 2.\nfig, ax = plt.subplots(nrows=3,ncols=1, figsize=(10,8))\nax = ax.flat\n\nfor k in range(3):\n    ax[k].plot(t,xTrue[:,k], label='True')#, linewidth = 3)\n    ax[k].plot(t,Xa[:,k], '--', label='EnKF analysis')#, linewidth = 3)\n    ax[k].plot(t[ind_m],y[:,k], 'o', fillstyle='none', \\\n               label='Observation')#, markersize = 8, markeredgewidth = 2)\n    ax[k].plot(t,Xb[:,k], ':', label='Unfiltered')#, linewidth = 3)\n    ax[k].set_xlabel('t')\n    ax[k].axvspan(0, T_m, color='lightgray', alpha=0.4, lw=0)    \nax[0].legend(loc=\"center\", bbox_to_anchor=(0.5,1.25),ncol =4,fontsize=15)   \nax[0].set_ylabel('S(t)')\nax[1].set_ylabel('I(t)')\nax[2].set_ylabel('R(t)')\nfig.subplots_adjust(hspace=0.5)",
    "crumbs": [
      "Ensemble Filters",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Example 3: SIR Model</span>"
    ]
  },
  {
    "objectID": "02examples/EnKF/032EnKFExample3.html#conclusion",
    "href": "02examples/EnKF/032EnKFExample3.html#conclusion",
    "title": "14  Example 3: SIR Model",
    "section": "14.2 Conclusion",
    "text": "14.2 Conclusion\nThe ensemble Kalman filter, even with sparse observations and a nonlinear system, does an excellent job of\n\ntracking within the DA window\nforecasting way beyond the window, whereas the unfiltered/unassimilated, freely evolving system deviates considerably, as is to be expected from the nonlinear SIR system.\n\n\n\n\n\nAsch, Mark. 2022. A Toolbox for Digital Twins: From Model-Based to Data-Driven. Philadelphia, PA: Society for Industrial; Applied Mathematics. https://doi.org/10.1137/1.9781611976977.\n\n\nAsch, Mark, Marc Bocquet, and Maëlle Nodet. 2016. Data Assimilation: Methods, Algorithms, and Applications. Philadelphia, PA: Society for Industrial; Applied Mathematics. https://doi.org/10.1137/1.9781611974546.\n\n\nCalvello, Edoardo, Sebastian Reich, and Andrew M. Stuart. 2022. “Ensemble Kalman Methods: A Mean Field Perspective.” arXiv (to appear in Acta Numerica 2025). http://arxiv.org/abs/2209.11371.\n\n\nCarrillo, J. A., F. Hoffmann, A. M. Stuart, and U. Vaes. 2024a. “Statistical Accuracy of Approximate Filtering Methods.” https://arxiv.org/abs/2402.01593.\n\n\n———. 2024b. “The Mean Field Ensemble Kalman Filter: Near-Gaussian Setting.” https://arxiv.org/abs/2212.13239.\n\n\nDashti, Masoumeh, and Andrew M. Stuart. 2015. “The Bayesian Approach to Inverse Problems.” In Handbook of Uncertainty Quantification, edited by Roger Ghanem, David Higdon, and Houman Owhadi, 1–118. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-319-11259-6_7-1.\n\n\nHuang, Daniel Zhengyu, Jiaoyang Huang, Sebastian Reich, and Andrew M Stuart. 2022. “Efficient Derivative-Free Bayesian Inference for Large-Scale Inverse Problems.” Inverse Problems 38 (12): 125006. https://doi.org/10.1088/1361-6420/ac99fa.\n\n\nIglesias, Marco A, Kody J H Law, and Andrew M Stuart. 2013. “Ensemble Kalman Methods for Inverse Problems.” Inverse Problems 29 (4): 045001. https://doi.org/10.1088/0266-5611/29/4/045001.\n\n\nLaw, Kody, Andrew Stuart, and Konstantinos Zygalakis. 2015. Data Assimilation: A Mathematical Introduction. Vol. 62. Texts in Applied Mathematics. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-319-20325-6.\n\n\nSanita Vetra-Carvalho, Lars Nerger, Peter Jan van Leeuwen, and Jean-Marie Beckers. 2018. “State-of-the-Art Stochastic Data Assimilation Methods for High-Dimensional Non-Gaussian Problems.” Tellus A: Dynamic Meteorology and Oceanography 70 (1): 1–43. https://doi.org/10.1080/16000870.2018.1445364.\n\n\nSärkkä, S., and L. Svensson. 2023. Bayesian Filtering and Smoothing. 2nd ed. Institute of Mathematical Statistics Textbooks. Cambridge University Press. https://doi.org/10.1017/9781108917407.\n\n\nWu, Jin-Long, Matthew E. Levine, Tapio Schneider, and Andrew Stuart. 2023. “Learning about Structural Errors in Models of Complex Dynamical Systems.” https://arxiv.org/abs/2401.00035.",
    "crumbs": [
      "Ensemble Filters",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Example 3: SIR Model</span>"
    ]
  },
  {
    "objectID": "01theory/032EnSRF.html",
    "href": "01theory/032EnSRF.html",
    "title": "15  Deterministic Ensemble Kalman Filters",
    "section": "",
    "text": "15.1 The filtering problem.\nNonlinear state equation and observations are given by,\n\\[\\begin{align}\nx_{k+1}^{n} &=  \\mathcal{M}  ( {x}_{k}^{n} ) +  w_{k}^{n} , \\quad n=1, \\ldots, N_e, \\\\\n{y}_{k+1}    &=  \\mathcal{H} (x_{k+1}) +  v_{k+1},\n\\end{align}\\]\nwhere the noise/error terms\n\\[\nw_k \\sim  \\mathcal{N}(0, Q), \\quad  v_k \\sim  \\mathcal{N}(0, R).\n\\]",
    "crumbs": [
      "Ensemble Filters",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Deterministic Ensemble Kalman Filters</span>"
    ]
  },
  {
    "objectID": "01theory/032EnSRF.html#the-filtering-problem.",
    "href": "01theory/032EnSRF.html#the-filtering-problem.",
    "title": "15  Deterministic Ensemble Kalman Filters",
    "section": "",
    "text": "Filtering problem\n\n\n\nPredict the optimal state from the noisy measurements.",
    "crumbs": [
      "Ensemble Filters",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Deterministic Ensemble Kalman Filters</span>"
    ]
  },
  {
    "objectID": "01theory/032EnSRF.html#recall-ensemble-kalman-filter-enkf",
    "href": "01theory/032EnSRF.html#recall-ensemble-kalman-filter-enkf",
    "title": "15  Deterministic Ensemble Kalman Filters",
    "section": "15.2 Recall: Ensemble Kalman Filter (EnKF)",
    "text": "15.2 Recall: Ensemble Kalman Filter (EnKF)\nJust to recall the notation, here are the two steps (forecast-analysis, or predict-correct) of the EnKF.\nPrediction/Forecast Step\n\nevolve each ensemble member forward \\[ x_{k+1}^{n} =  \\mathcal{M}  ( {x}_{k}^{n} ) +  w_{k}^{n}, \\quad n=1,\n\\ldots, N_e\\]\ncompute ensemble mean \\[ \\bar{x} = \\frac{1}{N_e} \\sum_{n=1}^{N_e}  x_{n}^{\\mathrm{f}} \\]\ncompute covariance \\[ P^{\\mathrm{f}} = \\frac{1}{N_e - 1} X'^{\\mathrm{f}} ( X'^{\\mathrm{f}} )^{\\mathrm{T}} , \\]\nwhere \\(X'^{\\mathrm{f}} = x^{\\mathrm{f}} - \\bar{x}\\) is the ensemble state perturbation/anomaly matrix.\n\nCorrection/Analysis Step\n\ncompute the optimal Kalman gain \\[ K = P^{\\mathrm{f}} H^{\\mathrm{T}} (H P^{\\mathrm{f}} H^{\\mathrm{T}} + R)^{-1}  \\]\nupdate the ensemble using perturbed observations \\[  x_n^{\\mathrm{a}} =  x_n^{\\mathrm{f}}  + K( y_n + \\epsilon_y - H  x_n^{\\mathrm{f}}), \\quad n=1,\n\\ldots, N_e,\n\\] where \\(\\epsilon_n\\) is the stochastic perturbation of the observations \\(y.\\)",
    "crumbs": [
      "Ensemble Filters",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Deterministic Ensemble Kalman Filters</span>"
    ]
  },
  {
    "objectID": "01theory/032EnSRF.html#sec-ensrf",
    "href": "01theory/032EnSRF.html#sec-ensrf",
    "title": "15  Deterministic Ensemble Kalman Filters",
    "section": "15.3 Ensemble Square Root Filters (EnSRF)",
    "text": "15.3 Ensemble Square Root Filters (EnSRF)\n\n\n\n\n\n\nIdea\n\n\n\nUpdate the ensemble to preserve a covariance that is consistent with the KF theoretical covariance \\[ P_n^{\\mathrm{a}} = (I - K_n H) P_n^{\\mathrm{f}}.\n\\]\n\n\nRecall the fully nonlinear formulation of the Kalman gain in terms of the state and observation anomalies described in Section 11.2,\n\\[ K =  \\mathbf{X}'^{\\mathrm{f}}(\\mathbf{Y}'^{\\mathrm{f}})^\\mathrm{T} S^{-1},\n\\] where \\[ S = \\mathbf{Y}'^{\\mathrm{f}} ( \\mathbf{Y}'^{\\mathrm{f}} )^{\\mathrm{T}} +  R.\n\\]\nThen, to compute the posterior variance, we need to evaluate \\[ P_n^{\\mathrm{a}} =   \\mathbf{X}'^{\\mathrm{a}} (  \\mathbf{X}'^{\\mathrm{a}} )^{\\mathrm{T}}.\n\\]\nSo supposing there is a transform matrix, \\(T,\\) such that \\[   \\mathbf{X}'^{\\mathrm{a}} =   \\mathbf{X}'^{\\mathrm{f}} T,\n\\] we can substitute in the definition of \\(P_n^{\\mathrm{a}}\\) to obtain \\[\\begin{align}\nP_n^{\\mathrm{a}} &=  \\mathbf{X}'^{\\mathrm{f}} T (  \\mathbf{X}'^{\\mathrm{f}} T )^{\\mathrm{T}} \\\\\n                 &=  \\mathbf{X}'^{\\mathrm{f}} (T T^{\\mathrm{T}} ) (  \\mathbf{X}'^{\\mathrm{f}}  )^{\\mathrm{T}}.\n\\end{align}\\] And, on the other hand, for the consistency, we require \\[ (I - K_n H) P_n^{\\mathrm{f}} =   \\mathbf{X}'^{\\mathrm{f}}\n     \\left[ I - (  \\mathbf{Y}'^{\\mathrm{f}})^{\\mathrm{T}}  S^{-1}   \\mathbf{Y}'^{\\mathrm{f}}   \\right] (  \\mathbf{X}'^{\\mathrm{f}}  )^{\\mathrm{T}},\n\\] where we have used the relations \\[\\begin{align}\n  K                &=   P^{\\mathrm{f}} H^{\\mathrm{T}} (H P^{\\mathrm{f}} H^{\\mathrm{T}} + R)^{-1}, \\\\\n  P_n^{\\mathrm{f}} &=    \\mathbf{X}'^{\\mathrm{f}} (  \\mathbf{X}'^{\\mathrm{f}} )^{\\mathrm{T}} \\\\\n   \\mathbf{Y}'^{\\mathrm{f}}  &= H  \\mathbf{X}'^{\\mathrm{f}}.\n\\end{align}\\]\nHence, \\(T\\) must satisfy the so-called square root condition, \\[\\begin{align}\nT T^{\\mathrm{T}}  &= I - (  \\mathbf{Y}'^{\\mathrm{f}})^{\\mathrm{T}}  S^{-1}  \\mathbf{Y}'^{\\mathrm{f}} \\\\\n                  &= I - ( \\mathbf{Y}'^{\\mathrm{f}})^{\\mathrm{T}}   \n                     \\left[ \\mathbf{Y}'^{\\mathrm{f}} ( \\mathbf{Y}'^{\\mathrm{f}} )^{\\mathrm{T}} +  R  \\right]^{-1}  \\mathbf{Y}'^{\\mathrm{f}}\n\\end{align}\\]\nWe can replace the inversion of \\(S\\) by the much simpler and more stable inversion of the diagonal measurement error covariance \\(R\\) using the Sherman-Woodbury-Morrison formula, \\[\n(A + UCV^{\\mathrm{T}})^{-1} = A^{-1} - A^{-1} V (C^{-1} + V A^{-1} U)^{-1} V A^{-1}.\n\\]\nIdentifying \\(A=I,\\) \\(C = R^{-1},\\) \\(U = \\mathbf{Y}^{\\mathrm{T}},\\) \\(V=\\mathbf{Y},\\) we obtain the simpler form of the square root condition \\[\nT T^{\\mathrm{T}}  = \\left[I +  ( \\mathbf{Y}'^{\\mathrm{f}})^{\\mathrm{T}}  R^{-1}  \\mathbf{Y}'^{\\mathrm{f}} \\right]^{-1}.\n\\tag{15.1}\\] This form is the basis of the ETKF, or transform filter.\nFinally, the square root \\(T\\) can be obtained from the eigenvalue factorization, \\[  \nT T^{\\mathrm{T}}  = (U \\Sigma U^{\\mathrm{T}})^{-1}\n\\tag{15.2}\\] and thus \\[\nT = U \\Sigma^{-1/2} U^{\\mathrm{T}}.\n\\]\nNote that \\(T\\) is not unique, since for any orthogonal matrix \\(\\tilde{U},\\) the product \\(T \\tilde{U}\\) will also satisfy the square root condition. This leads, in principle, to a large number of alternative forms for \\(T\\) and the resulting square root filters.\nIt can be shown that, in general, this process can yield a biased and overconfident estimator of the posterior covariance. In order to remedy this, it suffices to ensure that \\(T\\) is symmetric, which is the case in the above derivation.",
    "crumbs": [
      "Ensemble Filters",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Deterministic Ensemble Kalman Filters</span>"
    ]
  },
  {
    "objectID": "01theory/032EnSRF.html#etkf-algorithm",
    "href": "01theory/032EnSRF.html#etkf-algorithm",
    "title": "15  Deterministic Ensemble Kalman Filters",
    "section": "15.4 ETKF Algorithm",
    "text": "15.4 ETKF Algorithm\nThe ensemble transform Kalman filter is one possible implementation of a square root filter. For greater generality, we will use the consistent formulation of (Sanita Vetra-Carvalho and Beckers 2018). The analysis update is given by the general, linear transformation \\[\n   \\mathbf{X}^{\\mathrm{a}} = \\overline{\\mathbf{X}}^{\\mathrm{f}} + \\mathbf{X}'^{\\mathrm{f}} (\\overline{W} + W'),\n\\] where the anomaly weight matrix, \\(W',\\) is computed from the square root condition (Equation 15.1), and the weight matrix, \\(\\overline{W},\\) is obtained from the formula for the Kalman gain matrix expressed in terms of the square root factorization (Equation 15.2). Each square root filter will just have different forms of these two matrices—all the rest will be the same.\nThe steps of the ETKF algorithm are as follows.\n\nForecast the state.\nCompute the means and anomalies of the state forecast and the observations.\nSetup the square root condition matrix and compute it’s eigenvalue decomposition.\nCompute the two weight matrices.\nUpdate the state analysis.\n\nSteps 3 and 4:\n\\[\n  U \\Sigma U^{\\mathrm{T}} =  I +  ( \\mathbf{Y}'^{\\mathrm{f}})^{\\mathrm{T}}  R^{-1}  \\mathbf{Y}'^{\\mathrm{f}},\n\\]\n\\[\n   W' =  U \\Sigma^{-1/2} U^{\\mathrm{T}}\n\\]\nand\n\\[\n  \\overline{W} =  U \\Sigma^{-1} U^{\\mathrm{T}}  (\\mathbf{Y}'^{\\mathrm{f}})^{\\mathrm{T}}  R^{-1}  D,\n\\]\nwhere the innovation (observations - average)\n\\[\n   D \\doteq Y - \\overline{\\mathbf{Y}},\n\\]\nwith\n\\[\n   \\overline{\\mathbf{Y}} \\doteq \\overline{ \\mathcal{H}( \\mathbf{X} )} .\n\\]",
    "crumbs": [
      "Ensemble Filters",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Deterministic Ensemble Kalman Filters</span>"
    ]
  },
  {
    "objectID": "01theory/032EnSRF.html#etkf-in-practice",
    "href": "01theory/032EnSRF.html#etkf-in-practice",
    "title": "15  Deterministic Ensemble Kalman Filters",
    "section": "15.5 ETKF in practice",
    "text": "15.5 ETKF in practice\nThere are a number of possible modifications that render the filter\n\nunbiased,\nnon-collapsing,\ncomputationally more stable,\ncomputationally cheaper.\n\nOne pathway is to scale the forecast observation ensemble perturbation matrix \\(Y^{\\mathrm{f}}\\) so as to normalize the standard deviation—which then equals one—and thus reduce loss of accuracy due to roundoff errors.\nA second path is to avoid the eigenvalue decompostion of \\(T T^{\\mathrm{T}}\\) and replace it by an SVD of \\(T\\) alone. This is particularly advantageous in high dimensions and in the presence of bad conditioning.\n\n\n\n\nAsch, Mark. 2022. A Toolbox for Digital Twins: From Model-Based to Data-Driven. Philadelphia, PA: Society for Industrial; Applied Mathematics. https://doi.org/10.1137/1.9781611976977.\n\n\nAsch, Mark, Marc Bocquet, and Maëlle Nodet. 2016. Data Assimilation: Methods, Algorithms, and Applications. Philadelphia, PA: Society for Industrial; Applied Mathematics. https://doi.org/10.1137/1.9781611974546.\n\n\nCalvello, Edoardo, Sebastian Reich, and Andrew M. Stuart. 2022. “Ensemble Kalman Methods: A Mean Field Perspective.” arXiv (to appear in Acta Numerica 2025). http://arxiv.org/abs/2209.11371.\n\n\nCarrillo, J. A., F. Hoffmann, A. M. Stuart, and U. Vaes. 2024a. “Statistical Accuracy of Approximate Filtering Methods.” https://arxiv.org/abs/2402.01593.\n\n\n———. 2024b. “The Mean Field Ensemble Kalman Filter: Near-Gaussian Setting.” https://arxiv.org/abs/2212.13239.\n\n\nDashti, Masoumeh, and Andrew M. Stuart. 2015. “The Bayesian Approach to Inverse Problems.” In Handbook of Uncertainty Quantification, edited by Roger Ghanem, David Higdon, and Houman Owhadi, 1–118. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-319-11259-6_7-1.\n\n\nHuang, Daniel Zhengyu, Jiaoyang Huang, Sebastian Reich, and Andrew M Stuart. 2022. “Efficient Derivative-Free Bayesian Inference for Large-Scale Inverse Problems.” Inverse Problems 38 (12): 125006. https://doi.org/10.1088/1361-6420/ac99fa.\n\n\nIglesias, Marco A, Kody J H Law, and Andrew M Stuart. 2013. “Ensemble Kalman Methods for Inverse Problems.” Inverse Problems 29 (4): 045001. https://doi.org/10.1088/0266-5611/29/4/045001.\n\n\nLaw, Kody, Andrew Stuart, and Konstantinos Zygalakis. 2015. Data Assimilation: A Mathematical Introduction. Vol. 62. Texts in Applied Mathematics. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-319-20325-6.\n\n\nSanita Vetra-Carvalho, Lars Nerger, Peter Jan van Leeuwen, and Jean-Marie Beckers. 2018. “State-of-the-Art Stochastic Data Assimilation Methods for High-Dimensional Non-Gaussian Problems.” Tellus A: Dynamic Meteorology and Oceanography 70 (1): 1–43. https://doi.org/10.1080/16000870.2018.1445364.\n\n\nSärkkä, S., and L. Svensson. 2023. Bayesian Filtering and Smoothing. 2nd ed. Institute of Mathematical Statistics Textbooks. Cambridge University Press. https://doi.org/10.1017/9781108917407.\n\n\nWu, Jin-Long, Matthew E. Levine, Tapio Schneider, and Andrew Stuart. 2023. “Learning about Structural Errors in Models of Complex Dynamical Systems.” https://arxiv.org/abs/2401.00035.",
    "crumbs": [
      "Ensemble Filters",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Deterministic Ensemble Kalman Filters</span>"
    ]
  },
  {
    "objectID": "02examples/EnKF/032EnKFExample4.html",
    "href": "02examples/EnKF/032EnKFExample4.html",
    "title": "16  Example 4: ETKF for Lorenz63 system",
    "section": "",
    "text": "16.1 Ensemble KF for Data Assimilation\nHere we will generalize the ensemble Kalman filter to take into account the possibility of sparse observations. This is usually the case in real-life systems, where observations are only available et fixed instants, and hence the filtering can only be applied at these times. Inbetween observations, the system evolves freely (without correction) according to its underlying state equation.\nSuppose we have \\(N_y\\) measurements/observations at an interval of \\(\\delta t_y.\\) This gives measurements for times \\(t_0 \\le t \\le t_m,\\) where \\(t_m = N_m \\delta t_m.\\) This can be considered as the assimilation window. The system then evolves freely for \\(t &gt; t_m\\) until some final forecast window time \\(t_f.\\) The state, or equation itself is simulated with a smaller \\(\\delta t\\) and for a large number \\(N_t\\) steps, giving \\(t_f = N_t \\delta t.\\) Usually, for real life systems, we will have\n\\[\n  \\delta t_m \\ge \\delta t,  \\quad  N_m \\le N_t , \\quad t_m \\le t_f.\n\\]\nFor code testing, we make the simplifying (unrealistic) academic assumption that\n\\[\n  \\delta t_m = \\delta t, \\quad N_m = N_t, \\quad t_f = t_m.\n\\]\nThis implies the availabilty of measurements at each (and every) time step. Note that in many of the previous examples, this was indeed the case.\ndef enKF_Lorenz63_setup(dt, T, dt_m, T_m, sig_w, sig_v): \n    \"\"\"\n    Prepare input (true state and observations) for the stochastic \n    ensemble filter of the Lorenz63 system.\n\n    Parameters:\n        dt: time step for state evolution\n        T:  time interval for state evolution\n        dt_m: time interval between 2 measurements (can equal dt for dense observations)\n        T_m: time interval for observations\n        sig_w: state noise sd., cov. Q = sig_w**2 x np.eye(3)\n        sig_v: measurement noise sd., cov. R = sig_v**2 x np.eye(3) \n    \"\"\"\n    # parameters Lorenz63\n    sigma = 10.0     \n    rho  = 28.0     \n    beta = 8.0/3.0\n    dim_x = 3\n    dim_y = 3\n    # noise covariances\n    Q  = sig_w**2 * np.eye(dim_x)\n    R  = sig_v**2 * np.eye(dim_y)\n    # measurement operator (identity here)\n    def H(u):\n        w = u\n        return w\n\n    # Solve system and generate noisy observations\n    Nt = int(T/dt)     # number of time steps\n    Nm = int(T_m/dt_m) # number of observations\n    t = np.linspace(0, Nt, Nt+1) * dt # time vector (including 0 and T)\n    ind_m = (np.linspace(int(dt_m/dt),int(T_m/dt),Nm)).astype(int) # obs. indices\n    t_m = t[ind_m] # measurement time vector\n    x0True = np.array([1.0, 1.0, 1.0]) # True initial conditions\n    sqrt_Q = np.linalg.cholesky(Q) # noise std dev.\n    sqrt_R = np.linalg.cholesky(R)\n    # initialize (correctly!)\n    xTrue = np.zeros([Nt+1, dim_x])\n    xTrue[0, :] = x0True\n    y = np.zeros((Nm, dim_y))\n    km = 0 # index for measurement times\n    y[0,:] = H(xTrue[0,:]) + sig_v * np.random.randn(dim_y) \n    for k in range(Nt):\n        w_k = sqrt_Q @ np.random.randn(dim_x)\n        xTrue[k+1,:] = RK4(Lorenz63,xTrue[k,:], dt,sigma, rho, beta) #+ w_k\n        if (km &lt; Nm) and (k+1 == ind_m[km]):\n            v_k = sqrt_R @ np.random.randn(dim_y)\n            y[km,:]     = H(xTrue[k+1,:]) + v_k\n            km = km + 1\n    # plot state and measurements\n    fig, ax = plt.subplots(nrows=3,ncols=1, figsize=(10,8))\n    ax = ax.flat\n    #t = T*dt\n    for k in range(3):\n        ax[k].plot(t,xTrue[:,k], label='True', linewidth = 3)\n        ax[k].plot(t[ind_m],y[:,k], 'o', fillstyle='none', \\\n                   label='Observation', markersize = 8, markeredgewidth = 2)\n        ax[k].set_xlabel('t')\n        ax[k].axvspan(0, T_m, color='lightgray', alpha=0.4, lw=0)    \n    ax[0].legend(loc=\"center\", bbox_to_anchor=(0.5,1.25),ncol =4,fontsize=15)   \n    ax[0].set_ylabel('x(t)')\n    ax[1].set_ylabel('y(t)')\n    ax[2].set_ylabel('z(t)')\n    fig.subplots_adjust(hspace=0.5)\n\n    return Q, R, xTrue, y, ind_m, Nt, Nm\nQ, R, xTrue, y, ind_m, Nt, Nm = enKF_Lorenz63_setup(dt=0.01, T=10, dt_m=0.2, T_m =2, sig_w=0.001, sig_v=0.15)",
    "crumbs": [
      "Ensemble Filters",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Example 4: ETKF for Lorenz63 system</span>"
    ]
  },
  {
    "objectID": "02examples/EnKF/032EnKFExample4.html#ensemble-transform-kalman-filter",
    "href": "02examples/EnKF/032EnKFExample4.html#ensemble-transform-kalman-filter",
    "title": "16  Example 4: ETKF for Lorenz63 system",
    "section": "16.2 Ensemble Transform Kalman Filter",
    "text": "16.2 Ensemble Transform Kalman Filter\nHere we implement the Livings/Dance unbiased variant of the deterministic ensemble square root filter (Sanita Vetra-Carvalho and Beckers 2018), where\n\nthe forecast anomaly, \\(\\mathbf{Y}',\\) is normalized to have unit variance,\nan SVD factorization is used to compute the transformation matrix, \\(T.\\)\n\n\ndef eTKF_Lorenz63_DA(x0, P0, Q, R, Y, ind_m, Nt, Nm, Ne=10):\n    \"\"\"\n    Run DA on the Lorenz63 system using the deterministic \n    ensemble square root filter (ETKF) with sparse observations \n    in the DA window, defined by time index set `ind_m`.\n\n    Parameters:\n    \n    \"\"\"\n    # parameters Lorenz63\n    sigma = 10.0     \n    rho   = 28.0     \n    beta  = 8.0/3.0\n    def Hx(u):\n        w = u\n        return w\n    Nx     = x0.shape[-1]\n    Ny     = y.shape[-1]\n    enkf_m = np.empty((Nt+1, Nx))\n    enkf_P = np.empty((Nt+1, Nx, Nx))\n    Xa     = np.empty((Nx, Ne))\n    Xf     = np.empty((Nx, Ne))\n    HXf    = np.empty((Ny, Nx))\n\n    Xa[:,:] = np.tile(x0, (Ne,1)).T  + np.linalg.cholesky(P0)@np.random.randn(Nx, Ne) # initial ensemble state\n    P       = P0 # initial state covariance\n    enkf_m[0, :]    = x0 \n    enkf_P[0, :, :] = P0\n\n    i_m = 0 # index for measurement times\n    \n    for i in range(Nt):\n        # ==== predict/forecast ====\n        for e in range(Ne):\n            w_i = np.linalg.cholesky(Q) @ np.random.randn(Nx)#, Ne)\n            Xf[:,e] = RK4(Lorenz63, Xa[:,e], dt, sigma, rho, beta) + w_i # predict state ensemble\n        mX  = np.mean(Xf, axis=1)                  # state ensemble mean\n        Xfp = (Xf - mX[:, None])#/ np.sqrt(Ne - 1) # state forecast anomaly\n        P   = Xfp @ Xfp.T  / (Ne - 1)              # predict covariance \n        # ==== prepare analysis step =====\n        if (i_m &lt; Nm) and (i+1 == ind_m[i_m]):\n            Rd  = np.diag(R)  # Observation error (diagonal)\n            HXf = Hx(Xf)                                 # nonlinear observation\n            mY  = np.mean(HXf, axis=1)                   # observation ensemble mean\n            HXp = (HXf - mY[:, None]) #/ np.sqrt(Ne - 1) # observation anomaly\n            #Scaling of obs. perturbations (Livings, 2005), for numerical stability\n            S_hat = np.diag(1/np.sqrt(Rd)) @ HXp / np.sqrt(Ne - 1)       \n            #svd of S_hat transposed\n            U, s, Vh = np.linalg.svd(S_hat.T, full_matrices=False) \n            #perturbation weight\n            mat = np.diag(1 / np.sqrt(1 + np.square(s)))\n            Wp1 = mat @ U .T\n            Wp  = U @ Wp1\n            #innovation\n            d = Y[i_m, :].T - mY\n            #mean weight\n            D = np.linalg.inv(np.sqrt(R)) @ d\n            D2 = Vh @ D\n            D3 = np.diag(1/(1+np.square(s))) @ np.diag(s) @ D2\n            wm = U @ D3  / np.sqrt(Ne - 1)      \n            #add pert and mean (!row-major formulation in Python!)\n            W = Wp + wm[:,None]        \n            # ==== correct/analyze ====\n            Xa = mX[:,None] + Xfp @ W\n            mX = np.mean(Xa[:,:], axis=1)                   # state analysis ensemble mean\n            Xap = (Xa[:,:] - mX[:, None]) / np.sqrt(Ne - 1) # state analysis anomaly\n            P = Xap @ Xap.T                                 # update covariance\n            i_m = i_m + 1\n        else:\n            Xa[:,:] = Xf  # when there is no obs, then state=forecast\n        # ==== save ====\n        enkf_m[i+1] = mX                # save KF state estimate (mean)\n        enkf_P[i+1] = P                 # save KF error estimate (covariance)\n    return enkf_m, enkf_P\n\n\n# Initialize and run the analysis\nsig_w = 0.0015\nsig_v = 0.15\nQ  = sig_w**2 * np.eye(3) #* 1.e-6 # for comparison with DT\nR  = sig_v**2 * np.eye(3)\n\nx0 = np.array([2., 3., 4.]) # a little off wrt. [1,1,1]\nsig_vv = 0.1\nP0 = np.eye(3) * sig_vv**2 # Initial estimate covariance\nNe = 10\nXa, P = eTKF_Lorenz63_DA(x0, P0, Q, R, y, ind_m, Nt, Nm, Ne=10)\n\n\n# Post-process and plot the results\n# generate unfiltered state\nXb = np.empty((Nt+1, 3))\nXb[0,:] = x0\nfor i in range(Nt):\n    Xb[i+1,:] =  RK4(Lorenz63, Xb[i,:], dt, sigma, rho, beta)    \n# plot state and measurements\nt = np.linspace(0, Nt, Nt+1) * dt # time vector\nT_m = 2.\nfig, ax = plt.subplots(nrows=3,ncols=1, figsize=(10,8))\nax = ax.flat\n\nfor k in range(3):\n    ax[k].plot(t,xTrue[:,k], label='True')#, linewidth = 3)\n    ax[k].plot(t,Xa[:,k], '--', label='EnKF analysis')#, linewidth = 3)\n    ax[k].plot(t[ind_m],y[:,k], 'o', fillstyle='none', \\\n               label='Observation')#, markersize = 8, markeredgewidth = 2)\n    ax[k].plot(t,Xb[:,k], ':', label='Unfiltered')#, linewidth = 3)\n    ax[k].set_xlabel('t')\n    ax[k].axvspan(0, T_m, color='lightgray', alpha=0.4, lw=0)    \nax[0].legend(loc=\"center\", bbox_to_anchor=(0.5,1.25),ncol =4,fontsize=15)   \nax[0].set_ylabel('x(t)')\nax[1].set_ylabel('y(t)')\nax[2].set_ylabel('z(t)')\nfig.subplots_adjust(hspace=0.5)",
    "crumbs": [
      "Ensemble Filters",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Example 4: ETKF for Lorenz63 system</span>"
    ]
  },
  {
    "objectID": "02examples/EnKF/032EnKFExample4.html#conclusion",
    "href": "02examples/EnKF/032EnKFExample4.html#conclusion",
    "title": "16  Example 4: ETKF for Lorenz63 system",
    "section": "16.3 Conclusion",
    "text": "16.3 Conclusion\nThe ensemble transform Kalman filter, even with very sparse observations and a chaotic system, does an excellent job of\n\ntracking within the DA window,\nforecasting way beyond the window, whereas the unfiltered/unassimilated, freely evolving system deviates considerably, as is to be expected from the chaotic Lorenz system.\n\nFrom \\(t \\approx 8,\\) the KF starts to deviate very slightly, mostly a phase difference. However, in real situations, there will already be new measurements available by this time. Then the KF analysis will kick in again, and rectify the trajectory.\n\n\n\n\nAsch, Mark. 2022. A Toolbox for Digital Twins: From Model-Based to Data-Driven. Philadelphia, PA: Society for Industrial; Applied Mathematics. https://doi.org/10.1137/1.9781611976977.\n\n\nAsch, Mark, Marc Bocquet, and Maëlle Nodet. 2016. Data Assimilation: Methods, Algorithms, and Applications. Philadelphia, PA: Society for Industrial; Applied Mathematics. https://doi.org/10.1137/1.9781611974546.\n\n\nCalvello, Edoardo, Sebastian Reich, and Andrew M. Stuart. 2022. “Ensemble Kalman Methods: A Mean Field Perspective.” arXiv (to appear in Acta Numerica 2025). http://arxiv.org/abs/2209.11371.\n\n\nCarrillo, J. A., F. Hoffmann, A. M. Stuart, and U. Vaes. 2024a. “Statistical Accuracy of Approximate Filtering Methods.” https://arxiv.org/abs/2402.01593.\n\n\n———. 2024b. “The Mean Field Ensemble Kalman Filter: Near-Gaussian Setting.” https://arxiv.org/abs/2212.13239.\n\n\nDashti, Masoumeh, and Andrew M. Stuart. 2015. “The Bayesian Approach to Inverse Problems.” In Handbook of Uncertainty Quantification, edited by Roger Ghanem, David Higdon, and Houman Owhadi, 1–118. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-319-11259-6_7-1.\n\n\nHuang, Daniel Zhengyu, Jiaoyang Huang, Sebastian Reich, and Andrew M Stuart. 2022. “Efficient Derivative-Free Bayesian Inference for Large-Scale Inverse Problems.” Inverse Problems 38 (12): 125006. https://doi.org/10.1088/1361-6420/ac99fa.\n\n\nIglesias, Marco A, Kody J H Law, and Andrew M Stuart. 2013. “Ensemble Kalman Methods for Inverse Problems.” Inverse Problems 29 (4): 045001. https://doi.org/10.1088/0266-5611/29/4/045001.\n\n\nLaw, Kody, Andrew Stuart, and Konstantinos Zygalakis. 2015. Data Assimilation: A Mathematical Introduction. Vol. 62. Texts in Applied Mathematics. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-319-20325-6.\n\n\nSanita Vetra-Carvalho, Lars Nerger, Peter Jan van Leeuwen, and Jean-Marie Beckers. 2018. “State-of-the-Art Stochastic Data Assimilation Methods for High-Dimensional Non-Gaussian Problems.” Tellus A: Dynamic Meteorology and Oceanography 70 (1): 1–43. https://doi.org/10.1080/16000870.2018.1445364.\n\n\nSärkkä, S., and L. Svensson. 2023. Bayesian Filtering and Smoothing. 2nd ed. Institute of Mathematical Statistics Textbooks. Cambridge University Press. https://doi.org/10.1017/9781108917407.\n\n\nWu, Jin-Long, Matthew E. Levine, Tapio Schneider, and Andrew Stuart. 2023. “Learning about Structural Errors in Models of Complex Dynamical Systems.” https://arxiv.org/abs/2401.00035.",
    "crumbs": [
      "Ensemble Filters",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Example 4: ETKF for Lorenz63 system</span>"
    ]
  },
  {
    "objectID": "01theory/041BIP.html",
    "href": "01theory/041BIP.html",
    "title": "17  Bayesian Inversion",
    "section": "",
    "text": "17.1 Introduction\nBayesian approaches, provide a foundation for inference from noisy and limited data, a natural mechanism for regularization in the form of prior information, and in very general cases—e.g., non-linear forward operators, non-Gaussian errors—a quantitative assessment of uncertainty in the results. Indeed, the output of Bayesian inference is not a single value for the quantity of interest, but a probability distribution that summarizes all available information about this quantity, be it a vector of parameters or a function (i.e., a signal or spatial field). Exploration of this posterior distribution—and thus estimating means, higher moments, and marginal densities of the inverse solution—may require repeated evaluations of the forward operator. For complex physical models and high-dimensional model spaces, this can be computationally prohibitive.\nBayesian inference provides an attractive setting for the solution of inverse problems. Measurement errors,forward model uncertainties, and complex prior information can all be combined to yield a rigorous and quantitative assessment of uncertainty in the solution of the inverse problem.",
    "crumbs": [
      "Inverse Problems",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Bayesian Inversion</span>"
    ]
  },
  {
    "objectID": "01theory/041BIP.html#theory",
    "href": "01theory/041BIP.html#theory",
    "title": "17  Bayesian Inversion",
    "section": "17.2 Theory",
    "text": "17.2 Theory\nA Bayesian Inverse Problem (BIP) is defined as follows:\n\nGiven:\n\nobservational data and their uncertainties,\na (possibly stochastic) forward model that maps model parameters to observations,\nand a prior probability distribution on model parameters that encodes any prior knowledge or assumptions about the parameters.\n\nFind:\n\nthe posterior probability distribution of the parameters conditioned on the observational data.\n\n\nThis probability density function (pdf) is defined as the Bayesian solution of the inverse problem. The posterior distribution assigns to any candidate set of parameter fields our belief (expressed as a probability) that a member of this candidate set is the ``true’’ parameter field that gave rise to the observed data.\nOf course, all of this is summarized in Bayes’ theorem, expressed as folllows.\nWhat can be said about the value of an unknown or poorly known variable/parameter, \\(\\theta,\\) that represents the parameters of the system, if we have some measured data \\(\\mathcal{D}\\) and a model \\(\\mathcal{M}\\) of the underlying mechanism that generated the data? This is precisely the Bayesian context, where we seek a quantification of the uncertainty in our knowledge of the parameters that according to Bayes’ Theorem takes the form\n\\[\\begin{equation}\\label{eq:Bayes-2}\np(\\theta\\mid\\mathcal{D})=\\frac{p(\\mathcal{D}\\mid\\theta)p(\\theta)}{p(\\mathcal{D})}=\\frac{p(\\mathcal{D}\\mid\\theta)p(\\theta)}{\\int_{\\theta}p(\\mathcal{D}\\mid\\theta)p(\\theta)}.\n\\end{equation}\\]\nHere, the physical model that generates the data is represented by the conditional probability (also known as the likelihood) \\(p(\\mathcal{D}\\mid\\theta),\\) and the prior knowledge of the system by the term \\(p(\\theta).\\) The denominator is considered as a normalizing factor and represents the total probability of \\(\\mathcal{D}.\\) From these we can then calculate the resulting posterior probability, \\(p(\\theta\\mid\\mathcal{D}).\\) A generalization can include a model, \\(\\mathcal{M},\\) for the parameters. In this case (6.1) can be written as\n\\[\\begin{equation}\\label{eq:Bayes-3}\n    p(\\theta\\mid\\mathcal{D},\\mathcal{M})=\\frac{p(\\mathcal{D}\\mid\\theta,\\mathcal{M})p(\\theta\\mid \\mathcal{M})}{p(\\mathcal{D}\\mid \\mathcal{M})}.\n\\end{equation}\\]\nThis is depicted in the flowchart.",
    "crumbs": [
      "Inverse Problems",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Bayesian Inversion</span>"
    ]
  },
  {
    "objectID": "01theory/041BIP.html#inverse-problem",
    "href": "01theory/041BIP.html#inverse-problem",
    "title": "17  Bayesian Inversion",
    "section": "17.3 Inverse Problem",
    "text": "17.3 Inverse Problem\nWe consider the inverse problem of recovering an unknown function \\(u\\) from known data \\(y\\) related by\n\\[\ny = \\mathcal{G}(u) + \\eta,\n\\] where \\(\\mathcal{G}\\) denotes the forward model from inputs to observables, \\(\\eta \\sim \\mathcal{N}(0, \\Gamma)\\) represents the model error and observation noise, with \\(\\Gamma\\) a positive definite noise covariance matrix. In the Bayesian approach, \\(u\\) is a random variable with prior distribution \\(p_u(u).\\) Then, the Bayesian solution of the inverse problem is the posterior distribution \\(p_{u\\vert y}(u)\\) of \\(u\\) given \\(y,\\) which by formal application of Bayes’ Theorem can be characterized by\n\\[\n  p_{u\\vert y}(u) \\propto \\exp \\left(  -\\Phi(u;y) \\right) p_u (u),\n\\] with \\[\n  \\Phi(u;y) \\doteq \\frac{1}{2} \\left\\Vert y - \\mathcal{G}(u) \\right\\Vert_{\\Gamma}^2.\n\\]\nTBC…",
    "crumbs": [
      "Inverse Problems",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Bayesian Inversion</span>"
    ]
  },
  {
    "objectID": "01theory/041BIP.html#examples",
    "href": "01theory/041BIP.html#examples",
    "title": "17  Bayesian Inversion",
    "section": "17.4 Examples",
    "text": "17.4 Examples\n\n\n\n\nAsch, Mark. 2022. A Toolbox for Digital Twins: From Model-Based to Data-Driven. Philadelphia, PA: Society for Industrial; Applied Mathematics. https://doi.org/10.1137/1.9781611976977.\n\n\nAsch, Mark, Marc Bocquet, and Maëlle Nodet. 2016. Data Assimilation: Methods, Algorithms, and Applications. Philadelphia, PA: Society for Industrial; Applied Mathematics. https://doi.org/10.1137/1.9781611974546.\n\n\nCalvello, Edoardo, Sebastian Reich, and Andrew M. Stuart. 2022. “Ensemble Kalman Methods: A Mean Field Perspective.” arXiv (to appear in Acta Numerica 2025). http://arxiv.org/abs/2209.11371.\n\n\nCarrillo, J. A., F. Hoffmann, A. M. Stuart, and U. Vaes. 2024a. “Statistical Accuracy of Approximate Filtering Methods.” https://arxiv.org/abs/2402.01593.\n\n\n———. 2024b. “The Mean Field Ensemble Kalman Filter: Near-Gaussian Setting.” https://arxiv.org/abs/2212.13239.\n\n\nDashti, Masoumeh, and Andrew M. Stuart. 2015. “The Bayesian Approach to Inverse Problems.” In Handbook of Uncertainty Quantification, edited by Roger Ghanem, David Higdon, and Houman Owhadi, 1–118. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-319-11259-6_7-1.\n\n\nHuang, Daniel Zhengyu, Jiaoyang Huang, Sebastian Reich, and Andrew M Stuart. 2022. “Efficient Derivative-Free Bayesian Inference for Large-Scale Inverse Problems.” Inverse Problems 38 (12): 125006. https://doi.org/10.1088/1361-6420/ac99fa.\n\n\nIglesias, Marco A, Kody J H Law, and Andrew M Stuart. 2013. “Ensemble Kalman Methods for Inverse Problems.” Inverse Problems 29 (4): 045001. https://doi.org/10.1088/0266-5611/29/4/045001.\n\n\nLaw, Kody, Andrew Stuart, and Konstantinos Zygalakis. 2015. Data Assimilation: A Mathematical Introduction. Vol. 62. Texts in Applied Mathematics. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-319-20325-6.\n\n\nSanita Vetra-Carvalho, Lars Nerger, Peter Jan van Leeuwen, and Jean-Marie Beckers. 2018. “State-of-the-Art Stochastic Data Assimilation Methods for High-Dimensional Non-Gaussian Problems.” Tellus A: Dynamic Meteorology and Oceanography 70 (1): 1–43. https://doi.org/10.1080/16000870.2018.1445364.\n\n\nSärkkä, S., and L. Svensson. 2023. Bayesian Filtering and Smoothing. 2nd ed. Institute of Mathematical Statistics Textbooks. Cambridge University Press. https://doi.org/10.1017/9781108917407.\n\n\nWu, Jin-Long, Matthew E. Levine, Tapio Schneider, and Andrew Stuart. 2023. “Learning about Structural Errors in Models of Complex Dynamical Systems.” https://arxiv.org/abs/2401.00035.",
    "crumbs": [
      "Inverse Problems",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Bayesian Inversion</span>"
    ]
  },
  {
    "objectID": "01theory/042EKI.html",
    "href": "01theory/042EKI.html",
    "title": "18  Ensemble Kalman Inversion (EKI)",
    "section": "",
    "text": "18.1 Properties of EKI",
    "crumbs": [
      "Inverse Problems",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Ensemble Kalman Inversion (EKI)</span>"
    ]
  },
  {
    "objectID": "01theory/042EKI.html#properties-of-eki",
    "href": "01theory/042EKI.html#properties-of-eki",
    "title": "18  Ensemble Kalman Inversion (EKI)",
    "section": "",
    "text": "a derivative- and gradient-free optimization method;\nfull uncertainty quantification;\nrelatively few evaluations of the (possibly expensive) forward model;\nrobust to noise in the evaluation of the model.",
    "crumbs": [
      "Inverse Problems",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Ensemble Kalman Inversion (EKI)</span>"
    ]
  },
  {
    "objectID": "01theory/042EKI.html#formulation",
    "href": "01theory/042EKI.html#formulation",
    "title": "18  Ensemble Kalman Inversion (EKI)",
    "section": "18.2 Formulation",
    "text": "18.2 Formulation\nRecall the inverse problem: recover an unknown function \\(\\theta\\) from known data \\(y\\) related by\n\\[\ny = \\mathcal{G}(\\theta) + \\eta,\n\\] where \\(\\mathcal{G}\\) denotes the forward model from inputs to observables, \\(\\eta \\sim \\mathcal{N}(0, \\Sigma_{\\eta})\\) represents the model error and observation noise, with \\(\\Sigma_{\\eta}\\) a positive definite noise covariance matrix.\nConceptually, the idea of EKI is very simple. Just apply the standard EnKF to an augmented system of state plus parameters, for a given number of iterations, to invert for \\(\\theta.\\) We begin by pairing the parameter-to-data map with a dynamical system for the parameter, and then employ techniques from filtering to estimate the parameter given the data.\nThe system for EKI is written as,\n\\[\\begin{align*}\n    \\theta_{n+1} &=  \\theta_n\\\\\n    y_{n+1} &=  \\mathcal{G}(\\theta_{n+1}) + \\eta_{n+1},\n\\end{align*}\\]\nwhere the operator \\(\\mathcal{G}\\) contains both the system dynamics and the observation function.\nConsider the stochastic dynamical system\n\\[\\begin{align}\n  &\\textrm{evolution:}    && \\theta_{n+1} = \\theta_{n}  +  \\omega_{n+1}, &&\\omega_{n+1} \\sim \\mathcal{N}(0,\\Sigma_{\\omega}),\\\\\n  &\\textrm{observation:}  && x_{n+1} = \\mathcal{F}(\\theta_{n+1}) + \\nu_{n+1}, &&\\nu_{n+1} \\sim \\mathcal{N}(0,\\Sigma_{\\nu}).\n\\end{align}\\]\nWe seek the best Gaussian approximation of the posterior distribution of \\(\\theta\\) for ill-posed inverse problems, where the prior is a Gaussian, \\(\\theta_0 \\sim \\mathcal{N}(r_0, \\Sigma_0).\\)\nConsider the case:\n\n\\(\\Sigma_{\\omega} = \\frac{\\Delta t}{1 - \\Delta t} C_{n}\\), where \\(C_{n}\\) is the covariance estimation at the current step.\n\n\\(x_{n+1} = \\begin{bmatrix} y \\\\ r_0 \\end{bmatrix}, \\quad\n\\mathcal{F}(\\theta) = \\begin{bmatrix} \\mathcal{G}(\\theta) \\\\ \\theta  \\end{bmatrix},\\quad\n\\textrm{and}\\quad \\Sigma_{\\nu} = \\frac{1}{\\Delta t} \\begin{bmatrix} \\Sigma_{\\eta} & 0 \\\\ 0 & \\Sigma_0\\end{bmatrix}\\)\n\nwhere \\(r_0\\) and \\(\\Sigma_0\\) are prior mean and covariance, and the hyperparameter \\(0 &lt; \\Delta t &lt; 1\\) is set to \\(1/2.\\)\nLinear Analysis\nIn the linear setting,\n\\[ \\mathcal{G}(\\theta) = G\\cdot \\theta \\qquad F = \\begin{bmatrix} G \\\\ I  \\end{bmatrix} \\]\nThe update equations become\n\\[\\begin{align*}\n    \\hat{m}_{n+1} &=  m_n\\\\\n    \\hat{C}_{n+1} &=  \\frac{1}{1 - \\Delta t} C_{n}\n\\end{align*}\\]\nand\n\\[\\begin{align*}\n        m_{n+1} &= m_{n} + \\hat{C}_{n+1} F^T (F  \\hat{C}_{n+1} F^T + \\Sigma_{\\nu,n+1})^{-1} (x_{n+1} - F m_{n}) \\\\\n         C_{n+1}&= \\hat{C}_{n+1} - \\hat{C}_{n+1} F^T(F  \\hat{C}_{n+1} F^T + \\Sigma_{\\nu,n+1})^{-1} F \\hat{C}_{n+1},\n\\end{align*}\\]\nWe have the following theorem about the convergence of the algorithm in the setting of the linear forward model:\n\n\n\n\n\n\nTheorem (EKI)\n\n\n\nAssume that the prior covariance matrix \\(\\Sigma_{0} \\succ 0\\) and initial covariance matrix \\(C_{0} \\succ 0.\\) The iteration for the conditional mean \\(m_n\\) and covariance matrix \\(C_{n}\\) characterizing the distribution of \\(\\theta_n|Y_n\\) converges exponentially fast to the posterior mean, \\(m_{\\mathrm{post}},\\) and covariance, \\(C_{\\mathrm{post}}.\\)\n\n\nReference\nPlease see Efficient Derivative-free Bayesian Inference for Large-Scale Inverse Problems, the arXiv version of (Huang et al. 2022).\nNote\nThis result has been recently extended to near-Gaussian, nonlinear cases in (Carrillo et al. 2024a) and (Carrillo et al. 2024b).",
    "crumbs": [
      "Inverse Problems",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Ensemble Kalman Inversion (EKI)</span>"
    ]
  },
  {
    "objectID": "01theory/042EKI.html#algorithms-eki-etki",
    "href": "01theory/042EKI.html#algorithms-eki-etki",
    "title": "18  Ensemble Kalman Inversion (EKI)",
    "section": "18.3 Algorithms: EKI, ETKI",
    "text": "18.3 Algorithms: EKI, ETKI\nWe will formulate the inversion for the stochastic ensemble filter (EKI) and then for the ensemble transfer filter (ETKI), the latter having a better convergence behavior.\n\n\n\n\n\n\nEKI\n\n\n\n\nPrediction step: \\[\n\\hat{m}_{n+1} = m_n, \\quad  \n\\hat{\\theta}^j_{n+1} = \\hat{m}_{n+1} + \\sqrt{\\frac{1}{1-\\Delta \\tau}} \\left(  \\theta_n^j - m_n \\right)\n\\]\nAnalysis step:\n\n\\[\\begin{align*}\n&\\hat{x}_{n+1}^{j} = \\mathcal{F}(\\hat{\\theta}_{n+1}^{j}) , \\qquad \\hat{x}_{n+1} = \\frac{1}{J}\\sum_{j=1}^{J}\\hat{x}_{n+1}^{j},\\\\\n&\\hat{C}_{n+1}^{\\theta x} = \\frac{1}{J-1}\\sum_{j=1}^{J}(\\hat{\\theta}_{n+1}^{j} - \\hat{m}_{n+1})(\\hat{x}_{n+1}^{j} - \\hat{x}_{n+1})^\\mathrm{T} , \\\\\n&\\hat{C}_{n+1}^{xx} = \\frac{1}{J-1}\\sum_{j=1}^{J}(\\hat{x}_{n+1}^{j} - \\hat{x}_{n+1})(\\hat{x}_{n+1}^{j} - \\hat{x}_{n+1})^\\mathrm{T} +\\Sigma_{\\nu,n+1}, \\\\\n&\\theta_{n+1}^{j} = \\hat{\\theta}_{n+1}^{j} + \\hat{C}_{n+1}^{\\theta x}\\left(\\hat{C}_{n+1}^{xx}\\right)^{-1}(x - \\hat{x}_{n+1}^{j} - \\nu_{n+1}^{j}),\\\\\n&m_{n+1} = \\frac{1}{J} \\sum_{j=1}^{J} \\theta_{n+1}^{j} ,\n\\end{align*}\\]\nwhere \\(\\nu_{n+1}^{j} \\sim \\mathcal{N}(0,\\Sigma_{\\nu,n+1}).\\)\n\n\nA few remarks.\n\nThe covariance update equation, \\[\nC_{n+1} = \\hat{C}_{n+1} + \\hat{C}_{n+1}^{\\theta x} \\left( \\hat{C}_{n+1}^{x x}  \\right) \\left(\\hat{C}_{n+1}^{\\theta x}\\right)^\\mathrm{T}\n\\] does not hold here. This will be remedied by the ETKF.\nThe factor \\(\\sqrt{1/(1-\\Delta \\tau)}\\) produces covariance inflation and prevents filter collapse.\nIn the analysis step, noise is added in the \\(\\theta\\) update instead of in the \\(\\hat{x}\\) step to ensure symmetry and positive-definiteness of \\(\\hat{C}_{n+1}^{xx}.\\)\n\nFor the ensemble transform filter (described above in Section 15.3), we need to define some matrix square roots for the covariance matrices \\(\\hat{C}_{n+1}^{xx}\\) and \\(\\hat{C}_{n+1}^{\\theta x},\\) as follows. We denote the matrix square roots \\(\\hat{Z}_{n+1},\\, Z_{n+1} \\in \\mathbb{R}^{N_{\\theta}\\times J}\\) of \\(\\hat{C}_{n+1}^{\\theta x},\\,C_{n+1}^{\\theta x}\\) and \\(\\hat{\\mathcal{Y}}_{n+1}\\) of \\(\\hat{C}_{n+1}^{x x},\\) and define,\n\\[\\begin{align*}\n    \\hat{Z}_{n+1} &= \\frac{1}{\\sqrt{J-1}}\\Big(\\hat{\\theta}_{n+1}^{1} - \\hat{m}_{n+1}\\quad \\hat{\\theta}_{n+1}^{2} - \\hat{m}_{n+1}\\quad...\\quad\\hat{\\theta}_{n+1}^{J} - \\hat{m}_{n+1} \\Big),\\\\\n    Z_{n+1} &= \\frac{1}{\\sqrt{J-1}}\\Big(\\theta_{n+1}^{1} - m_{n+1}\\quad \\theta_{n+1}^{2} - m_{n+1}\\quad...\\quad\\theta_{n+1}^{J} - m_{n+1} \\Big),\\\\\n    \\hat{\\mathcal{Y}}_{n+1} &= \\frac{1}{\\sqrt{J-1}}\\Big(\\hat{x}_{n+1}^{1} - \\hat{x}_{n+1}\\quad \\hat{x}_{n+1}^{2} - \\hat{x}_{n+1}\\quad...\\quad\\hat{x}_{n+1}^{J} - \\hat{x}_{n+1} \\Big).\n\\end{align*}\\]\n\n\n\n\n\n\nETKI\n\n\n\n\nPrediction step :\n\n\\[\\begin{align*}\n\\hat{\\theta}_{n+1}^{j} &= \\theta_{n}^{j}  + \\omega_{n+1}^{j},\\\\\n\\hat{m}_{n+1} &= \\frac{1}{J}\\sum_{j=1}^{J}\\hat{\\theta}_{n+1}^{j}\n\\end{align*}\\]\n\nAnalysis step :\n\n\\[\\begin{align*}\n&m_{n+1} = \\hat{m}_{n+1} + \\hat{C}_{n+1}^{\\theta x}\\left(\\hat{C}_{n+1}^{xx}\\right)^{-1}(x - \\hat{x}_{n+1})\\\\\n&Z_{n+1} = \\hat{Z}_{n+1} T\n\\end{align*}\\]\nwhere \\(T = P(\\Gamma + I)^{-\\frac{1}{2}}P^\\mathrm{T}\\), with\n\\[\\begin{align*}\n\\textrm{SVD:} \\quad \\hat{\\mathcal{Y}}_{n+1} \\Sigma_{\\nu,n+1}^{-1} \\hat{\\mathcal{Y}}_{n+1} = P\\Gamma P^\\mathrm{T}\n\\end{align*}\\]\nRemark\nWhen \\(\\Sigma_{\\omega} = \\gamma C_n\\), the prediction step can be treated deterministically, as follows,\n\\[\\begin{align*}\n\\hat{m}_{n+1} &= m_n \\\\\n\\hat{\\theta}_{n+1}^{j} &= \\hat{m}_{n+1} + \\sqrt{1 + \\gamma} \\; (\\theta_n^{j} - m_n)\\\\\n\\end{align*}\\]",
    "crumbs": [
      "Inverse Problems",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Ensemble Kalman Inversion (EKI)</span>"
    ]
  },
  {
    "objectID": "01theory/042EKI.html#conclusions",
    "href": "01theory/042EKI.html#conclusions",
    "title": "18  Ensemble Kalman Inversion (EKI)",
    "section": "18.4 Conclusions",
    "text": "18.4 Conclusions\nKalman-based inversion has been widely used to construct derivative-free optimization and s ampling methods for nonlinear inverse problems. In the paper (Huang et al. 2022), the authors developed new Kalman-based inversion methods, for Bayesian inference and uncertainty quantification, which build on the work in both optimization and sampling. They propose a new method for Bayesian inference based on filtering a novel mean-field dynamical system subject to partial noisy observations, and which depends on the law of its own filtering distribution, together with application of the Kalman methodology. Theoretical guarantees are presented: for linear inverse problems, the mean and covariance obtained by the method converge exponentially fast to the posterior mean and covariance. For nonlinear inverse problems, numerical studies indicate the method delivers an excellent approximation of the posterior distribution for problems which are not too far from Gaussian.\nIn terms of performance:\n\nThe methods are shown to be superior to existing coupling/transport methods, collectively known as iterative Kalman methods.\nDeterministic, such as ETKF, rather than stochastic implementations of Kalman methodology are found to be favorable.\n\n\n\n\n\nAsch, Mark. 2022. A Toolbox for Digital Twins: From Model-Based to Data-Driven. Philadelphia, PA: Society for Industrial; Applied Mathematics. https://doi.org/10.1137/1.9781611976977.\n\n\nAsch, Mark, Marc Bocquet, and Maëlle Nodet. 2016. Data Assimilation: Methods, Algorithms, and Applications. Philadelphia, PA: Society for Industrial; Applied Mathematics. https://doi.org/10.1137/1.9781611974546.\n\n\nCalvello, Edoardo, Sebastian Reich, and Andrew M. Stuart. 2022. “Ensemble Kalman Methods: A Mean Field Perspective.” arXiv (to appear in Acta Numerica 2025). http://arxiv.org/abs/2209.11371.\n\n\nCarrillo, J. A., F. Hoffmann, A. M. Stuart, and U. Vaes. 2024a. “Statistical Accuracy of Approximate Filtering Methods.” https://arxiv.org/abs/2402.01593.\n\n\n———. 2024b. “The Mean Field Ensemble Kalman Filter: Near-Gaussian Setting.” https://arxiv.org/abs/2212.13239.\n\n\nDashti, Masoumeh, and Andrew M. Stuart. 2015. “The Bayesian Approach to Inverse Problems.” In Handbook of Uncertainty Quantification, edited by Roger Ghanem, David Higdon, and Houman Owhadi, 1–118. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-319-11259-6_7-1.\n\n\nHuang, Daniel Zhengyu, Jiaoyang Huang, Sebastian Reich, and Andrew M Stuart. 2022. “Efficient Derivative-Free Bayesian Inference for Large-Scale Inverse Problems.” Inverse Problems 38 (12): 125006. https://doi.org/10.1088/1361-6420/ac99fa.\n\n\nIglesias, Marco A, Kody J H Law, and Andrew M Stuart. 2013. “Ensemble Kalman Methods for Inverse Problems.” Inverse Problems 29 (4): 045001. https://doi.org/10.1088/0266-5611/29/4/045001.\n\n\nLaw, Kody, Andrew Stuart, and Konstantinos Zygalakis. 2015. Data Assimilation: A Mathematical Introduction. Vol. 62. Texts in Applied Mathematics. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-319-20325-6.\n\n\nSanita Vetra-Carvalho, Lars Nerger, Peter Jan van Leeuwen, and Jean-Marie Beckers. 2018. “State-of-the-Art Stochastic Data Assimilation Methods for High-Dimensional Non-Gaussian Problems.” Tellus A: Dynamic Meteorology and Oceanography 70 (1): 1–43. https://doi.org/10.1080/16000870.2018.1445364.\n\n\nSärkkä, S., and L. Svensson. 2023. Bayesian Filtering and Smoothing. 2nd ed. Institute of Mathematical Statistics Textbooks. Cambridge University Press. https://doi.org/10.1017/9781108917407.\n\n\nWu, Jin-Long, Matthew E. Levine, Tapio Schneider, and Andrew Stuart. 2023. “Learning about Structural Errors in Models of Complex Dynamical Systems.” https://arxiv.org/abs/2401.00035.",
    "crumbs": [
      "Inverse Problems",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Ensemble Kalman Inversion (EKI)</span>"
    ]
  },
  {
    "objectID": "02examples/EKI/one_dim_EKI.html",
    "href": "02examples/EKI/one_dim_EKI.html",
    "title": "19  Example 1: One-dimensional EKI",
    "section": "",
    "text": "19.1 Implement the one-dimensional EKI for a linear forward operator \\(\\mathcal{G}\\)\ndef eki_one_dim_lin(m_0, C_0, N, G, gamma, y, delt, h):\n    # Inputs:\n    # -------\n    # m_0, C_0: mean value and covariance of inital ensemble\n    # N:        number of iterations\n    # G:        one-dimensional forward operator of the model\n    # gamma:    covariance of the noise in the data\n    # y:        observed data \n    # h:        discretization step  \n    #\n    # Outputs:\n    # -------\n    # U: (JxN) matrix with the computed particles for each iteration\n    # m: vector of length N with the mean value of the particles\n    # C: vector of length N with the covariance of the particles\n    \n    m = np.zeros(N)\n    C = np.zeros(N)\n    U = np.zeros((J,N))\n    \n    #Construct initial ensemble and estimator\n    u_0 = np.random.normal(m_0, C_0, J)\n    U[:,0] = u_0\n    m[0] = np.mean(U[:,0])\n    C[0] = (U[:,0] - m[0]) @ (U[:,0] - m[0]).T / (J-1)\n    \n    for n in range(1,N):\n        \n        # Last iterate under forward operator:\n        G_u    = G*U[:, n-1]\n        Ghat   = np.mean(G_u)\n        U[:,n] = U[:,n-1] + h*(C[n-1] + delt)*G*(1/gamma)*((y - G_u))\n        \n        m[n] = np.mean(U[:,n])\n        C[n] = (U[:,n] - m[n]) @ (U[:,n] - m[n]).T / (J-1)\n            \n    return U,m,C\n#Set Parameters\nJ = 10\ngamma = 1\nm_0 = 0\nC_0 = 9e-1\nm_true = 0\nc_true = C_0\nG = 1.5\nN = 10000\nh = 1/100\ndelt = 1\n\n# Construct data under true parameter\nu_true = np.random.normal(m_true,c_true)\ny = G*u_true\n\nU,m,c = eki_one_dim_lin(m_0, C_0, N, G, gamma, y, delt, h)\nit=N\niterations=list(range(1,(it+1)))\nplt.xlabel('Iterations number n')\nplt.ylabel('Error')\nplt.loglog(iterations,np.sqrt((u_true*np.ones(N) - m)**2/(u_true**2)),\"r\",label='$u^\\dagger-m_n$')\nplt.legend(loc=\"upper right\")\nplt.show()\nit=300\niterations=list(range(1,it+1))\nplt.xlabel('Iteration number n')\nplt.ylabel('Covariance')\nplt.plot(iterations,c[0:it],\"r\", label='$c_n$')\nplt.plot(iterations,np.divide(np.cumsum(c[0:it]),iterations)[0:it],\"g\",label='$N^{-1}\\sum_k^N c_n$')\nplt.legend(loc=\"upper right\")\nplt.show()",
    "crumbs": [
      "Inverse Problems",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Example 1: One-dimensional EKI</span>"
    ]
  },
  {
    "objectID": "02examples/EKI/one_dim_EKI.html#implement-the-one-dimensional-eki-for-an-arbitrary-forward-operator-mathcalg",
    "href": "02examples/EKI/one_dim_EKI.html#implement-the-one-dimensional-eki-for-an-arbitrary-forward-operator-mathcalg",
    "title": "19  Example 1: One-dimensional EKI",
    "section": "19.2 Implement the one dimensional EKI for an arbitrary forward operator \\(\\mathcal{G}\\)",
    "text": "19.2 Implement the one dimensional EKI for an arbitrary forward operator \\(\\mathcal{G}\\)\n\ndef eki_one_dim(m_0, C_0, N, G, gamma, y, h):\n    # Inputs:\n    # -------\n    # m_0, C_0: mean value and covariance of inital ensemble\n    # N:        number of iterations\n    # G:        one-dimensional forward operator of the model\n    # gamma:    covariance of the noise in the data\n    # y:        observed data \n    # h:        discretization step  \n    #\n    # Outputs:\n    # -------\n    # U: (JxN) matrix with the computed particles for each iteration\n    # m: vector of length N with the mean value of the particles\n    # C: vector of length N with the covariance of the particles\n        \n    m = np.zeros(N)\n    C = np.zeros(N)\n    U = np.zeros((J,N))\n    \n    #Construct initial ensemble and estimator\n    u_0 = np.random.normal(m_0, C_0, J)\n    U[:,0] = u_0\n    m[0] = np.mean(U[:,0])\n    C[0] = (U[:,0] - m[0]) @ (U[:,0] - m[0]).T / (J-1)\n    \n    for n in range(1,N):\n        \n        # Last iterate under forward operator:\n        G_u = G(U[:,n-1])\n        uhat = np.mean(U[:,n-1])\n        Ghat = np.mean(G_u)\n        \n        cov_up = (U[:,n-1] - uhat) @ (G_u - Ghat).T / (J-1)\n        cov_pp = (G_u - Ghat) @ (G_u - Ghat).T / (J-1)\n        \n        U[:,n] = U[:,n-1] + cov_up*h/(h*cov_pp + gamma)*(y - G_u)        \n        \n        m[n] = np.mean(U[:,n])\n        C[n] = (U[:,n]-m[n])@(U[:,n]-m[n]).T/(J-1)\n            \n    return U, m, C\n\n\n#Set Parameters\nJ = 10\nr = 10\nk = 10\ngamma = 1\nm_0 = 0\nC_0 = 9e-2\nm_true = 0\nC_true = C_0\nN = 10000\nh = 1/100 # 1/N\n\ndef forward_log(z, k, r, h):\n    return k/(1 + np.exp(-r*k*h)*(k/z-1))\n\n# Construct data under true parameter\nu_true = np.random.normal(m_true, C_true)\ny = forward_log(u_true, k, r, h)\n\n# Use partial function\npartial_log = functools.partial(forward_log, k=10, r=10, h=1/100)\n\nU, m, C = eki_one_dim(m_0, C_0, N, partial_log, gamma, y, h)\n\n\nit = N\niterations=list(range(1,it+1))\nplt.xlabel('Iterations number n')\nplt.ylabel('Error')\nplt.plot(iterations,np.sqrt((u_true*np.ones(N) - m)**2/(u_true**2)),\"r\", label='$u^\\dagger-m_n$')\nplt.legend(loc=\"upper right\")\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.plot(iterations, u_true*np.ones(N), label=\"true\")\nplt.plot(iterations, m, label=\"inversion\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nit = 300\niterations = list(range(1,it+1))\nplt.xlabel('Iteration number n')\nplt.ylabel('Covariance')\nplt.plot(iterations,C[0:it],\"r\", label='$C_n$')\nplt.plot(iterations,np.divide(np.cumsum(C[0:it]),iterations)[0:it],\"g\",label='$N^{-1}\\sum_k^N C_n$')\nplt.legend(loc=\"upper right\")\nplt.show()",
    "crumbs": [
      "Inverse Problems",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Example 1: One-dimensional EKI</span>"
    ]
  },
  {
    "objectID": "02examples/EKI/one_dim_EKI.html#conclusions",
    "href": "02examples/EKI/one_dim_EKI.html#conclusions",
    "title": "19  Example 1: One-dimensional EKI",
    "section": "19.3 Conclusions",
    "text": "19.3 Conclusions\nThe convergence is very slow, and not very accurate. This will be remedied by the mean-field approach.\n\n\n\n\nAsch, Mark. 2022. A Toolbox for Digital Twins: From Model-Based to Data-Driven. Philadelphia, PA: Society for Industrial; Applied Mathematics. https://doi.org/10.1137/1.9781611976977.\n\n\nAsch, Mark, Marc Bocquet, and Maëlle Nodet. 2016. Data Assimilation: Methods, Algorithms, and Applications. Philadelphia, PA: Society for Industrial; Applied Mathematics. https://doi.org/10.1137/1.9781611974546.\n\n\nCalvello, Edoardo, Sebastian Reich, and Andrew M. Stuart. 2022. “Ensemble Kalman Methods: A Mean Field Perspective.” arXiv (to appear in Acta Numerica 2025). http://arxiv.org/abs/2209.11371.\n\n\nCarrillo, J. A., F. Hoffmann, A. M. Stuart, and U. Vaes. 2024a. “Statistical Accuracy of Approximate Filtering Methods.” https://arxiv.org/abs/2402.01593.\n\n\n———. 2024b. “The Mean Field Ensemble Kalman Filter: Near-Gaussian Setting.” https://arxiv.org/abs/2212.13239.\n\n\nDashti, Masoumeh, and Andrew M. Stuart. 2015. “The Bayesian Approach to Inverse Problems.” In Handbook of Uncertainty Quantification, edited by Roger Ghanem, David Higdon, and Houman Owhadi, 1–118. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-319-11259-6_7-1.\n\n\nHuang, Daniel Zhengyu, Jiaoyang Huang, Sebastian Reich, and Andrew M Stuart. 2022. “Efficient Derivative-Free Bayesian Inference for Large-Scale Inverse Problems.” Inverse Problems 38 (12): 125006. https://doi.org/10.1088/1361-6420/ac99fa.\n\n\nIglesias, Marco A, Kody J H Law, and Andrew M Stuart. 2013. “Ensemble Kalman Methods for Inverse Problems.” Inverse Problems 29 (4): 045001. https://doi.org/10.1088/0266-5611/29/4/045001.\n\n\nLaw, Kody, Andrew Stuart, and Konstantinos Zygalakis. 2015. Data Assimilation: A Mathematical Introduction. Vol. 62. Texts in Applied Mathematics. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-319-20325-6.\n\n\nSanita Vetra-Carvalho, Lars Nerger, Peter Jan van Leeuwen, and Jean-Marie Beckers. 2018. “State-of-the-Art Stochastic Data Assimilation Methods for High-Dimensional Non-Gaussian Problems.” Tellus A: Dynamic Meteorology and Oceanography 70 (1): 1–43. https://doi.org/10.1080/16000870.2018.1445364.\n\n\nSärkkä, S., and L. Svensson. 2023. Bayesian Filtering and Smoothing. 2nd ed. Institute of Mathematical Statistics Textbooks. Cambridge University Press. https://doi.org/10.1017/9781108917407.\n\n\nWu, Jin-Long, Matthew E. Levine, Tapio Schneider, and Andrew Stuart. 2023. “Learning about Structural Errors in Models of Complex Dynamical Systems.” https://arxiv.org/abs/2401.00035.",
    "crumbs": [
      "Inverse Problems",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Example 1: One-dimensional EKI</span>"
    ]
  },
  {
    "objectID": "01theory/011BayesTheory.html#bayesian-regression",
    "href": "01theory/011BayesTheory.html#bayesian-regression",
    "title": "1  Bayes’ Theorem",
    "section": "1.2 Bayesian Regression",
    "text": "1.2 Bayesian Regression\n\n1.2.1 Classical Linear Regression\nWe recall the classical linear regression models. We present a general formulation that will prepare the terrain for the Bayesian approach.\nRecall that in a regression problem we want to model the relationship between a dependent variable, \\(y,\\) that is observed and independent variables, \\({x_1,x_2, \\ldots, x_p},\\) that represent the properties of a process. We have at our disposal \\(n\\) data samples \\[\\begin{equation}\\label{eq:linreg_data}\n{\\cal D}=\\{ (\\mathbf {x}_i,\\,y_i),\\; i=1,\\dots, n \\} ,\n\\end{equation}\\] from which we can estimate the relationship between \\(y\\) and \\(x,\\) where \\(\\mathbf {x}_i = [x_{i1}, x_{i2}, \\ldots, x_{ip}]\\).\nThe data pairs come from observations, and we postulate a linear relationship between them of the form \\[\\begin{align*}\ny_{i} & =\\mathbf{x}_{i} \\boldsymbol{\\beta}+\\epsilon_{i}, \\quad i=1,\\ldots,n,\\\\\n& =\\beta_{0}+\\beta_{1}x_{i1}+\\beta_{2}x_{i2}+\\cdots+\\beta_{p}x_{ip}+\\epsilon_{i},\n\\end{align*}\\] where\n\n\\(x_{ij}\\) is the \\(i\\)-th value of the \\(j\\)-th covariate, \\(j=1,\\ldots,p,\\)\n\\(\\boldsymbol{\\beta}=[\\beta_0,\\beta_1,\\ldots,\\beta_p]^\\mathrm{T},\\)\nand \\(\\epsilon_i\\) are independent, identically distributed normal random variables, with mean zero and variance \\(\\sigma^2,\\) \\[\n  \\epsilon_{i}\\sim\\mathcal{N}(0,\\sigma^{2}).\n  \\]\n\nWe rewrite this in matrix-vector notation as \\[\\begin{equation}\n\\mathbf{y} = X \\boldsymbol{\\beta}+\\boldsymbol{\\epsilon},\n\\end{equation}\\] where the augmented matrix \\[\nX=\\left[\\begin{array}{ccccc}\n1      & x_{11} & x_{12} & \\cdots & x_{1p}\\\\\n1      & x_{21} & x_{22} & \\cdots & x_{2p}\\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\vdots\\\\\n1      & x_{1p} & x_{2p} &  \\cdots & x_{np}\n\\end{array}\\right]\n\\tag{1.1}\\] and \\[\n\\mathbf{y} = \\left[\\begin{array}{c}\n      y_1 \\\\\n      \\vdots \\\\\n      y_n\n      \\end{array}\\right].\n\\] An alternative way of writing this system, that will be needed below, is obtained by noticing that \\[\ny_{i}  - \\mathbf{x}_{i} \\boldsymbol{\\beta} = \\epsilon_{i},\n\\] which implies that the probability distribution of \\[\ny_{i} - \\mathbf{x}_{i} \\boldsymbol{\\beta} \\sim \\mathcal{N}(0,\\sigma^{2})\n\\] and hence that the probability distribution of each observation is Gaussian, \\[\ny_{i} \\sim \\mathcal{N}(\\mathbf{x}_{i} \\boldsymbol{\\beta},\\sigma^{2}),\n\\] or, in vector form, \\[\n\\mathbf{y} \\sim \\mathcal{N}(X \\boldsymbol{\\beta},\\sigma^{2}I),\n\\] where \\(I\\) is the identity matrix.\nFor simplicity, let us consider the special case of a straight line regression for a single covariate, \\(x,\\) \\[\n  y_i = \\beta_{0}+\\beta_{1}x_{i},\\quad i=1,\\ldots,n.\n\\] Then classical least squares regression consists of finding \\((\\beta_{0},\\beta_{1})\\) that minimizes the sum of the squared errors \\[\nE=\\sum_{i=1}^{m}\\epsilon_{i}^{2},\n\\] where \\[\n\\epsilon_{i} = \\left|\\beta_0+ \\beta_1 x_{i}-y_{i}\\right|.\n\\] The minimum was found by solving the equations for the optimum, \\[\n\\frac{\\partial E}{\\partial\\beta_0}=0\\,,\\quad\\frac{\\partial E}{\\partial\\beta_1}=0.\n\\] We find the two equations \\[\\begin{eqnarray*}\n    \\left(\\sum_{i=1}^{m}1\\right)\\beta_0+\\left(\\sum_{i=1}^{m}x_{i}\\right)\\beta_1 & = & \\sum_{i=1}^{m}y_{i},\\\\\n    \\left(\\sum_{i=1}^{m}x_{i}\\right)\\beta_0+\\left(\\sum_{i=1}^{m}x_{i}^{2}\\right)\\beta_1 & = & \\sum_{i=1}^{m}x_{i}y_{i}.\n\\end{eqnarray*}\\] This can be rewritten as the following matrix system, \\[\nX^{\\mathrm{T}} X \\boldsymbol{\\beta} = X^{\\mathrm{T}}  \\mathbf{y} ,\n\\tag{1.2}\\] with \\[\nX=\\left[\\begin{array}{cc}\n1      & x_{1} \\\\\n1      & x_{2} \\\\\n\\vdots & \\vdots \\\\\n1      & x_{n}\n\\end{array}\\right],\n\\quad  \\mathbf{y} =\\left[\\begin{array}{c}\ny_{1}\\\\\n\\vdots\\\\\ny_{n}\n\\end{array}\\right]\n\\quad\\mathrm{and}\\quad\n\\boldsymbol{\\beta} =\\left[\\begin{array}{c}\n\\beta_0\\\\\n\\beta_1\n\\end{array}\\right].\n\\]\nIn this univariate case, the system (Equation 1.2) can be solved explicitly. We obtain the optimal, least-squares parameter estimations, \\[\n\\beta_0 =  \\bar{y}-\\bar{x} \\beta_1 , \\quad \\beta_1 = \\sigma_{xy}/\\sigma_{x}^{2},\n\\] where the empirical means and variances are given by \\[\n\\bar{y}=\\frac{1}{n}\\sum_{i=1}^{n}y_{i},\\quad\\bar{x}=\\frac{1}{n}\\sum_{i=1}^{n}x_{i}\n\\] and \\[\n\\sigma^2_{xy}=\\frac{1}{n}\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)\\left(y_{i}-\\bar{y}\\right),\\quad\\sigma_{x}^{2}=\\frac{1}{n}\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)^{2}.\n\\]\nRecall the general result expressed now in terms of linear regression.\n\nTheorem 1.2 (Linear Regression) For \\(X \\in \\mathbb{R}^{n \\times p},\\) \\(\\boldsymbol{\\beta} \\in \\mathbb{R}^{p},\\) and \\(\\mathbf{y} \\in\\mathbb{R}^{n},\\) let \\(\\epsilon=\\epsilon(x)= X \\boldsymbol{\\beta} - \\mathbf{y}.\\) The general least-squares problem for linear regression is to find the vector \\(\\boldsymbol{\\beta}\\) that minimizes the residual sum of squares, \\[\n    \\sum_{i=1}^{n}\\epsilon_{i}^{2}=\\epsilon^{\\mathrm{T}}\\epsilon=( X \\boldsymbol{\\beta} - \\mathbf{y})^{\\mathrm{T}}( X \\boldsymbol{\\beta} - \\mathbf{y}).\n    \\] Any vector that provides a minimal value is called a least-squares solution. The set of all least-squares solutions is precisely the set of solutions of the normal equations, \\[\n       X^{\\mathrm{T}}X \\boldsymbol{\\beta} = X^{\\mathrm{T}}\\mathbf{y}.\n    \\]\nThere exists a unique least-squares solution, given by \\(\\boldsymbol{\\beta}  =\\left(X^{\\mathrm{T}}X\\right)^{-1}X^{\\mathrm{T}}\\mathbf{y},\\) if and only if \\(\\mathrm{rank}(X)=p.\\)\n\n\n\n1.2.2 Bayesian Linear Regression\nWe can now generalize to the case of Bayesian regression. In the Bayesian approach, not only the error term is random, but the elements of the model parameter vector \\(\\boldsymbol{\\beta}\\) are also considered to be random variables. We also suppose that we have a prior distribution \\(p(\\boldsymbol{\\beta})\\) available before any data are observed. Then, as observation data \\(X\\) become available, the initial probabilities of the parameters can be updated to a posterior probability distribution, according to Bayes’ Law. This posterior is a refined, narrower distribution and provides not only a better estimate of the parameters, but also a complete uncertainty quantification.\nRecall our linear model (Equation 1.1), \\[\n\\mathbf{y} = X \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon},\n\\] where the noise term is assumed to be Gaussian. The probabilistic model for regression is then a conditional probability, \\[\np(\\mathbf{y} \\mid X, \\boldsymbol{\\beta} ) = \\mathcal{N} \\left(\\mathbf{y} \\mid \\mu_X , \\Sigma_X  \\right)\n\\] and the Bayesian regression problem is to estimate the posterior probability distribution of the parameters, \\(p(\\boldsymbol{\\theta} \\mid \\mathbf{y}),\\) where \\(\\boldsymbol{\\theta}\\) includes \\(\\boldsymbol{\\beta}\\) and can also include the constant noise variance \\(\\sigma^2,\\) where \\(\\Sigma_X = \\sigma^2 I\\) and \\(I\\) is the identity matrix. In the above case, \\(\\mu_X = X \\boldsymbol{\\beta}\\) is a linear function of \\(X,\\) but, in general, linear regression can be extended to non-linear cases by simply replacing \\(X\\) by a non-linear function of the inputs, \\(\\phi(X).\\) This is called basis function expansion, and in particular, when \\(\\phi(X)=[1,x,x^2,\\ldots,x^d],\\) we obtain polynomial regression.\nIn general, we need to perform the following steps:\n\nSpecify the prior distribution of the parameters, \\(\\boldsymbol{\\beta}.\\) Note that the prior law can either be known (from historical data or models, for example), or its parameters can be included in the estimation process.\nSpecify the distribution of the noise term. This is always taken as Gaussian in the case of regression analysis. As before, its parameter (variance) is either known, or can be inferred.\nCompute the likelihood function of the parameters, using the noise distribution and the independence of each observation.\nCalculate the posterior distribution of the parameters from the prior and the likelihood.",
    "crumbs": [
      "Bayes' Theory",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Bayes' Theorem</span>"
    ]
  }
]